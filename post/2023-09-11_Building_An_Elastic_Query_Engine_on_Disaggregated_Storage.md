# Summary

主要动机：
- 介绍一种SharedDisk架构，通过存算分离来解决SharedNothing架构带来的硬件资源不匹配和缺乏扩缩弹性的问题。
- 要点是将节点的本地存储用作中间结果存储和持久化数据分片的直写缓存（注意是SharedDisk下对分片的缓存），并提供惰性一致性hash消除reshuffle。
- 文章主要在临时存储系统、查询调度、资源弹性和多租户机制上描述，同时也介绍了当前各个设计面临的困难。

核心思路：
- 在SharedDisk架构里引入临时存储的概念，即利用节点的本地存储作为某些算子的中间结果的存储和持久数据的缓存。
- 临时存储用于中间结果：本地内存、本地SSD和远端S3的三层存储。
- 临时存储用于持久数据：按文件做直写缓存权的分配，意味着持有缓存权就是持有最新数据和唯一的写入权。在扩缩上可以通过惰性一致性hash消除reshuffle对旧节点的影响。调度则根据查询涉及的文件，优先在缓存权节点上分配，如果该节点过载则工作窃取。

效果评估：
- 对中间结果而言，显著提高了读写效率。对持久存储缓存而言，读请求有80%的命中率，读写请求则是60%。

一些收获
- AP查询中如果能发现存在高度偏移的负载时，cache是一个很明显的优化方案。
- 不太清楚其他SharedDisk架构的设计，不知道Snowflake在设计上的优势。
- 另外我需要看看前面的The Snowflake Elastic Data Warehouse这篇。那篇似乎讲到更详细的内容，比如TableScan里做TaskStealing、CS里的优化器等。

# Introduction

希望解决现有的SharedNothingDBs的缺点
- 硬件负载不匹配：轻计算和重IO的查询可能由更好CPU但更差带宽的节点执行。部署两套集群提高了运维难度。
- 缺乏弹性：由于静态的并行度和分区配置，较难处理数据倾斜、时变负载的场景，以及扩缩容需要reshuffle
	- 另外提到传统数仓系统针对企业内部数据，解决的是可预测的数据量和查询效率的负载。现在则需要面临外部的不可预测的负载。

思路：存算分离，回到SharedDisk。数据存储在类似S3上，计算弹性由预热节点池支持（按需分配）。
- 抽离出中间结果作为持久存储并利用中间结果的存储作为直写cache
- 这样可以减少存算分离带来的多余网络开销。相对与单纯的SharedDisk，snowflake的方式是将文件缓存在某个节点上，后文提到是LazyConsistentHash。

# Design Overview

数据分为三部分：持久数据、元数据和中间结果数据。
- 中间结果数据就是类似Join等算子，能够产生的一些很大数据量的算子。他们需要把数据spill到SSD甚至远端S3上。

架构
- Cloud Service / 云服务：相当于用户接口的无状态服务，负责访问控制、查询计划优化、任务调度和监控。
- Virtual Warehouse / 虚拟数仓：一个VW相当于一组EC2实例，客户是以VW大小和使用时长为单位支付的。
- 弹性本地临时存储：因为S3不能提供低延时高吞吐的需求，所以构建了一个分布式临时存储。它不需要重分区和reshuffle.
- 弹性持久远端存储：使用S3。另外，S3支持的是不可变文件的存储。文件只能全部覆盖，甚至不支持append。所以这里用法是将一个块作为单位修改、写入，将byte作为单位读取和append。这应该是S3的正确用法。

# 临时存储系统

S3不能作为临时存储的原因是：没有低延时高吞吐的性能，并且提供了没有必要的高可用和持久性语义。

中间结果的存储架构：先利用本地内存和本地SSD，并允许数据spill到S3
- 未来挑战：spill到S3会降低性能，所以最好是给足资源供应。但资源估计本身就是很困难的，用户负载会变化。另外，给足供应和提高利用率是矛盾的。

持久数据的缓存
- 将临时存储作为cache是出于提高利用率的目的。因为中间结果用完即丢，那么业务不再高峰时中间结果就不会占用太多空间，此时可以用作缓存。
- 能带来性能提高的原因在于用户的查询本来就是高度偏移的，并且临时存储会比远端持久存储更快。
- 具体做法：
	- 根据文件名做一致性hash来分配文件的缓存权，每个文件的缓存权只会被一个节点占有。
	- 另外，cache是直写的，所以缓存权意味着该节点持有最新的数据和写入权。（有点像Frangipani）
	- 惰性一致性hash可以避免中间结果数据的reshuffle；sharedDisk避免了持久数据的reshuffle。可以说SharedDisk这一套是专门为弹性扩缩的组合拳。
- 未来挑战
	- 中间结果和持久数据共用本地存储是需要更好的分配策略的。目前中间结果更优先，这至少不是最优的性能策略
	- 另外，临时存储分为三层，每层之间都有一个LRU做spill。需要一种可以跨层的cache协调方法。

# 查询调度

调度这边主要逻辑是：
- 根据涉及到的读写文件，调度到一致性hash的节点上。
- 如果节点过载，那么可以分配新的节点来窃取工作。另外需要计算得知是否有更少的时间成本。

# 资源弹性

针对中间查询结果存储，仍然需要reshuffle。解决这个问题的方式是惰性一致性hash。

Snowflake 依靠缓存机制最终“收敛”到正确的状态。在扩容后不会立即reshuffle，而是等待任务再次执行时（比如工作窃取或再次执行相同的查询），将其安排在新节点。此时，对应文件将从远程持久存储中读取并缓存在本地。原节点的文件将不再被访问，并且最终将从缓存中逐出。

未来挑战：目前有20%用户使用了VW的弹性，但数据显示，节点变更的密度和查询变化的密度并不匹配。原因在与
- VW的集群拓扑变化是由用户主动触发的，而这种响应速度不敏捷是由于用户缺乏对负载的监控能力较弱。
- 弹性的粒度较粗，目前也只能做到随着query的变化而调整，未来则希望是task/stage这个粒度。

# 多租户

目前多租户的隔离通过VW的物理集群隔离实现，但这样也带来了资源利用率会更低的问题。另外，各VW的workload高峰并不同步，这就为错峰利用资源提供了可能性。

成本问题：snowflake为了提升弹性速度，为每个VW后备了一组pre-warming的EC2 pool，但这样自然也存在成本，原来云服务提供商（snowflake自身）是以hour为单位向用户计费，这样只要在1小时内有用户使用了池子中的节点，哪怕只用一会也可以按小时收费，这样就分摊了整个pool的成本，但现在云上的收费模式细化为按秒级计费，pre-warming pool的成本就不再可以忽视了，这也驱动着从预置资源到共享资源的演进。

所谓共享资源就是多个VW共用同一组EC instances集群，实现资源复用，但这样就带来隔离性的难题，不解决就会有长尾性能差这个大问题，尤其是临时性存储系统，如何实现一个解耦的、弹性的、多租户共享的临时性存储，同时提供租户间隔离性是一个具有巨大挑战的课题。