# [MIT6.824] Lecture10 - Aurora

> - http://nil.lcs.mit.edu/6.824/2020/labs/lab-raft.html



## Lecture 10 - Aurora(2017)：变种Qourum的容错能力、只接受Log的块存储服务、带缓存的只读副本

> - http://nil.lcs.mit.edu/6.824/2020/papers/aurora.pdf
> - https://zhuanlan.zhihu.com/p/232345104

### 架构演变

在MySQL基础上，结合Amazon自己的基础设施，Amazon为其云用户开发了改进版的数据库，叫做RDS。对于RDS来说，有且仅有一个EC2实例作为数据库。这个数据库将它的data page和WAL  Log存储在EBS，而不是对应服务器的本地硬盘。当数据库执行了写Log或者写page操作时，这些写请求实际上通过网络发送到了EBS服务器。所有这些服务器都在一个AZ中。下面是MySQL镜像方案的架构：

![mirrored-mysql](/static/image/2022-01-09/mirrored-mysql.png)

每一次写操作，例如数据库追加日志或者写磁盘的page，数据除了发送给AZ1的两个EBS副本之外（EBS通过Chain Replication机制交互），还需要通过网络发送到位于AZ2的副数据库。副数据库接下来会将数据再发送给AZ2的两个独立的EBS副本。之后，AZ2的副数据库会将写入成功的回复返回给AZ1的主数据库，主数据库看到这个回复之后，才会认为写操作完成了。

但如论文中表1所示，RDS的写操作代价极高，就如你所预期的一样高，因为需要写大量的数据。即使如之前的例子，执行类似于  x+10，y-10，这样的操作，虽然看起来就是修改两个整数，每个整数或许只有8字节或者16字节，但是对于data  page的读写，极有可能会比10多个字节大得多。因为每一个page会有8k字节，或者16k字节，或者是一些由文件系统或者磁盘块决定的相对较大的数字。这意味着，哪怕是只写入这两个数字，当需要更新data page时，需要向磁盘写入多得多的数据。如果使用本地的磁盘，明显会快得多。

接下来介绍Aurora的解决方案：

![aurora](/static/image/2022-01-09/aurora.png)

1. 现在替代EBS的位置，有6个数据的副本，位于3个AZ，每个AZ有2个副本。所以现在有了超级容错性，并且每个写请求都需要以某种方式发送给这6个副本。
   - 数据库服务发送给存储服务的数据只有Log，而非data page等大数据。这节约了相当多的带宽用量。
   - Aurora抛弃了Chain Replication，它并不需要6个副本都确认了写入才能继续执行操作。这里是一个变种的Quorum，具体而言是Write Quorum为4、Read Qourum为。那么只要4个副本写入成功，数据库就可以继续执行操作。
2. 存储服务器内存最终存储的还是数据库服务器磁盘中的page。
   - 当一个新的写请求到达时，新的Log条目，它会立即被追加到影响到的page的Log列表中。
   - 当读请求到来时，才立即执行这个Log，来更新EBS本地的data page。
3. Aurora不仅有可读写的主数据库实例，同时还有多个数据库的只读副本。
   - 只能支持一个写入者的原因是，Log需要按照数字编号。如果有多个数据库以非协同的方式处理写请求，那么为Log编号将会非常非常难。
   - 当向只读数据库发送读请求后，只读数据库需要弄清需要查询哪些data page，之后直接从存储服务器读取，同时会缓存读取到的page。
   - 只读数据库也需要更新自身的缓存，所以主数据库也会将它的Log的拷贝发送给每一个只读数据库。只读数据库用这些Log来更新它们缓存的page数据，进而获得数据库中最新的事务处理结果。（这意味着只读数据库会落后主数据库一点，通常来说不会是一个大问题。）（对只读服务器而言，是否是最终一致性呢？只读数据库是否应该作为Read Qourum或者是N的一员？）
   - 数据库背后的B-Tree结构非常复杂，可能会定期触发rebalance。中间状态的数据是不正确的，只有在rebalance结束了才可以从B-Tree读取数据。但是只读数据库直接从存储服务器读取数据库的page，它可能会看到在rebalance过程中的B-Tree。这时看到的数据是非法的，会导致只读数据库崩溃或者行为异常。

这里再提一下Aurora的容错目标：

- 对于写操作。如果只有一个AZ挂了，那么写操作不受影响。
- 对于读操作。当一个AZ加上另一个处于其他AZ的服务器挂了之后（即AZ+1台服务器），读操作不受影响。
- 容忍暂时的慢副本。Aurora期望能够在出现短暂的慢副本时，不像Chain Replication那样阻塞，而是仍然能够继续执行操作。
- 如果一个副本挂了，备份数据是争分夺秒的。通常来说服务器故障不是独立的，如果一个服务器挂了，通常意味着有很大的可能另一个服务器也会挂，因为它们有相同的硬件，或许从同一个公司购买，来自于同一个生产线。如果其中一个有缺陷，非常有可能会在另一个服务器中也会有相同的缺陷。所以，当出现一个故障时，第二个故障可能很快就会发生。对于Aurora的Quorum系统，有点类似于Raft，你**只能从局部故障中恢复**。所以这里需要快速生成新的副本（Fast Re-replication）。期望能够尽可能快的根据剩下的副本，生成一个新的副本。

接下来，Aurora的变种Qourum会很有用。

### Qourum Replication

Aurora使用的是一种经典quorum思想的变种。Quorum系统背后的思想是通过复制构建容错的存储系统，并确保即使有一些副本故障了，读请求还是能看到最近的写请求的数据。

假设有N个副本。为了能够执行写请求，必须要确保写操作被W个副本确认，W小于N。所以你需要将写请求发送到这W个副本。如果要执行读请求，那么至少需要从R个副本得到所读取的信息。这里的W对应的数字称为Write Quorum，R对应的数字称为Read Quorum。这是一个典型的Quorum配置。这里的关键点在于，W、R、N之间的关联。Quorum系统要求，任意你要发送写请求的W个服务器，必须与任意接收读请求的R个服务器有重叠。这意味着，R加上W必须大于N（ 至少满足R + W = N + 1 ），这样任意W个服务器至少与任意R个服务器有一个重合。

这里还有一个关键的点，客户端读请求可能会得到R个不同的结果。现在的问题是，客户端如何知道从R个服务器得到的R个结果中，哪一个是正确的呢？**通过不同结果出现的次数来投票（Vote）在这是不起作用的**，因为我们只能确保Read Quorum必须至少与Write  Quorum有一个服务器是重合的，这意味着客户端向R个服务器发送读请求，**可能只有一个服务器返回了正确的结果**。对于一个有6个副本的系统，可能Read  Quorum是4，那么你可能得到了4个回复，但是只有一个与之前写请求重合的服务器能将正确的结果返回，所以这里不能使用投票。在Quorum系统中使用的是**版本号**（Version）。所以，每一次执行写请求，你需要将新的数值与一个增加的版本号绑定。之后，客户端发送读请求，从Read Quorum得到了一些回复，客户端可以直接使用其中的最高版本号的数值。

如果你不能与Quorum数量的服务器通信，不管是Read Quorum还是Write Quorum，那么你只能不停的重试。

Aurora采用的变种Qourum是指N=6，W=4，R=3。它能够实现上一节描述的Aurora的容错目标：

- 对于写操作。W等于4意味着，当一个AZ彻底下线时，剩下2个AZ中的4个服务器仍然能完成写请求。
- 对于读操作。R等于3意味着，当一个AZ和一个其他AZ的服务器下线时，剩下的3个服务器仍然可以完成读请求。
- 对于慢副本。Quorum系统本身可以剔除暂时的慢副本。
- 对于快速生成副本。当3个服务器下线了，系统仍然支持读请求，但是却不能支持写请求。所以当3个服务器挂了，变种Quorum系统有足够的服务器支持读请求，并据此重建更多的副本，但是在新的副本创建出来替代旧的副本之前，系统不能支持写请求。
