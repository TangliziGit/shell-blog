# Notes on MIT 6.824

> 1. MIT6.824 Schedule: https://pdos.csail.mit.edu/6.824/schedule.html
> 1. MIT6.824分布式系统课程: https://www.zhihu.com/column/c_1273718607160393728
>

明确一下目标，学习这个课程是为了深入了解分布式应用的概念和指标（为什么使用分布式、困难是什么、一致性是什么等问题），以及现有分布式应用（MapReduce、GFS等）和主流问题解决方案（主备容灾细节、Raft等）。

- 关于分布式的概念，会在每个Lecture里记录下来。如果缺失了某个lecture，那么说明这节里没有什么需要注意的地方。
- 对于paper里的关键点，则会专门开一个标题去记录重点。
- 对于每个lecture，最好有一句话能够总结整个章节



## Lecture 1 - MapReduce：分布式计算 - 计算的抽象、分发和容错

### 分布式系统的目的 & 困难

1. **提升性能**
2. **错误容忍**（冗余计算、冗余存储）
4. 安全与隔离（例：区块链）
4. 物理容灾

分布式系统困难的原因：

1. 性能：并行问题
2. 容错：局部故障问题
3. 扩展性：并非机器加更多机器就有更好性能

### 分布式系统的三个抽象

- 存储
- 计算
- 通信

### 分布式系统的话题

- 实现：RPC方法、多线程技术、并发控制
- 可扩展性：系统应当可以通过增加机器来提高系统性能。我们希望通过增加机器的方式来实现扩展，但是现实中这很难实现，**需要架构设计来将可扩展性持续推进下去**。
  - 例：当用户量上涨，如何提高web系统性能？
    1. 部署多个web服务器，直到瓶颈从吞吐量转变为数据存储
    2. 数据库分库分表，需要大量工作

- 容错性：在成百上千台服务器上运行系统，故障将变为常态。常见的容错评判机制是可用性和自愈性
  - 一些提高容错性的手段：使用非易失性存储，建立副本机制在故障时切换（数据冗余）

- 一致性：强调强一致性的CAP、强调弱一致性的BASE
  - BASE是对CAP的：基本可用（Basically Available）、软状态（Soft State）和最终一致性（Eventually Consistent）


## Paper：MapReduce

> https://pdos.csail.mit.edu/6.824/papers/mapreduce.pdf

1. Core design
   - Users specify a map function that processes a key/value pair to generate a set of intermediate key/value pairs, and a reducefunction that merges all intermediate values associated with the same intermediate key.
2. Target
   - hides the messy details of parallelization, fault-tolerance, data distribution and load balancing in a library
3. Why use map reduce primitives?
   - parallelize large computations easily
   - to use re-execution as the primary mechanism for fault tolerance

### Model

- map(k1, v1) -> list(k2, v2)：接受kv二元组，产出中间kv对
  - 可以理解成按照k2聚合列表v2
- reduce(k2, list(v2)) -> list(v2)：合并中间kv对
  - 内部使用Emit处理结果

在业务上，MapReduce简化了在k2上聚合逻辑。

**Examples**

- 分布式Grep
- 倒排索引：map接受文件名和内容，产出词和文件名，reduce处理词和文件名列表
- 分布式排序：TODO：拆分数组，map接受子数组号，产出字数组号和已排序数组，reduce处理子数组k路归并

### 执行流程

![mapreduce](/static/image/2021-08-30/mapreduce.jpeg)

1. 用户程序首先将输入文件分割成若干份确定大小的分区，然后在worker节点和master节点执行用户程序。
2. master节点分配map和reduce任务
3. map节点读取分区数据，产生**中间kv，存储在内存中**
4. map节点**周期性存储中间数据于磁盘**，在map完成后向master通知输出R个文件的位置和大小。master更新数据结构，通知reduce处理新的文件
5. reduce节点受到通知后，通过RPC读取map本地数据。当读取完所有map数据后（全部结束？），所有数据按key排序。
6. 用户程序的reduce函数开始执行。reduce的输出以追加方式写入结果文件，当reduce结束则原子性修改文件名。
7. 全部map reduce执行完毕后，master开始执行用户程序

**注意**：

1. map本地文件会生成几个？

   1. R个。当所有map结束后，reducer依次拉取各个map本地数据做reduce。

2. map和reduce函数是同时执行么？即reduce读map的一部分周期提交数据进行处理？

   - 阅读后文发现，不是同时执行的。

     > When a map task completes, the worker sends a message to the master and includes the names of the R temporary files in the message.

   - 注意map函数不可以与reduce节点的排序同时执行，因为reduce函数需要等待某个key结束了map，然后再做数据的排序。

3. 为什么reduce函数执行前必须要排序？

   - 排序后的kv对，可以在同一时刻只reduce一个list。当所有list被reduce后，一次性写入输出文件并原子命名。
   - 如果不进行排序，将同时处理大量不同的key，会需要大规模的线程（内存、切换时间）、内存（极端情况是所有数据）

4. master维护什么状态？

   - 各个节点的状态机模型：idle、in-progress、completed
   - 节点的identy信息
   - 每个完成的map产出的文件位置和大小

### 容错机制

#### Worker 容错

master会周期性ping各个worker，如果失败则说明节点失败。

| 任务类型 \ 崩溃时当前任务状态 | 正在执行                         | 执行完成            |
| ----------------------------- | -------------------------------- | ------------------- |
| map                           | 重新分配节点，该节点返回idle状态 | 重新分配节点执行map |
| reduce                        | 重新分配节点，该节点返回idle状态 | 不做处理            |

**注意**：

1. 为什么map在失败后，不能使用当前任务输出的中间kv？
   - 因为中间kv是存于内存，同时周期性写入的文件只是本地文件。也因此map节点失败后，只能重新分配并执行。
2. 为什么reduce在执行完成后，不用处理失败？
   - 因为reduce输出文件是全局公用的，reduce节点失败不影响存储。



#### Master 容错

- **方案一：恢复数据** - 将所有状态存储checkpoint，失败后恢复即可
- **方案二：let it crash** - 单节点的master不太可能失败，若失败则用户自行处理



#### 原子性提交

- 当map和reduce函数是确定性的（纯函数），那么节点通过提交原子性commit，来达到无错误的顺序执行。
  - map节点执行成功后，向master发送信息。master将忽略已经执行完的任务的信息，以避免冗余。
  - reduce节点执行成功后，依靠操作系统的原子性重命名（mv）来提供原子性。
- 当map和reduce函数是非确定性的（非纯函数），那么会提供更弱的方式。具体是什么，论文似乎没有提到。



### 调优：备份任务

一个使运行时间变长的常见原因是某个任务掉队。

原因有很多，比如硬盘速度太慢、CPU缓存禁用等。

解决方法是增加备份机制，当某些任务执行时长太久，则新分配i一个节点给此任务。当主备某一个完成后，则其他任务终止。



## Lecture 3 - GFS：海量分布式存储 - 单节点维护元数据、数据节点处理读写、数据副本

### 分布式存储难点：性能与容错的权衡

使用分布式的系统，从目标来看都是为了将数据分片（将数据分割放到大量的服务器上，从而并行取数据）以达到性能提升。

但是如果你在成百上千台服务器进行分片，你将会看见常态的故障。在许多系统中会为了容错而降低性能，这就说明分布式系统需要在性能和一致性之间做权衡。

- 提升性能 -> 引入分布式 -> **数据分片**
- 系统存在局部错误 -> **支持容错能力 -> 引入数据副本 -> 提供副本间一致性能力** -> 降低性能



### 关于一致性的错误设计：不安全请求的顺序

课程中介绍了分布式kv存储设计为多写的场景：你无法保证每个replica能够接收到完全相同顺序的写请求，这会导致副本状态不一致问题。

这里可以推广为分布式系统需要处理不安全请求（影响系统状态的请求）顺序问题。你可以设计保证不安全请求在多个replica上顺序完全相同，也可以设计为只有一个replica接受不安全请求。



## Paper : GFS

> https://pdos.csail.mit.edu/6.824/papers/gfs.pdf





## Lecture 4 - VMware FT: Primary Backup Replication - 主备复制的方案、关注点和典型故障场景

> The Design of a Practical System for Fault-Tolerant Virtual Machines: https://pdos.csail.mit.edu/6.824/papers/vm-ft.pdf

VMware FT(Fault Tolarence)是指两个虚拟机的主备容错。它需要两个物理服务器，Primary虚机在其中一个物理服务器上，Backup在另一个物理服务器上。（将Primary和Backup运行在一台服务器的两个虚拟机里面毫无意义，因为容错本来就是为了能够抵御硬件故障）

### 两种备份方法 / 主备方案

这里介绍了两种复制方法：

1. State Transfer
   - 是指primary将自身**内存中的状态信息**定期发送给backup存储。当primary故障后，再从backup进行回复。
   - 需要注意的是，每次状态迁移需要进行一次大拷贝，但也可以做diff来减少传输量
2. Replicated State Machine
   - 将系统受到的一系列**外部输入**发送给backup进行存储。这基于一个事实：两个相同状态的系统在受到完全一致的外部输入后将保持一致、互为副本。

我们倾向于Replicated State Machine，是出于**数据传输量**的考虑：State Transfer显然需要更多的数据来做备份，而后者则只需存储client发送来的指令。但Replicated State Machine**需要对系统作出大量的假设**，较为复杂；而State Transfer只需要暴力存储状态即可。

当前的Replicated State Machine是在单核CPU下的方法：在**多核CPU下相同的指令并非造成相同的结果**。VMware后续推出了新的方法解决并行情况，但方案更倾向于状态迁移。

> 1. 在备份不一致的情况下，primary故障回复后导致混乱的问题如何处理？
>    - 后面的工作原理讲到：当backup确认受到外部输入后，再进行下一步工作（即输出控制）。所以避免了备份不一致的情况。属于强一致性。
> 2. 在Replicated State Machine中，随机操作如何处理？

### 主备同步的问题

一些主备同步中的重要问题：（不仅仅集中在Replicated State Macine方案上，其他方案也需要考虑）

- **同步级别、同步频率、状态定义**
  - 在VMware中，主备的状态是指primary内存中的每个bit，即主备在底层也是完全一致的。很少有系统如此设计备份方案，因为它过于困难（甚至需要考虑中断在主备中同一个位置进行）。他的优点是，在VMware FT支持的微处理器上，任何一个可运行的软件都可以具备容错性。你不需要考虑软件的任何逻辑和源码。
  - 然而大部分的系统类似GFS，备份数据是指应用程序级别数据chunk，每个针对chunk都保存有默认3个副本在不同的机架和服务器上。GFS只需要保证chunk副本的一致即可。
- **主备切换**：当系统primary故障后进行主备切换，在理想情形是应当没有任何客户端会注意到这里的切换。在切换过程中，必然会有异常，我们必须找到一种应对它们的方法。
- **备份故障**：当两个备份其中一个故障后，应当尽快上线新的备份避免所有备份宕机。
  - 注意：创建新的副本需要较大代价。因为此时新的副本没有一致的内部状态，所以创建时只能进行状态迁移，不能采用复制状态机。

### 工作原理

在真实场景下，一个局域网中存在着primary宿主机和backup宿主机，他们分别运行着primaryVMM和backupVMM (VMM，Virtual Machine Monitor)，用于监控主备VM。同时这个局域网中还存在者一些client（此处用于VM存储的disk server也可以算为client）。

#### 主备同步时机与流程

首先讲<u>主备同步的时机，并如何进行主备同步</u>。当client向primary发送一个请求分组后，将触发**primary host的中断，之后这个中断将数据送给VMM**。此时VMM可以发现该分组是需要发送给primary vm的，于是VMM开始如下两个操作：

1. 向**本地（primary）的vm模拟网络请求中断**，将数据发送给primary vm的应用程序中。（当primary VMM收到处理完的primary输出，并收到到backup的ACK后，再发送响应。这个过程被称为输出控制）
2. **向backup host发送一个相同的网络请求**，之后backup VMM将可以受到此分组。

此时主备vm都受到了相同的外部输入，他们会以相同的方式处理外部输入，并最终达到状态一致。最终，primary将网络响应发送给client，backup因为知道自己是备份所以丢弃响应。（论文中的Log Channel就特指局域网中priamry向backup VMM发送外部输入的信道）

#### 主备切换流程

其次，讲<u>主备切换</u>的过程。实际场景中，backup能够在一秒内受到很多条log，有一部分是primary的定时器中断（大概100次每秒）。当backup没有在一段时间内受到primary的定时器中断后，说明primary出现失败，需要主备切换了。

1. 首先backup不再接受来自primary VMM的log，而是接受网络输入作为外部输入源，同时不再丢弃输出分组。
2. backup在网络中做一些处理（？），使得client都转而访问backup。

> 1. Backup怎么让其他客户端向自己发送请求？
>
>    - Robert：（ARP欺骗）每个虚拟机也有一个唯一的MAC地址，当Backup虚机接手时，它会宣称它有Primary的MAC地址，并向外通告说，我是那个MAC地址的主人。
>
> 2. 在Replicated State Machine中，随机操作如何处理?
>
>    - VMware FT的设计者认为他们找到了所有类似的操作，对于每一个操作，Primary执行随机数生成，或者某个时间点生成的中断（依赖于执行时间点的中断）。而Backup虚机不会执行这些操作，Backup的VMM会探测这些指令，拦截并且不执行它们。VMM会让Backup虚机等待来自Log Channel的有关这些指令的指示。
>    - 关于设计伪随机：应该不能要求primary和backup使用同一个seed做随机。因为随机数场景在于各类加密算法，两台机器随机数完全相同可能打破一些算法的前提。

### 非确定性事件 （Non-Deterministic Events）

> https://zhuanlan.zhihu.com/p/190779044

通常情况下，代码执行都是直接明了的，但并不是说计算机中每一个指令都是由计算机内存的内容而确定的行为。这一节，我们来看一下不由当前内存直接决定的指令。这些指令在Primary和Backup的运行结果可能会不一样。这些指令就是所谓的非确定性事件。非确定性事件可以分成几类：

1. 客户端输入。
   - 当我们说输入的时候，我们实际上是指接收到了一个网络数据包。而一个网络数据包对于我们来说有两部分，一个是数据包中的**数据**，另一个是提示数据包送达了的**中断**。
   - 当网络数据包送达时，通常网卡的DMA（Direct Memory  Access）会将网络数据包的内容拷贝到内存，之后触发一个中断。操作系统会在处理指令的过程中消费这个中断。对于Primary和Backup来说，这里的步骤必须看起来是一样的，否则它们在执行指令的时候就会出现不一致。所以，这里的问题是，**中断在什么时候，具体在指令流中的哪个位置触发**
2. 怪异指令：有一些指令在不同的计算机上的行为是不一样的
   - 随机数生成器
   - 获取当前时间的指令，在不同时间调用会得到不同的结果
   - 获取计算机的唯一ID
3. 另外一个常见的非确定事件，即多CPU的并发。
   - 当服务运行在多CPU上时，指令在不同的CPU上会交织在一起运行，进而产生的指令顺序是不可预期的。
   - 另外，这里探究了论文未提即的log条目内容，教授猜测有三样内容：
     - 事件发生时的指令序号。因为如果要同步中断或者客户端输入数据，最好是Primary和Backup在相同的指令位置看到数据，所以我们需要知道指令序号。这里的指令号是自机器启动以来指令的相对序号，而不是指令在内存中的地址。比如说，我们正在执行第40亿零79条指令。所以日志条目需要有指令序号。对于中断和输入来说，指令序号就是指令或者中断在Primary中执行的位置。对于怪异的指令（Weird  instructions），比如说获取当前的时间来说，这个序号就是获取时间这条指令执行的序号。这样，Backup虚机就知道在哪个指令位置让相应的事件发生。
     - 日志条目的类型
     - 数据

> 1. 如果Backup领先了Primary会怎么样？
>    - 它会维护一个来自于Primary的Log条目的等待缓冲区，如果缓冲区为空，Backup是不允许执行指令的。如果缓冲区不为空，那么它可以根据Log的信息知道Primary对应的指令序号，并且会强制Backup虚机最多执行指令到这个位置。所以Backup总是落后于Primary至少一个Log。
> 2. 能不能输入送到Primary，输出从Backup送出？
>    - 这是一个很有意思的方法。Backup输出说明它已经完成一致性任务，不过primary仍需要获知backup是否收到log。如果没有primary没有感知到backup收到log，那么primary应该继续执行么？当然不应该，否则会出现不一致情况。

### 主备切换 & 主备故障的场景

#### 副本不一致下的主备切换

当primary VMM收到处理完的primary输出，并收到到backup的ACK后，再发送响应。这个过程被称为输出控制。

这属于强一致性，意味着网络消耗成为影响性能的重要因素。

#### 主备切换后的重复响应

当Backup的Log缓冲区仍有很多剩余未处理时，Primary由于故障而Backup触发接管机制。这时Backup的外部输入Log将引导vm发出重复的响应。

但是需要注意到，副本在TCP层面也进行了复制。这说明Backup知晓链接的TCP序列号信息，这样重复响应会被Client的TCP栈抛弃。同时，Backup和Client之间并没有真正保持TCP链接，所以Backup应该会受到Client的TCP Reset响应。（我猜测Backup可以不做处理，这样Client没有受到响应。那么client会有两种反应：请求没有到达primary，所以系统没有状态变化；或者响应没有到达client，所以系统已经发生变化。这本来就是client需要考虑的事情。所以backup在这个情况下，完全可以不处理Reset。）

通常而言，<u>分布式系统基本不可能保证不产生重复输出</u>。这需要其他机制来处理，一种可能是在应用层面设计序列号。

> 1. 主备在网络协议上是完全一致？IP地址都是一样的么？
>
>    需要注意的是，运行VM的物理服务器在网络传输上被设计为透明的。这意味着VM拥有与物理服务器一样的网络协议，它们拥有独立的IP和MAC（IP可能需要在物理机的网段中）。交换机、路由器等网络设备会正常执行它们的操作，并不会注意到VM的存在。

#### 主备网络链接断开：脑裂（Split Brain）

> **分布式场景下的定理：你无法判断另一个计算机是否真的挂了**

VMware的解决方法是主备依赖第三方的TestAndSet服务。当主备的网络通信断掉后，双方都会申请成为primary。TestAndSet服务相当于一个锁，它决定了主备之中谁应该成为priamry。

这里TestAndSet服务似乎是单点故障的受害者。但VMware肯定也考虑到这点，所以它应当也是有主备的。





## Paper：Raft

> http://nil.lcs.mit.edu/6.824/2020/labs/lab-raft.html

Raft是一个为了管理多副本日志的共识算法。一致性算法允许一组机器作为一致的组工作，同时可在一些成员出现故障时存活下来。它**强调容错性（崩溃容错和拜占庭容错，这里只讨论前者）。**

### 引入

Raft与其他共识算法相似（特别是VSR, viewstamped replication），区别在于：

- **Strong leader**：Raft使用了一种比其他共识算法更强的领导形式。
  - 提升系统性能：在一些无Leader的多副本系统（如Paxos）中，通常需要在一轮消息中确定一个临时Leader，然后在下一轮消息中再确认请求。这花费了两倍的时间。
  - 帮助理解Raft系统。
- **Leader election**：Raft使用随机计时器来选举领袖。这只在心跳包上增加了少量的机制，就能简单和迅速解决冲突。
- Membership changes：这允许集群在配置更改期间继续正常运行。

对比更复杂的Paxos，Raft通过下面的方法进行了简化：

- 分治：Raft将算法分解成**Leader选举、日志复制、安全性和成员变动**。
- 简化状态空间



### 复制状态机

与GFS和HDFS类似，Raft将上层应用程序作为复制状态机管理（见上文VMware FT）。Raft作为共识模块，<u>可以在节点失败的场景下，保证日志内容和顺序的最终一致性。</u>具体而言，Raft能够提供如下的属性：

- **安全性**（safety）：在**所有非拜占庭条件下（包括网络延迟、网络分区、丢包、包重复、包乱序）**，从不返回错误的结果。
- **可用性**：大多数节点正常运行并可相互通信情况下，系统保持正常运行。当节点失败重启后，能够恢复状态并重新加入集群。
- **不依赖时间**：错误的时钟和消息延迟在极端的场景下，会导致可用性问题。
- **性能**：只需过半的节点同步Log，那么就可以响应客户端。



### Raft共识算法

每个节点都有一种临时角色：Leader、Follower和Candidate。他们的职能和状态转移如下：

- Leader：主要用于处理客户端请求、发送AppendEntry（心跳包&日志同步）
  - 变为Follower：受到高Term的VoteRequest & 高Term的AppendEntry & 高Term的AppendEntry响应
- Follower：（初始值）被动接受Leader的AppendEntry进行日志同步，接受Candidate的VoteRequest选主
  - 变为Candidate：选举定时器到期
- Candidate：选主时主动发送VoteRequest
  - 变为Leader：选举通过过半仲裁
  - 变为Follower：受到高Term的VoteRequest & 高Term的AppendEntry & 高Term的VoteRequestResponse

注意：

1. 其实<u>任何角色受到任何高任期的请求和响应，都会转变为对应节点的Follower</u>。
2. 每个RPC都会在响应超时情况下进行重试尝试，并且每个RPC都是并行执行。
3. 每个Term会将时间分割开来，它在整体上表现成逻辑时钟。



#### 选主流程

- **触发时机**：某个Follower / Candidate在选举定时内，没有收到Leader的AppendEntry

- **选主流程[发送者]**：
  1. 节点状态改变：`{Role=Candidate, Term+=1, VotedFor=self}`
  2. 并发发送`VoteRequest{Term, CandidateID}`给所有server
  3. 统计结果`VoteRequestResponse{Term, VoteGranted}`：
     - `Term > currentTerm` => 成为对应节点Follower：`{Role=Follower, Term, VotedFor=nil}`
     - `Votes > serverCount / 2` => 成为Leader：`{Role=Leader}`
     - 选举超时 => 重新选主
  4. （当受到同等或高等Term的AppendEntry时，转变为Follower）
- **选主流程[接收者]**：`VoteRequest{Term, CandidateID, LastLogTerm, LastLogIndex}`
  - `Term < currentTerm || LastLogTerm < currentLastLogTerm || LastLogIndex < currentLastLogIndex` => 响应投票失败
  - `Term == currentTerm` => 检查votedFor是否为本人，投票并变更votedFor
  - `Term > currentTerm` => 转变为对应Follower `{Term, Role=Follower, votedFor=CandidateID}` ，并投票

**注意**：

- 这个RPC是幂等的。
- 可以将选主流程认为是：在`{Term, LastLogTerm, LastLogIndex, sendingTime}`上识别偏序集的过程（尤其是Term）。
- 最新的Log被定义为，有最高的Term和最高的Index。
- 会存在脑裂，但是不过半的小集群Leader不会提交Log。于是小集群Leader不会响应Client的请求，也不会执行Client的命令。
- 在网络恢复后，小集群Leader将收到大集群Leader的AppendEntry。此时更新状态，并改变角色为Follower。
- 随机化的定时器可以解决Candidate同时请求投票，导致的系统一直投票死锁的情况。但是选举定时器的随机范围需要规范确定：
  - 下限：应当是心跳包时间间隔的n倍，这样能确保follower能够避免因为偶然的网络异常而触发选举
  - 上限：它影响了Leader失败后系统恢复速度。当故障频繁时，需要重点权衡。
    - 我们需要考虑在两个节点超时时间差之内，应当可容纳单趟VoteRequest RPC的时长。



#### 日志复制

每个Client请求包含由复制状态机执行的命令。Leader将命令附加到其日志作为新条目，然后与每个其他服务器并行的复制Log。当Log已提交时，Leader将Log应用于其状态机，并将该执行的结果返回给客户端。如果Followers崩溃或运行缓慢，或者如果网络数据包丢失，那么Leader将无限期重试。

- **触发时机**：？？？（定期AppendEntry还是Start接口）
- **复制流程[发送者]**：
  - 发送`AppendEntry{}`
  - 统计响应`AppendEntryResponse{Term, Success}`：
    - `Term > currentTerm` => 成为对应节点Follower：`{Role=Follower, Term, VotedFor=nil}`
    - `Success` => 更新matchIndex & nextIdx & 根据所有matchIdx更新commitIdx(仅对当前term的log做检查)
    - `!Success` -> 对应server nextIdx--
    - `Timeout` -> retry
  - 从结果上来看，对于本次Log的响应：
    半数成功 -> 根据所有matchIdx更新commitIdx（已提交）本地执行命令，并响应Client
- **复制流程[接受者]**：
  - `Term < currentTerm` => 响应复制失败
  - `Term >= currentTerm && lastLogIndexTerm match ` =>  添加Log；更新commitIndex；转变为对应Follower `{Term, Role=Follower, votedFor=nil}` 

**注意**：

- 这个RPC是幂等的。
- 对Leader控制的网络分区来说，当term和index一致，那么log一致。这是单节点leader能够保证的，它不会像多节点那样有冲突。
- 当prevLogTerm & Index不同时，不更新。一旦更新，那么从冲突点开始的所有log，复制append req的logs。所以完全一致
- 如果Follower的日志与Leader的日志不一致，在下一次AppendEntries中一致性检查将失败。RPC被拒绝后，Leader递减nextIndex并重试AppendEntries。最终，nextIndex将达到Leader和Follower日志匹配的点。



#### 安全性TODO

首先总结一下Raft的安全特性：

- **选举安全**：在任期内最多只能选出一位领导人。
  - 论证：因为Candidate是通过半数仲裁赢得选举，同时一个节点在同一个任期里只能投票给一个节点。在同一Term中选出两个Leader一定意味着至少存在一个节点给两个节点同时投同意票，这是不可能的。
- **Leader仅能追加日志**：Leader从不覆盖或删除其日志中的条目，它只添加新条目。
- **日志匹配**：如果两个日志包含一个具有相同Index和Term的条目，那么从零到给定Index的所有条目中的日志都是相同的。
  - 论证：
    1. 首先，当term和index一致，那么log内容一致。这是单节点leader能够保证的。
    2. 其次，在Leader做AppendEntry时每个Log都有唯一的prevLogIndex和prevLogTerm，仅当被同步节点具有响应前置Log，它才能接受此Log并附加在前置Log之后（类似于区块链）。
    3. 所以，当两个节点（Leader节点同理）接受相同Index和Term的Log后，可以说明之前的所有Log都是相同的。
- **Leader完整性**：如果一个Log是已提交的，那么该Log将会出现在任何更高Term的Leader日志中。
  - 论证：
    1. 首先，已提交的Log是指大多数节点都存在此Log，称这个Log为LogX。
    2. 其次，成为Leader需要通过过半仲裁。那么持有LogX的节点和投赞成票的节点会**有交集**，至少存在一个拥有LogX的节点投赞成票。
    3. 所以可以认为当前Leader拥有LogX。
- **状态机安全**：如果一个服务器在给定Index上应用了一个Log到它的状态机，那么任何其他服务器会应用这个Index里相同的Log。（我的理解：在同一个已提交的Log的Index上，不会存在另一个不同内容的Log在相同Index上提交）
  - 论证：TODO




#### Log压缩 TODO



## Lecture 5 - Raft: Fault Tolerance I

在之前的三个多副本系统中，都或多或少地使用了单节点去决定某个副本谁是主。

使用单节点来决定的优势在与，单节点的决策不会出现矛盾。但同时单节点又面临了单点故障的问题（Single Point of Failure）。正因为单节点没有决策矛盾，所以它被用来处理脑裂的场景：当局部故障出现后（主备heartbeat断开），应决定谁是主备份。

### 脑裂：为什么要用单节点决定主备份？

> - MapReduce复制了计算，但是复制这个动作，或者说整个MapReduce被一个单主节点控制。
>
> - GFS以主备的方式复制数据。它会实际的复制文件内容。但是它也依赖一个单主节点，来确定每一份数据的主拷贝的位置。
>
> - VMware FT，它在一个Primary虚机和一个Backup虚机之间复制计算相关的指令。但是，当其中一个虚机出现故障时，为了能够正确的恢复。需要一个Test-and-Set服务来确认，Primary和Backup虚机只有一个能接管计算任务。
>
> 这三个例子中，它们都是一个多副本系统（replication system），但是在背后，它们存在一个共性：它们需要一个单节点来决定，在多个副本中，谁是主（Primary）。

此处可以理解为如何设计分布式锁，首先回忆一下处理竞争的正确方式：让某个节点知晓自己优先请求到资源，而且其他节点也要能确认存在某个节点首先请求到资源。

假设设置一个多副本的TestAndSet服务（S1和S2），同时其中的多副本没有建立主备复制机制。即每次client（C1和C2）都要访问某个server去选主。这里我们考虑两个场景：

1. 每个client都需要访问所有server，如果响应一致则选主成功。这其实对提高系统容错性没有帮助，两台服务器都需要正常运行且网络链接正常。
2. 每个client只需要访问某个server，如果某个响应成功，则选主成功。但这又是一个典型设计错误：不安全请求顺序对多副本系统不确定。

<details>
<summary>（一个不切实际的畅想）</summary>
> （思考：但如果多副本的TestAndSet服务之间建立了主备机制，那么两个备份系统同时挂掉的可能性会降低么？）
>
> 分析故障问题和解决手段：
>
> - 存在一些client与S1网络不通，而其余client与S1相通。
>   - 若client之间能够网络互连：总存在一个client能够成为主备份，那么让该client广播通知其他client。若某个client首次受到其他client选主成功的告知，那么转发；如果不是首次受到，那么不转发。<u>半数以上通过即成功。</u>结果就是每个能链接其他client的client都会受到至少一个选主告知。
>     - 如果存在某个client与所有client网络不互联：那么等待该client与选主单节点的网络回复，或受到某个client的选主告知后即可回复一致性。否则，其他client可认为该client已经挂掉，不可用。
> - S1机器挂掉 / 不存在与S1网络相通的client （可以认为这两个是同一个场景）
>   - 所有client在某个Timeout后使用上面的共识算法，告知各个client。当某个client知道所有client都链接不了server（类似OSPF通告向量），那么向S2请求选主。
>     - 若S1网络依然断开（S2与S1交换状态信息发现S1滞后），那么触发主备切换，S1停止接受请求，S2发送选主成功响应，同时通知S1S2成为新的主。
>     - 若S1网络恢复（S2与S1交换状态信息发现S2滞后），S2发送选主失败响应。
> 
> （思考：选主策略的一种工程上的考虑）
>
> - client查询当前时间，广播通告选择最小时间的client做主，<u>半数以上通过即成功</u>
> - client广播自己的唯一编号，通告选择最小标号的client做主，<u>半数以上通过即成功</u>
</details>

上世纪80年代排除脑裂的两种技术：

1. 构建一个不可能出现故障的网络。不可能出现故障的网络一直在我们的身边：连接了CPU和内存的线路就是不可能出现故障的网络。如果客户端不能与一个服务器交互，那么这个服务器肯定是关机了。
2. 人工检查问题。



### 过半仲裁 Quorum

如果服务器的数量是奇数的，那么当出现一个网络分割时，两个网络分区将不再对称。且必然不可能有超过一个分区拥有过半数量的服务器。（所有服务器数量的一半，而不是当前开机服务器数量的一半）。如果系统有 2 \* F + 1 个服务器，那么系统最多可以接受F个服务器出现故障，仍然可以正常工作。

在过半票决这种思想的支持下，大概1990年的时候，有两个系统基本同时被提出。这两个系统指出，你可以使用这种过半票决系统，从某种程度上来解决之前明显不可能避免的脑裂问题。两个系统中的一个叫做Paxos，Raft论文对这个系统做了很多的讨论；另一个叫做ViewStamped Replication（VSR）。尽管Paxos的知名度高得多，Raft从设计上来说，与VSR更接近。



### 模块化的Raft

Raft会以库的形式存在于服务中。从软件的角度来看，我们可以认为在某个节点的上层是有状态的应用程序代码，下层是raft模块。我们拿带raft的kv数据库做一个例子：

假设客户端将请求发送给Raft的Leader节点，在服务端程序的内部，应用程序只会将来自客户端的请求对应的操作向下发送到Raft层，并告知把这个操作提交到多副本的日志中，并在完成时通知我。**当且仅当Raft的Leader知道了过半节点的副本都有了这个操作的拷贝之后**。Raft层会向上发送一个通知到应用程序说：刚刚的操作，我已经提交给所有副本，现在你**可以真正的执行这个操作**了。



### 日志在Raft中的作用

1. 保证操作顺序性：Log包含的term和index，对系统而言用于保证log的顺序性。每个log拥有唯一的term和index。
2. 临时存储：Follower需要在确定操作被committed，才能将其应用到副本状态机中。有些未提较的log可能在未来切换leader后，被替换而不复存在。
3. 备份重传：当新机器加入集群中时，状态中的log entry是空的。所以leader需要给它传输先前已有的数据。
4. 状态恢复：Log存储于非易失介质中，当节点重启后可以通过从头执行log，来恢复副本状态机的内存信息。

> 1. 假设Leader每秒可以执行1000条操作，Follower只能每秒执行100条操作，并且这个状态一直持续下去，会怎样？
>
>    Leader将会是系统中最新的副本，同时Follower的log将无限增长。**Raft中并无流控机制**（流量控制，并非拥塞控制）。所以生产环境下，如果需要最大化系统性能，Leader需要加入流控能力控制速度。
>
> 2. **当系统所有节点宕机会发生什么？**
>
>    重启后，进行Leader选举。Leader会在下一个AppendEntry操作时，按照nextIndex（初始化为LastLogIndex）发送给Followers。此时Follower应当检查prevLogTerm和Index来确认最近一次Log是否为系统的最新Log。若是则发送success，Leader在多数成功下更新commitIndex。若否，则Leader**回退prevLog**Term和Index来检验该Follower最近Log，并计算最近的commitIndex。
>
>    当commitIndex确认后，Raft将命令发送给上层应用。应用程序需要**按序进行每一个Log的操作**。
>
>    加粗的两个操作是最消耗时间的机制，Raft对这两种问题也提出了解决方案。



### Leader在Raft中的应用

> 1. 在单向的网络出现故障情况下，raft如何工作？
>
>    Leader将收不到自己心跳包响应，导致没有log可以提交，同时没有新的Leader被选举出来。解决这个问题的方法是，Leader在受不到一定的响应后自动卸任（不发送心跳包，可能是进入Follower状态）。这样新的Leader将会出现。



### 一些零散的注意点

1. 对于get请求，需要leader做一次半数以上的AppendEntry，才能保证当前leader是系统唯一leader。这样才能保证读到的commit操作结果不是脏数据（旧数据）。
2. 对于client的put和get请求，client最少需要等待两倍RTT才能获得响应：一个RTT是client和leader间的往返，一个RTT是leader与半数follower并发AppendEntry的往返。
3. 对于raft暴露出来的接口，事实上只有一个：Start(command)和对应的ApplyMsg{command, logIndex, valid?}响应通道。课程在这里选择使用异步的方式，让应用程序等待commited logs。
4. 在任意一个任期内，每一个节点只会对一个候选人投一次票。这样，就不可能有两个候选人同时获得过半的选票，因为每个节点只会投票一次。所以这里是过半原则导致了最多只能有一个胜出的候选人，这样我们在每个任期会有最多一个选举出的候选人。