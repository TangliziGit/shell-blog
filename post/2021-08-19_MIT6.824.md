# Notes on MIT 6.824

> 1. MIT6.824 Schedule: https://pdos.csail.mit.edu/6.824/schedule.html
> 1. MIT6.824分布式系统课程: https://www.zhihu.com/column/c_1273718607160393728
>

明确一下目标，学习这个课程是为了深入了解分布式应用的概念和指标（为什么使用分布式、困难是什么、一致性是什么等问题），以及现有分布式应用（MapReduce、GFS等）和主流问题解决方案（主备容灾细节、Raft等）。

- 关于分布式的概念，会在每个Lecture里记录下来。如果缺失了某个lecture，那么说明这节里没有什么需要注意的地方。
- 对于paper里的关键点，则会专门开一个标题去记录重点。
- 对于每个lecture，最好有一句话能够总结整个章节



## Lecture 1 - MapReduce：分布式计算 - 计算的抽象、分发和容错

### 分布式系统的目的 & 困难

1. 并行以提升**性能**
2. **错误容忍**（冗余计算、冗余存储）
3. 物理原因
4. 安全与隔离（例：区块链）

分布式系统困难的原因：

1. 并行问题
2. 局部故障问题
3. 性能达标：并非机器加更多机器就有更好性能

### 分布式系统的三个抽象

- 存储
- 计算
- 通信

### 分布式系统的话题

- 实现：RPC方法、多线程技术、并发控制
- 可扩展性：系统应当可以通过增加机器来提高系统性能。我们希望通过增加机器的方式来实现扩展，但是现实中这很难实现，**需要架构设计来将可扩展性持续推进下去**。
  - 例：当用户量上涨，如何提高web系统性能？
    1. 部署多个web服务器，直到瓶颈从吞吐量转变为数据存储
    2. 数据库分库分表，需要大量工作

- 容错性：在成百上千台服务器上运行系统，故障将变为常态。常见的容错评判机制是可用性和自愈性
  - 一些提高容错性的手段：使用非易失性存储，建立副本机制在故障时切换（数据冗余）

- 一致性：强调强一致性的CAP、强调弱一致性的BASE
  - BASE是对CAP的：基本可用（Basically Available）、软状态（Soft State）和最终一致性（Eventually Consistent）


## Paper：MapReduce

> https://pdos.csail.mit.edu/6.824/papers/mapreduce.pdf

1. Core design
   - Users specify a map function that processes a key/value pair to generate a set of intermediate key/value pairs, and a reducefunction that merges all intermediate values associated with the same intermediate key.
2. Target
   - hides the messy details of parallelization, fault-tolerance, data distribution and load balancing in a library
3. Why use map reduce primitives?
   - parallelize large computations easily
   - to use re-execution as the primary mechanism for fault tolerance

### Model

- map(k1, v1) -> list(k2, v2)：接受kv二元组，产出中间kv对
  - 可以理解成按照k2聚合列表v2
- reduce(k2, list(v2)) -> list(v2)：合并中间kv对
  - 内部使用Emit处理结果

在业务上，MapReduce简化了在k2上聚合逻辑。

**Examples**

- 分布式Grep
- 倒排索引：map接受文件名和内容，产出词和文件名，reduce处理词和文件名列表
- 分布式排序：TODO：拆分数组，map接受子数组号，产出字数组号和已排序数组，reduce处理子数组k路归并

### 执行流程

![mapreduce](/static/image/2021-08-30/mapreduce.jpeg)

1. 用户程序首先将输入文件分割成若干份确定大小的分区，然后在worker节点和master节点执行用户程序。
2. master节点分配map和reduce任务
3. map节点读取分区数据，产生**中间kv，存储在内存中**
4. map节点**周期性存储中间数据于磁盘**，在map完成后向master通知输出R个文件的位置和大小。master更新数据结构，通知reduce处理新的文件
5. reduce节点受到通知后，通过RPC读取map本地数据。当读取完所有map数据后（全部结束？），所有数据按key排序。
6. 用户程序的reduce函数开始执行。reduce的输出以追加方式写入结果文件，当reduce结束则原子性修改文件名。
7. 全部map reduce执行完毕后，master开始执行用户程序

**注意**：

1. map本地文件会生成几个？

   1. R个。当所有map结束后，reducer依次拉取各个map本地数据做reduce。

2. map和reduce函数是同时执行么？即reduce读map的一部分周期提交数据进行处理？

   - 阅读后文发现，不是同时执行的。

     > When a map task completes, the worker sends a message to the master and includes the names of the R temporary files in the message.

   - 注意map函数不可以与reduce节点的排序同时执行，因为reduce函数需要等待某个key结束了map，然后再做数据的排序。

3. 为什么reduce函数执行前必须要排序？

   - 排序后的kv对，可以在同一时刻只reduce一个list。当所有list被reduce后，一次性写入输出文件并原子命名。
   - 如果不进行排序，将同时处理大量不同的key，会需要大规模的线程（内存、切换时间）、内存（极端情况是所有数据）

4. master维护什么状态？

   - 各个节点的状态机模型：idle、in-progress、completed
   - 节点的identy信息
   - 每个完成的map产出的文件位置和大小

### 容错机制

#### Worker 容错

master会周期性ping各个worker，如果失败则说明节点失败。

| 任务类型 \ 崩溃时当前任务状态 | 正在执行                         | 执行完成            |
| ----------------------------- | -------------------------------- | ------------------- |
| map                           | 重新分配节点，该节点返回idle状态 | 重新分配节点执行map |
| reduce                        | 重新分配节点，该节点返回idle状态 | 不做处理            |

**注意**：

1. 为什么map在失败后，不能使用当前任务输出的中间kv？
   - 因为中间kv是存于内存，同时周期性写入的文件只是本地文件。也因此map节点失败后，只能重新分配并执行。
2. 为什么reduce在执行完成后，不用处理失败？
   - 因为reduce输出文件是全局公用的，reduce节点失败不影响存储。



#### Master 容错

- **方案一：恢复数据** - 将所有状态存储checkpoint，失败后恢复即可
- **方案二：let it crash** - 单节点的master不太可能失败，若失败则用户自行处理



#### 原子性提交

- 当map和reduce函数是确定性的（纯函数），那么节点通过提交原子性commit，来达到无错误的顺序执行。
  - map节点执行成功后，向master发送信息。master将忽略已经执行完的任务的信息，以避免冗余。
  - reduce节点执行成功后，依靠操作系统的原子性重命名（mv）来提供原子性。
- 当map和reduce函数是非确定性的（非纯函数），那么会提供更弱的方式。具体是什么，论文似乎没有提到。



### 调优：备份任务

一个使运行时间变长的常见原因是某个任务掉队。

原因有很多，比如硬盘速度太慢、CPU缓存禁用等。

解决方法是增加备份机制，当某些任务执行时长太久，则新分配i一个节点给此任务。当主备某一个完成后，则其他任务终止。



## Lecture 3 - GFS：海量分布式存储 - 单节点维护元数据、数据节点处理读写、数据副本

### 分布式存储难点：性能与容错的权衡

使用分布式的系统，从目标来看都是为了将数据分片（将数据分割放到大量的服务器上，从而并行取数据）以达到性能提升。

但是如果你在成百上千台服务器进行分片，你将会看见常态的故障。在许多系统中会为了容错而降低性能，这就说明分布式系统需要在性能和一致性之间做权衡。

- 提升性能 -> 引入分布式 -> **数据分片**
- 系统存在局部错误 -> **支持容错能力 -> 引入数据副本 -> 提供副本间一致性能力** -> 降低性能



### 关于一致性的错误设计：不安全请求的顺序

课程中介绍了分布式kv存储设计为多写的场景：你无法保证每个replica能够接收到完全相同顺序的写请求，这会导致副本状态不一致问题。

这里可以推广为分布式系统需要处理不安全请求（影响系统状态的请求）顺序问题。你可以设计保证不安全请求在多个replica上顺序完全相同，也可以设计为只有一个replica接受不安全请求。



## Paper : GFS

> https://pdos.csail.mit.edu/6.824/papers/gfs.pdf





## Lecture 4 - VMware FT：Primary Backup Replication - 主备复制的方案、关注点和典型故障场景

> The Design of a Practical System for Fault-Tolerant Virtual Machines: https://pdos.csail.mit.edu/6.824/papers/vm-ft.pdf

VMware FT(Fault Tolarence)是指两个虚拟机的主备容错。它需要两个物理服务器，Primary虚机在其中一个物理服务器上，Backup在另一个物理服务器上。（将Primary和Backup运行在一台服务器的两个虚拟机里面毫无意义，因为容错本来就是为了能够抵御硬件故障）

### 两种备份方法 / 主备方案

这里介绍了两种复制方法：

1. State Transfer
   - 是指primary将自身**内存中的状态信息**定期发送给backup存储。当primary故障后，再从backup进行回复。
   - 需要注意的是，每次状态迁移需要进行一次大拷贝，但也可以做diff来减少传输量
2. Replicated State Machine
   - 将系统受到的一系列**外部输入**发送给backup进行存储。这基于一个事实：两个相同状态的系统在受到完全一致的外部输入后将保持一致、互为副本。

我们倾向于Replicated State Machine，是出于**数据传输量**的考虑：State Transfer显然需要更多的数据来做备份，而后者则只需存储client发送来的指令。但Replicated State Machine**需要对系统作出大量的假设**，较为复杂；而State Transfer只需要暴力存储状态即可。

当前的Replicated State Machine是在单核CPU下的方法：在**多核CPU下相同的指令并非造成相同的结果**。VMware后续推出了新的方法解决并行情况，但方案更倾向于状态迁移。

> 1. 在备份不一致的情况下，primary故障回复后导致混乱的问题如何处理？
>    - 后面的工作原理讲到：当backup确认受到外部输入后，再进行下一步工作（即输出控制）。所以避免了备份不一致的情况。属于强一致性。
> 2. 在Replicated State Machine中，随机操作如何处理？

### 主备同步的问题

一些主备同步中的重要问题：（不仅仅集中在Replicated State Macine方案上，其他方案也需要考虑）

- **同步级别、同步频率、状态定义**
  - 在VMware中，主备的状态是指primary内存中的每个bit，即主备在底层也是完全一致的。很少有系统如此设计备份方案，因为它过于困难（甚至需要考虑中断在主备中同一个位置进行）。他的优点是，在VMware FT支持的微处理器上，任何一个可运行的软件都可以具备容错性。你不需要考虑软件的任何逻辑和源码。
  - 然而大部分的系统类似GFS，备份数据是指应用程序级别数据chunk，每个针对chunk都保存有默认3个副本在不同的机架和服务器上。GFS只需要保证chunk副本的一致即可。
- **主备切换**：当系统primary故障后进行主备切换，在理想情形是应当没有任何客户端会注意到这里的切换。在切换过程中，必然会有异常，我们必须找到一种应对它们的方法。
- **备份故障**：当两个备份其中一个故障后，应当尽快上线新的备份避免所有备份宕机。
  - 注意：创建新的副本需要较大代价。因为此时新的副本没有一致的内部状态，所以创建时只能进行状态迁移，不能采用复制状态机。

### 工作原理

在真实场景下，一个局域网中存在着primary宿主机和backup宿主机，他们分别运行着primaryVMM和backupVMM (VMM，Virtual Machine Monitor)，用于监控主备VM。同时这个局域网中还存在者一些client（此处用于VM存储的disk server也可以算为client）。

#### 主备同步时机与流程

首先讲<u>主备同步的时机，并如何进行主备同步</u>。当client向primary发送一个请求分组后，将触发**primary host的中断，之后这个中断将数据送给VMM**。此时VMM可以发现该分组是需要发送给primary vm的，于是VMM开始如下两个操作：

1. 向**本地（primary）的vm模拟网络请求中断**，将数据发送给primary vm的应用程序中。（当primary VMM收到处理完的primary输出，并收到到backup的ACK后，再发送响应。这个过程被称为输出控制）
2. **向backup host发送一个相同的网络请求**，之后backup VMM将可以受到此分组。

此时主备vm都受到了相同的外部输入，他们会以相同的方式处理外部输入，并最终达到状态一致。最终，primary将网络响应发送给client，backup因为知道自己是备份所以丢弃响应。（论文中的Log Channel就特指局域网中priamry向backup VMM发送外部输入的信道）

#### 主备切换流程

其次，讲<u>主备切换</u>的过程。实际场景中，backup能够在一秒内受到很多条log，有一部分是primary的定时器中断（大概100次每秒）。当backup没有在一段时间内受到primary的定时器中断后，说明primary出现失败，需要主备切换了。

1. 首先backup不再接受来自primary VMM的log，而是接受网络输入作为外部输入源，同时不再丢弃输出分组。
2. backup在网络中做一些处理（？），使得client都转而访问backup。

> 1. Backup怎么让其他客户端向自己发送请求？
>
>    - Robert：（ARP欺骗）每个虚拟机也有一个唯一的MAC地址，当Backup虚机接手时，它会宣称它有Primary的MAC地址，并向外通告说，我是那个MAC地址的主人。
>
> 2. 在Replicated State Machine中，随机操作如何处理?
>
>    - VMware FT的设计者认为他们找到了所有类似的操作，对于每一个操作，Primary执行随机数生成，或者某个时间点生成的中断（依赖于执行时间点的中断）。而Backup虚机不会执行这些操作，Backup的VMM会探测这些指令，拦截并且不执行它们。VMM会让Backup虚机等待来自Log Channel的有关这些指令的指示。
>    - 关于设计伪随机：应该不能要求primary和backup使用同一个seed做随机。因为随机数场景在于各类加密算法，两台机器随机数完全相同可能打破一些算法的前提。

### 非确定性事件 （Non-Deterministic Events）

> https://zhuanlan.zhihu.com/p/190779044

通常情况下，代码执行都是直接明了的，但并不是说计算机中每一个指令都是由计算机内存的内容而确定的行为。这一节，我们来看一下不由当前内存直接决定的指令。这些指令在Primary和Backup的运行结果可能会不一样。这些指令就是所谓的非确定性事件。非确定性事件可以分成几类：

1. 客户端输入。
   - 当我们说输入的时候，我们实际上是指接收到了一个网络数据包。而一个网络数据包对于我们来说有两部分，一个是数据包中的**数据**，另一个是提示数据包送达了的**中断**。
   - 当网络数据包送达时，通常网卡的DMA（Direct Memory  Access）会将网络数据包的内容拷贝到内存，之后触发一个中断。操作系统会在处理指令的过程中消费这个中断。对于Primary和Backup来说，这里的步骤必须看起来是一样的，否则它们在执行指令的时候就会出现不一致。所以，这里的问题是，**中断在什么时候，具体在指令流中的哪个位置触发**
2. 怪异指令：有一些指令在不同的计算机上的行为是不一样的
   - 随机数生成器
   - 获取当前时间的指令，在不同时间调用会得到不同的结果
   - 获取计算机的唯一ID
3. 另外一个常见的非确定事件，即多CPU的并发。
   - 当服务运行在多CPU上时，指令在不同的CPU上会交织在一起运行，进而产生的指令顺序是不可预期的。
   - 另外，这里探究了论文未提即的log条目内容，教授猜测有三样内容：
     - 事件发生时的指令序号。因为如果要同步中断或者客户端输入数据，最好是Primary和Backup在相同的指令位置看到数据，所以我们需要知道指令序号。这里的指令号是自机器启动以来指令的相对序号，而不是指令在内存中的地址。比如说，我们正在执行第40亿零79条指令。所以日志条目需要有指令序号。对于中断和输入来说，指令序号就是指令或者中断在Primary中执行的位置。对于怪异的指令（Weird  instructions），比如说获取当前的时间来说，这个序号就是获取时间这条指令执行的序号。这样，Backup虚机就知道在哪个指令位置让相应的事件发生。
     - 日志条目的类型
     - 数据

> 1. 如果Backup领先了Primary会怎么样？
>    - 它会维护一个来自于Primary的Log条目的等待缓冲区，如果缓冲区为空，Backup是不允许执行指令的。如果缓冲区不为空，那么它可以根据Log的信息知道Primary对应的指令序号，并且会强制Backup虚机最多执行指令到这个位置。所以Backup总是落后于Primary至少一个Log。
> 2. 能不能输入送到Primary，输出从Backup送出？
>    - 这是一个很有意思的方法。Backup输出说明它已经完成一致性任务，不过primary仍需要获知backup是否收到log。如果没有primary没有感知到backup收到log，那么primary应该继续执行么？当然不应该，否则会出现不一致情况。

### 主备切换 & 主备故障的场景

#### 副本不一致下的主备切换

当primary VMM收到处理完的primary输出，并收到到backup的ACK后，再发送响应。这个过程被称为输出控制。

这属于强一致性，意味着网络消耗成为影响性能的重要因素。

#### 主备切换后的重复响应

当Backup的Log缓冲区仍有很多剩余未处理时，Primary由于故障而Backup触发接管机制。这时Backup的外部输入Log将引导vm发出重复的响应。

但是需要注意到，副本在TCP层面也进行了复制。这说明Backup知晓链接的TCP序列号信息，这样重复响应会被Client的TCP栈抛弃。同时，Backup和Client之间并没有真正保持TCP链接，所以Backup应该会受到Client的TCP Reset响应。（我猜测Backup可以不做处理，这样Client没有受到响应。那么client会有两种反应：请求没有到达primary，所以系统没有状态变化；或者响应没有到达client，所以系统已经发生变化。这本来就是client需要考虑的事情。所以backup在这个情况下，完全可以不处理Reset。）

通常而言，<u>分布式系统基本不可能保证不产生重复输出</u>。这需要其他机制来处理，一种可能是在应用层面设计序列号。

> 1. 主备在网络协议上是完全一致？IP地址都是一样的么？
>
>    需要注意的是，运行VM的物理服务器在网络传输上被设计为透明的。这意味着VM拥有与物理服务器一样的网络协议，它们拥有独立的IP和MAC（IP可能需要在物理机的网段中）。交换机、路由器等网络设备会正常执行它们的操作，并不会注意到VM的存在。

#### 脑裂（Split Brain）

**分布式场景下的定理：你无法判断另一个计算机是否真的挂了**

VMware的解决方法是主备依赖第三方的TestAndSet服务。当主备的网络通信断掉后，双方都会申请成为primary。TestAndSet服务相当于一个锁，它决定了主备之中谁应该成为priamry。

这里TestAndSet服务似乎是单点故障的受害者。但VMware肯定也考虑到这点，所以它应当也是有主备的。
