# [MIT6.824] Lecture8&9 - ZooKeeper


## Lecture 8 & 9 - ZooKeeper(2010)：客户端顺序一致性

> - [一致性模型笔记](https://int64.me/2020/%E4%B8%80%E8%87%B4%E6%80%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0.html)
>
> - http://nil.lcs.mit.edu/6.824/2020/papers/zookeeper.pdf

### 一致性保证：写线性一致性 & 客户端顺序一致性

与线性一致一样，这些保证与序列有关。Zookeeper有两个主要的保证，它们在论文的2.3有提及。总的说来，Zookeeper提供了客户端顺序一致性和线性一致的写。

- Linearizable writes：写请求线性一致
  - 如果一个写请求在另一个写请求开始前就结束了，那么Zookeeper实际上也会先执行第一个写请求，再执行第二个写请求。
- FIFO client order：**FIFO客户端顺序保证**。任何一个客户端的请求，都会按照客户端指定的顺序来执行
  - 写请求：会以这个客户端发送的相对顺序，加入到所有客户端的写请求中。所以，如果一个客户端顺序完成三个写操作，那么在最终整体的写请求的序列中，可以看到这个客户端的写请求以相同顺序出现。对于异步的请求，可以假设，客户端实际上会对它的写请求打上序号。
  - 读请求：**需要注意只经过副本去处理读**。读请求也会像上文的样子，客户端顺序完成三个读操作，最终会看到副本会依次做读请求。注意，处理读请求时Log的长度也会增长，后来的读请求会比之前的能看到更多的Log。

> - 如果一个客户端正在与一个副本读交互，之后这个副本故障了。客户端需要将读请求发送给另一个副本。这时，尽管客户端切换到了一个新的副本，FIFO客户端序列仍然有效。为什么？
>
> 这里工作的原理是，**每个Log条目都会被Leader打上zxid的标签**，这些标签就是Log对应的条目号。任何时候一个副本回复一个客户端的读请求，首先这个读请求是在Log的某个特定点执行的，其次回复里面会带上zxid，对应的就是Log中执行点的前一条Log条目号。**客户端会记住最高的zxid**，当客户端发出一个请求到一个相同或者不同的副本时，它会在它的请求中带上这个最高的zxid。这样，其他的**副本就知道，应该至少在Log中这个点或者之后执行这个读请求**。这里有个有趣的场景，如果第二个副本并没有最新的Log，当它从客户端收到一个请求，客户端说，上一次我的读请求在其他副本Log的这个位置执行。那么**在获取到对应这个位置的Log之前，这个副本不能响应客户端请求。**要么副本阻塞了对于客户端的响应，要么副本拒绝了客户端的读请求并说：我并不了解这些信息，去问问其他的副本，或者过会再来问我。（很像阻塞/非阻塞IO）

**FIFO客户端请求序列是对一个客户端的所有读请求，写请求生效**。所以，如果我发送一个写请求给Leader，在Leader  commit这个请求之前需要消耗一些时间，所以我现在给Leader发了一个写请求，而Leader还没有处理完它，或者commit它。之后，我发送了一个读请求给某个副本。这个读请求需要暂缓一下，以确保FIFO客户端请求序列。读请求需要暂缓，直到这个副本发现之前的写请求已经执行了。这是FIFO客户端请求序列的必然结果，（对于某个特定的客户端）读写请求是线性一致的。

这里还需要提一下`sync(path)`操作，它是指等待在`sync`操作前的所有写操作，本质上是写请求。是一个弥补非线性一致的方法。一般场景是这样的：客户端先发送`sync`再做读请求，那么副本会先sync请求后，再返回读请求。这个读请求可以保证看到sync对应的状态，所以可以合理的认为是最新的。

### 就绪文件

> https://zhuanlan.zhihu.com/p/215451863

我们假设有另外一个分布式系统，这个分布式有一个Master节点，而Master节点在Zookeeper中维护了一个配置，这个配置对应了一些file（也就是znode）。在这里我们需要一种原子效果的更新。因为只有这样，其他的客户端才能读出完整更新的配置，而不是读出更新了一半的配置。这是人们使用Zookeeper管理配置文件时的一个经典场景。那么原子更新是如何实现？

如果就绪文件**Ready file**存在，那么允许读这个配置。如果Ready file不存在，那么说明配置正在更新过程中，我们不应该读取配置。如果Master要更新配置，那么第一件事情是删除Ready file。之后它会更新各个保存了配置的Zookeeper  file（也就是znode），这里或许有很多的file。当所有组成配置的file都更新完成之后，Master会再次创建Ready file。为了确保这里的执行顺序，Master以某种方式为这些请求打上了tag，表明了对于这些写请求期望的执行顺序。之后Zookeeper Leader需要按照这个顺序将这些写请求加到多副本的Log中。

![ready-file](/static/image/2021-12-26/ready-file.jpeg)

所以，如果客户端看见了Ready file，那么副本接下来执行的读请求，会在Ready  file重新创建的位置之后执行。这意味着，Zookeeper可以保证这些读请求看到之前对于配置的全部更新。所以，尽管Zookeeper不是完全的线性一致，但是由于写请求是线性一致的，并且读请求是随着时间在Log中单调向前的，我们还是可以得到合理的结果。

![exists-before-ready](/static/image/2021-12-27/exists-before-ready.jpeg)

这里可能有的问题是，客户端可能会在这个就绪文件存在时，通过调用exist来判断Ready file是否存在。之后，随着时间的推移，客户端读取了组成配置的第一个file，但是，之后在读取第二个file时，Master可能正在更新配置。这样导致客户端读到的是一个不正常的，由旧配置的f1和新配置的f2组成的配置。

但实际上，客户端`exists`不仅会查询Ready file是否存在，还会建立一个针对这个Ready file的watch。客户端在请求`exists(path, watch=true)`后，当副本读到path相关的操作后，首先发送消息通知客户端。所以，`delete`操作会优先于任何写操作到达副本上，任何读操作会在这里被放弃。

> - 关于watch的原理
>
> 假设某个客户端与副本在交互，客户端发送了一个exist请求。相应的副本会生成一个watch的表单，表明哪些客户端watch了哪些file。如果收到了一个删除Ready file的请求，副本会查看watch表单，并且发现针对这个Ready file有一个watch。watch表单或许是以file名的hash作为key，这样方便查找。

### API

Zookeeper的API能够解决什么样的事情？

- Test-and-Set
- 集群通用配置信息：比如发布Master的地址
- 选举Master
- 服务注册

它的API中定位结构类似znode组成的树，znode会有三种类型

- Regular znode：永久的znode，除非删除掉。
- Ephemeral znode：短暂/会话znode，当会话结束时撤销。
- Sequentail znode：顺序znode，在实际创建时会在名字后添加序号。当有多个客户端创建同一个名字znode时，ZooKeeper保证没有命名冲突。

API包括3个写操作和3个读操作，详情可见论文2.2。下面是一些需要注意的地方：

- `create`是排他的操作，当多个客户端创建同一个名字znode时，保证只有一个客户端能受到创建成功的消息，其他客户端会受到失败消息。
- 所有写操作都带有`version`信息（`create`默认0），所以都是基于key级别的原子CAS操作模式。几乎所有读操作都有`watch`选项（除了`list`），当客户端受到文件变动时，需要做同步操作。（？）

这种API可以实现一系列有用的模式：

- 自旋锁：原子CAS提供的getData+setData
  - 需要注意的是这个方式针对客户端数量是`O(n^2)`的。如100个客户端同时请求修改，那么一共需要`n^2`的通信次数
  - 高负载的情况需要合理的等待机制。如raft中的随机等待时间，这时在处理未知数量客户端的合理方式。或者直接用下面的lock。
- 非扩展锁：利用`create(ephemeral=true)`和`exist(watch=true)`。当create成功则获得锁，不成功则exist监听删除变动做等待
  - 注意一定要用会话级别的znode文件。防止客户端宕机不释放锁。
  - 在`create`和`exist`之间如果锁已释放，那么不需要做watch等待，只需要判断exist

