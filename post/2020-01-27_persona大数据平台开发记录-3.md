# persona大数据平台开发记录 - 实时日志收集与传输

本文记录`flume`安装,  配置和使用, 结合`spark streaming`.


## flume安装

```bash
wget http://mirror.bit.edu.cn/apache/flume/1.9.0/apache-flume-1.9.0-bin.tar.gz
sudo tar xvf apache-flume-1.9.0-bin.tar.gz -C /opt/
sudo mv /opt/apache-flume-1.9.0-bin /opt/flume
```


## flume配置

- `conf/flume-env.sh`
```bash
export JAVA_HOME=/usr/lib/jvm/java=8-jdk
```

- `job/file-flume-spark.conf`
采用pull方式进行`spark`和`flume`间交互  
相见<http://spark.apache.org/docs/latest/streaming-flume-integration.html>  
```
a1.sources = r1
a1.sinks = k1
a1.channels = c1

a1.sources.r1.type = exec
a1.sources.r1.command = tail -F /data/data.log
a1.sources.r1.shell = /bin/sh -c

a1.sinks.k1.type = org.apache.spark.streaming.flume.sink.SparkSink
a1.sinks.k1.hostname = localhost
a1.sinks.k1.port = 8888
a1.sinks.k1.channel = memoryChannel

a1.channels.c1.type = memory

a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
```

- 配置`jar`包
将以下包放入`lib/`下
```bash
spark-streaming-flume-sink_2.11-2.4.4.jar
avro-1.8.2.jar
avro-ipc-1.8.2.jar
```


## flume运行

1. 直接运行
```bash
bin/flume-ng agent -n a1 -c conf/ -f job/file-flume-spark.conf -Dflume.root.logger=INFO,console
```

2. 写成`systemctl`服务
`to be continued`


## spark streaming读入

```scala
val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("Persona")
val streamingContext: StreamingContext = new StreamingContext(sparkConf, Seconds(5))

val inputStream: ReceiverInputDStream[SparkFlumeEvent] = FlumeUtils.createPollingStream(streamingContext, "localhost", 8888)

val stream: DStream[String] = inputStream.flatMap(flumeEvent =>
  new String(flumeEvent.event.getBody.array(), "utf-8").split(" ")
)

stream.print

streamingContext.start()
streamingContext.awaitTermination()
```
