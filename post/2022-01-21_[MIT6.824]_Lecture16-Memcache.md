# [MIT6.824] Lecture16 - Memcache

> - https://timilearning.com/posts/mit-6.824/lecture-16-memcache-at-facebook/
> - http://nil.lcs.mit.edu/6.824/2020/papers/memcache-fb.pdf

这一讲主要是关于Facebook如何利用memcached来构建并优化大规模Web系统，

# Prerequisites

## Look-aside cache

> - https://tanzu.vmware.com/content/blog/an-introduction-to-look-aside-vs-inline-caching-patterns

| Pattern          | How it reads                                                 | How it writes                                                |
| :--------------- | :----------------------------------------------------------- | :----------------------------------------------------------- |
| Look-Aside Cache | ![img](https://content.cdntwrk.com/files/aHViPTYzOTc1JmNtZD1pdGVtZWRpdG9yaW1hZ2UmZmlsZW5hbWU9aXRlbWVkaXRvcmltYWdlXzU4OWUzYTBmZjJkMGQucG5nJnZlcnNpb249MDAwMCZzaWc9NGRlMTVhODA3NmFlYTdhNDMxOGU1MmQ0OTc2ZWY2ZDI%253D)<br>Application requests data from cacheCache delivers data, if availableIf data not available, application gets data from backing store and writes it to the cache for future requests (**read aside**) | Application writes new data or updates to existing data in both the cache and the backing store -or- all writes are done to the backing store and the cache copy is invalidated |
| Inline Cache     | ![img](https://content.cdntwrk.com/files/aHViPTYzOTc1JmNtZD1pdGVtZWRpdG9yaW1hZ2UmZmlsZW5hbWU9aXRlbWVkaXRvcmltYWdlXzU4OWUzYTI0MTA5MTUucG5nJnZlcnNpb249MDAwMCZzaWc9ZjAzYzE4NGI2MzkwNjRiYThiNTQ0YjZhMzc2NzBiOTE%253D)<br>Application requests data from cacheCache delivers data if available**Key Difference**: If data not available, cache retrieves data from backing store (**read thru**), caches it, then returns the value to the requesting application | Application writes new data or updates existing data in cacheCache will synchronously (**write through**) or asynchronously (**write behind**) write data to the backing store |

## Consistent hashing

> - https://gardiant.github.io/2019/03/14/%E4%B8%80%E8%87%B4%E6%80%A7hash%E4%B8%8E%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/
> - https://en.wikipedia.org/wiki/Consistent_hashing

一致性哈希算法在 1997 年由麻省理工学院提出，是一种特殊的哈希算法，在移除或者添加一个服务器时，能够尽可能小地改变已存在的服务请求与处理请求服务器之间的映射关系。一致性哈希解决了简单哈希算法在分布式哈希表中存在的动态伸缩等问题 。 具体步骤而言：

- 数据结构：
  - 哈希环：将数据hash编码成一个哈希环。将个服务器抽象地扩充为M个虚拟节点：`[A1, A2, ..., B1, B2, ...]`。将虚拟节点分散地分布（可能根据UUID取模计算）于哈希环上，每个虚拟节点负责存储相邻的左边区间，比如`A2`负责`[A1对应位置, A2对应位置]`的数据。
  - BST：对于查询某个hash对应的服务器，这里使用了BST记录server对应的环位置和serverID。如果没有后继server，那么使用第一个server。
- 对于查询。根据数据hash计算对应哈希环上的位置，通过BST得到某个虚拟节点对应的真实服务器，再进行数据访问。
- 对于数据变化。
  - 加入数据。与查询过程相同，找到对应真实服务器后，向服务器请求添加数据。具体实现可能是拉链法。
  - 删除数据。与查询过程相同，由服务器来执行删除。
- 对于服务器数量变化。
  - 扩展服务器。如添加一台服务器E在`[A, B, C, D]`的服务器中，E的UUID取模后如果是1，那么将在AB中添加一台服务器E`[A, E, B, C, D]`。现在B服务器变为负责EB之间的hash，而E服务器则会负责AE间hash。于是转移者部分数据到E上。
  - 缩容服务器。遇上相同，只是将废弃服务器数据转移到其他服务器上。



## Incast congestion

> - https://www.mtyun.com/library/an-brief-intro-to-tcp-incast
> - https://www.pdl.cmu.edu/Incast/
> - https://www.pdl.cmu.edu/PDL-FTP/Storage/FASTIncast.pdf
> - https://www.orczhou.com/index.php/2011/10/tcpip-protocol-start-rto/

Incast，一般指TCP Incast，是指一种在多对一网络通信中出现的灾难性TCP吞吐量崩溃问题。当一个客户端同时接受到大量服务器的响应时，由于交换机对应端口缓存不能装下所有这些响应，导致部分丢包。客户端于是重发请求，这又进一步导致了交换机缓存溢出。这种现象将会显著减低吞吐量，并且提高客户端的响应时间。

虽然一些TCP协议的优化比如NewReno、SACK等能够稍微提高吞吐量，但是本质上还是会导致Incast问题（见2引用FASTIncst）。所以还需要其他方式来解决这种问题。在引用中提到三种可能的解决方案：

- 提高交换机的缓存大小。不过成本很高。
- 流量控制。当机器在单个交换机上时，以太网流量控制是有效的，但由于线首阻塞，跨交换机中继是危险的。
- 降低最小RTO允许节点以数倍的节点数量维持高吞吐量。我们发现，在真实的集群环境中，提供微秒级别的TCP重传可以实现多达47台服务器的全吞吐量。

![reducing-rto-to-avoid-incast](/static/image/2022-01-21/reducing-rto-to-avoid-incast.png)

# Lecture 14 - memcache(2013)

Facebook的架构包括多个web、memcache和数据库服务器。一组web和memcache服务器组成一个前端集群，多个前端集群组成一个Region。同一个Region内的前端集群共用同一个存储集群。Facebook在世界不同地区复制集群，指定一个地区为主要地区，其他地区为次要地区。

![memcache-architecture](/static/image/2022-01-21/memcache-architecture.png)

## Cluster中的挑战：延时和负载

对于web&cache组成的一个集群而言，主要有两大优化：

- 降低web与cache间的**时延**

  1. 通过数据依赖做**DAG的批请求**，这样能减少网络往返次数。
  2. **UDP读TCP写**。UDP减少了TCP中的安全机制，降低了时延和开销。不过这里UDP应该在应用层做了魔改：添加了序号防止丢包和乱序。
  3. 滑动窗口**解决Incast拥塞**。因为memcached使用一致性hashing来处理分布式哈希，所以web请求将会有大量来自服务器的响应到达客户端上，导致Incast congestion。这里memcache用到自己实现的类似与TCP滑动窗口的流量控制来解决这个问题。论文中讲到不同于TCP窗口只针对一个流，他们的可以针对目标不依赖的窗口。

- 降低cache与存储间的**负载**：减少因为cache missing导致的负载问题。memcache使用的是数据修改租约lease

  - 针对**过期写**。指当web服务器在memcache中为一个键设置了一个过时的值。常见于对键的并发更新被重新排序的情况下。当客户端遇到cache miss时，memcache服务器将给它一个lease。当同步的数据写回cache时，它必须提供这个token供memcache验证。而且当memcache收到该键的删除请求时，它将使该键的任何token失效。

  ```
    key 'k' not in cache
    C1 get(k), misses  (a token should be assigned here)
    C1 reads v1 from DB as the value of k
      C2 writes k = v2 in DB
      C2 delete(k)  (recall that any DB writes will invalidate key in cache)
    C1 set(k, v1)  (validate the token; if not then ignore it)
    now mc has stale data, since delete(k) has already happened
    will stay stale indefinitely until k is next written
  ```

  

  - 针对**惊群效应**。这里的惊群效应体现在很多请求在互相抢夺资源做同一件事：许多客户端尝试为一个无效的键读取数据，导致将有很多写回请求打到memcached上。为了防止它，memcache服务器每10秒只给每个键一次租期。如果在租约发出后10秒内又有另一个客户端请求密钥，则该请求将不得不等待。

  > **惊群效应**：当大量等待某个事件的进程或线程在该事件发生时被唤醒，但只有一个进程能够处理该事件时，就会出现这种问题。当进程醒来时，<u>它们将各自尝试处理事件，但只有一个会获胜。所有进程都将争夺资源，可能会冻结计算机，直到群体再次平静下来</u>



## Region中的挑战：复制

因为cache是无状态的，所以可以像web一样做弹性分配。随着系统负载增加，他们可以通过在前端集群中添加更多的memcache和web服务器来扩展他们的系统，并进一步划分key set。然而，这有两个主要的局限性:

- 随着memcache服务器数量的增加，Incast拥塞将变得更糟，因为客户端必须与更多的服务器通信。

- 分区对热键并没有多大帮助，因为单个服务器需要处理该键的所有请求。在这种情况下，复制数据可以帮助我们在不同的服务器之间共享负载。

**Region Pool**

数据集中不常被访问且规模较大的项目很少需要复制。对于这些键，有一个适当的优化，每个区域只存储一个副本。

Facebook将这些密钥存储在一个区域池中，该区域池包含一组memcache服务器，由多个前端集群共享。这比具有低访问速率的过度复制项更有效。

**Cold Cluster Warmup**

当一个新的前端集群上线时，任何对它的请求都会导致缓存丢失，这可能会导致数据库过载。Facebook有一个名为Cold Cluster Warmup的机制来缓解这种情况。

>  Cold Cluster Warmup通过允许“冷集群”(即拥有空缓存的前端集群)中的客户端从“暖集群”(即拥有正常命中率缓存的集群)而不是持久化存储中检索数据来缓解这一问题。



## Region间的挑战：一致性

Memcache指定一个Region含有主数据库，因此所有写操作都必须进入的主Region的数据库，而其他Region则只包含数据存储的只读副本。它们使用**MySQL的复制机制**使复制数据库与主数据库保持同步。这里的关键挑战是保持memcache中的数据与主数据库一致，而主数据库可能位于另一个区域。

![mysql-replication](/static/image/2022-01-21/mysql-replication.png)

这里考虑到用户不会在意他们会偶尔看到老旧数据，所以为了性能考虑Facebook会让从Region看到轻微的老旧数据。所以这里的目标是减少老旧数据的窗口，并保证所有region是最终一致性的。

- 对于主Region的写请求
  - 写操作直接发送到区域中的存储集群，然后存储集群将其复制到次要区域。如果在将更改复制到这些区域时存在延迟，次要区域中的客户端可能会为这里修改的任何键读取过时的数据。
- 对于从Region的写请求

```
  Key k starts with value v1
  C1 is in a secondary region
  C1 updates k=v2 in primary DB
  C1 delete(k)  (in local region)
  C1 get(k), miss
  C1 reads local DB  -- sees v1, not v2!
  later, v2 arrives from primary DB (replication lag)
```

这违反了写后读一致性的保证，远程标记可以用于防止这种情况：

1. 在Region Pool中设置 remote marker $r_k$。可以将$r_k$看作一条memcache记录，它表示键k的额外信息。

2. 将写操作发送到主区域，并在请求中包含$r_k$，以便主区域在复制写操作时知道将$r_k$作废。

3. 删除本端集群中的k。

于是web服务器下一次对k的请求将导致缓存丢失，之后它将检查区域池来找到rk。如果rk存在，则意味着本地区域中的数据已经过时，服务器将把读操作定向到主区域。否则，它将从本地区域读取。
