# [MIT6.824] Lecture14 - FaRM: Optimistic Concurrency Control

> - https://timilearning.com/posts/mit-6.824/lecture-14-occ/
> - http://nil.lcs.mit.edu/6.824/2020/papers/farm-2015.pdf
> - http://nil.lcs.mit.edu/6.824/2020/papers/farm-faq.txt

这一讲是针对微软提出的主存计算平台的研究原型FaRM（Fast Remote Memory）。它支持分布式事务，并提供乐观并发控制，宣称没有妥协地实现了严格可串行化、高性能和高可用。

FaRM主要利用了两个偏向硬件的技术来做性能优化：

- 使用单边RDMA绕过CPU和系统调用来进行数据读取。
- 利用NVRAM做正常工作时的数据存储，并提供了较廉价的方法（？）。

接下来我们来讨论一下FaRM是如何进行优化的。

# Lecture 14 - FaRM(2015)

首先，FaRM系统是在一个数据中心中被组织的。这个数据中心将包含一个配置管理器ZooKeeper节点，一些主节点和备份节点。注意这里的备份节点不参与读操作，只会接受作为备份的写操作，这很像VMware的FT而不是Raft类的复制算法来支撑它。其中ZooKeeper用来记录各个存活节点的角色。

下面介绍FaRM的一些特性：

- 所有数据存储于RAM之中，在数据读取过程中没有任何硬盘操作；同时使用NVRAM，所以没有任何的硬盘写操作。这降低了系统硬盘IO的性能瓶颈。
- FaRM的事务和复制协议都使用了单边RDMA，它能够绕过CPU来对某个服务器主存进行快速访问能力。这降低了CPU和内核系统调用的性能瓶颈。

这两种特性分别带来了不同的挑战，下面分别讨论一下FaRM是如何解决的。



## 降低瓶颈：使用NVRAM和RDMA

**NVRAM的挑战**

FaRM使用RAM来存储数据，它最大的缺点在于存储节点宕机后数据会消失。FaRM分情况应对这个问题：

- 对于断电的情况，分布式UPS被用来解决问题。当节点断电后，服务器立即切换UPS电源，FaRM此时通过SSD进行数据持久化。恢复供电后FaRM可以根据镜像来恢复内存。
  - 分布式UPS是平行于集中式UPS的概念，它指每个服务器/机架边会有一个UPS。分布式UPS比集中式好的地方在于，它排除了集中式UPS会全体失效的问题。
  - 这里也可以使用NVDIMM来解决，不过FaRM使用了更廉价的方法。
- 对于其他导致宕机的情况，如硬件崩溃、软件缺陷等，FaRM选择将副本节点的主存数据读出。



**RDMA的挑战**

> - https://www.mimuw.edu.pl/~iwanicki/courses/ds/2017/presentations/group-1/05_Lysiak.pdf

一个通用的计算机网络通信方式如下图。数据需要经过内核的多次系统调用和中断层层包装，然后在网卡上发送对应的二进制位到网络上，最终还要在目标机上重复这个过程才到达目标应用程序。

![networking-before-rdma](/static/image/2022-01-17/networking-before-rdma.png)

这个过程中需要大量的CPU计算，包括系统调用、中断、和函数帧跳转等。FaRM提出了两个想法来降低这里的消耗：

- 绕过内核。应用程序直接与网卡通信，避免系统调用。
- 使用单边RDMA。使用RDMA，对某个服务器的主存进行读写可以绕过CPU。

![networking-after-rdma](/static/image/2022-01-17/networking-after-rdma.png)

但回想Spanner对分布式事务的实现，其中2PC阶段大量需要CPU来进行申请锁、检查快照版本号等工作。这里直接读取主存势必导致一致性不能达到严格序列化，比如旧事务能读到新事务的数据。FaRM用一种乐观并发控制来处理，它只会用到读写者的CPU。

## 乐观并发控制

下面是FaRM提供的API。这些API非常简单，需要注意的是最后的commit会返回Bool值，表示事务是否提交。如果没有提交，那么可以使用while循环进行重试。

![farm-api](/static/image/2022-01-17/farm-api.png)

在深入了解事务如何工作的细节之前，请注意每个FaRM机器都维护作为事务日志或消息队列使用的日志。这些日志用于FaRM集群中发送-接收对之间的通信。发送方使用单边RDMA写入将记录追加到日志中，接收方通过其网卡确认，而不涉及CPU。接收方通过周期性轮询其日志的头部来处理记录。

在执行过程中，任意一个应用程序线程都可以开启一个事务，同时该线程会作为事务协调者来直接与副本来进行交流。当事务需要提交时，应用程序会调用FaRM走提交过程。整体流程见下图：

![farm-commit-protocol](/static/image/2022-01-17/farm-commit-protocol.png)

接下来我们详细对6个过程来讨论。

### Execute phase

执行阶段，对读请求来说，TC会直接用单边RDMA向对应主节点请求数据；对写请求而言，TC需要在本地缓存变动。如果数据在本地存储，那么TC直接进行数据读写，而不通过RDMA。

注意，TC会记录所有它访问的数据的地址和版本号，它们会在接下来的阶段用到。接下来可以进入Commit阶段了。

### Commit phase

![farm-arch](/static/image/2022-01-17/farm-arch.png)

首先介绍每个节点的主存结构：

- Region。主存的第一项就是存储数据分片的region，它被分配有2GB，并被复制到包括主节点和备份节点在内的1+f台机器上。
  - 在Region中，每个数据对象都有一个附带的64位版本号和锁标志。并且每个对象都有一个用于索引的地址。
- Ring buffer。主存中还有一个用作FIFO队列的ring buffer，作为接受事务Log的buffer和消息队列。
  - 事务Log在这里会被当作系统的写请求来对待。每个发送者都会通过RDMA向队列尾提交事务Log。

其次，每个FaRM实例都运行着一个事件循环，执行应用程序的代码并轮询RDMA队列。

**LOCK**

注意，LOCK过程只针对事务中的写请求：

- 对写数据，TC通过RDMA向对应主节点发送一个LOCK记录。
  - 这个记录包括TID、所有写数据对应的分片ID、数据地址、数据版本号和新值。
- 当参与者主节点发现LOCK记录后，会在对应版本号上尝试用CAS锁住地址对应的数据，并在对应数据头上标记LOCK字段。接下来主节点发送成功或失败信息。实际上LOCK操作会失败，具体情况如下：
  1. 其他事务已经占用该记录的锁，该数据头上已经标记LOCK。
  2. 版本号没有对齐，导致CAS失败
- 当TC受到所有的LOCK请求成功的消息后，进行下一步骤；如果LOCK操作失败，那么TC会直接撤销事务：向所有参与事务的主节点发送ABORT，来释放占用的锁。

**VALIDATE**

对于事务中的读操作：

- TC通过单边RDMA来重新获取每个对象的版本号及其锁标志的状态，来做提交验证。
  - 如果设置了锁标志，或者版本号在事务读取后发生了更改，那么TC将中止事务。

这种优化避免了在只读事务中持有锁，从而加快了它们的执行速度。

其实在非失败条件下，锁定和验证步骤就已经能够保证所有已提交FaRM事务的严格可序列化性。它们确保执行并发事务的结果与依次执行的结果相同，并且产生结果的顺序与实时一致。具体来说，LOCK+VALIDATE保证:

- 如果事务没有冲突，它读取的对象版本就不会发生变化。

- 如果事务有冲突，它将看到一个改变的版本号（某个事务已经写过它）或它访问的对象上的锁（某个事务正在写它）。

这里我们可以论证一下：

- 对于任何写请求Wx，在当前请求LOCK后并且事务提交结束前，不可能有任何其他写请求起效（因为LOCK占用），也不可能有任何读请求在Validate后读到旧数据（因为版本号改变）。可以说写写&读写之间相当于上了2PL锁。
- 但是如果在Validate过程中，出现其他事务的写请求Wx，并且本事务没有申请x的锁Lx。那么Wx将成功获取锁，Vx也可以成功通过验证。于是本事务提交成功，矛盾出现。
  - 我认为出现这种问题的原因是，Validate和Commit这两个步骤应当是原子性的（V和C之间没有任何W），但是FaRM没有实现它。你也不能在Rx或Vx的时候上锁，因为这里用RDMA绕过了CPU。不过，可以在Validate之前增加一个写性质的LockValidate步骤：让事务参与者根据读请求的地址，在数据上加锁（若加锁不成功则撤销事务）。直到TC提交事务后再释放所有写操作的锁。

```
  T1:                  Wx  C
  T2:  Rx  Ry  Rz  Vx  Vy  Vz  C

(Where L and V represent the Lock and Validate stages)
T1 happens before T2, but T2 does not see T2's result 
```

**COMMIT BACKUP**

但是在出现系统故障时，上述协议不足以保证可串行性。我们可能会遇到这样一种情况：事务在提交到一个主节点后，其他主节点有部分宕机，导致提交时失败了。这违反了事务原子性，事务原子性要求如果一个事务在一台机器上成功，那么它必须在所有其他机器上成功。

FaRM通过使用COMMIT-BACKUP记录来实现这种原子性：在发送COMMIT-PRIMARY记录之前，协调器将向事务中涉及的主节点的所有备份节点发送COMMIT-BACKUP记录，并等待确认在所有备份中被持久化。这个COMMIT-BACKUP记录包含与事务的LOCK记录相同的数据。

通过等待确认从至少一个主节点存储COMMIT PRIMARY记录，TC就能确保系统故障不会影响到事务原子性。

**COMMIT PRIMARY**

- 如果所有的主节点都对LOCK和VALIDATE请求响应成功，那么TC将决定提交事务。它通过在主节点的日志中附加一条COMMIT-PRIMARY记录，将这个决策广播给所有写请求相关的主节点。
  - 这个COMMIT-PRIMARY记录包含要提交的事务的ID。
- 接下来，主节点将事务的LOCK记录中对象的新值复制到内存中，增加对象的版本号，并清除对象的锁标志来处理该记录。

- 当TC受到一个来自主服务器的RDMA确认信息后，TC会直接向客户端报告成功。稍后我们将看到为什么这是安全的。

**TRUNCATE**

TC在接收到来自所有存储了提交记录的主节点的确认后，将发送TRUNCATE请求来截断主节点和备份节点上的日志。
