# persona大数据平台开发记录 - 离线数据处理

使用`spark`将数据从`mysql`中读出, 进行数据处理, 并存入`redis`


## 定义实体类

```scala

case class User (id: Int, name: String, birth: Date, sex: String, area: String, lastLoginTime: Date,
                 termId: Int, courseId: Int, selectCourseTime: Date
                )

case class Reply(id: Long, replyerId: Long, anonymous: Boolean, postId: Long,
             countVote: Int, countComment: Int, replyTime: Date,
             deleted: Boolean, tagAgree: Boolean, tagTop: Boolean, activeFlag: Boolean
            )

case class Post(id: Long, `type`: Int, posterId: Long, postTime: Date, lastReplyTime: Date,
                anonymous: Boolean, tagAgree: Boolean, tagTop: Boolean, tagSolve: Boolean,
                tagLector: Boolean, countBrowse: Boolean, countReply: Boolean,
                countVote: Boolean, deleted: Boolean, activeFlag: Boolean
               )

case class Comment(id: Long, commentorId: Long, anonymous: Boolean,
                   postId: Long, replyId: Long, countVote: Int, commentTime: Date,
                   deleted: Boolean, tagAgree: Boolean, tagTop: Boolean, activeFlag: Boolean
                )
```


## MySql数据读入

```scala
/** spark session */
private val conf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("Persona")
  .set("spark.redis.host", redisHost)
  .set("spark.redis.port", redisPort)
  .set("spark.redis.auth", redisAuth)

val session: SparkSession = SparkSession.builder().config(conf).getOrCreate()

/** data loading */
val dataFrameReader: DataFrameReader = session.read.format("jdbc")
  .option("url", mySqlUrl)
  .option("user", mySqlUsername)
  .option("password", mySqlPassword)

val userDataset: Dataset[User] = dataFrameReader
  .limit(1000)
  .load

val userDataFrame: DataFrame = userDataset.toDF

```


## 数据处理

编写`Column`作为用户群的`Filter`
```scala
val MALE: Column = col("sex").equalTo("男")
```

编写`Counting`任务
```scala
private val userFields: Array[String] =
  Array("sex", "birth", "area")

def countForUser(): Unit =
  for (group <- GroupUtil.map.keys; column <- userFields)
    Counting.count(
      DatasetUtil.userDataset.toDF,
      group, "user", column
    )

```


## Redis数据写入

```scala
private def count(dataFrame: DataFrame, group: String, table: String, column: String): Unit = dataFrame
  .filter(GroupUtil.getGroupFilter(group))
  .groupBy(column)
  .count
  .na.fill("unknown")
  .na.drop
  .write
  .format("org.apache.spark.sql.redis")
  .option("table", s"$group:$table:$column")
  .option("key.column", column)
  .mode(SaveMode.Overwrite)
  .save
```


## Redis查看数据

```scala
> keys *
...
11076) "adult:post:lastReplyTime:2016-03-26"

> hgetall adult:post:lastReplyTime:2016-03-26
1) "count"
2) "1"

> hget adult:post:lastReplyTime:2016-03-26 count
"1"

> flushall
OK

```


## Redis数据读入

```scala
val df = session.read
  .format("org.apache.spark.sql.redis")
  .option("table", "person")
  .load()
```


## 定时调用

最后定时调用即可
```scala
object Runner extends App {

  // Total: 11076
  val start: Instant = Instant.now
  Counting.countForUser()     // 9268
  Counting.countForReply()    // 495
  Counting.countForPost()     // 1115
  Counting.countForComment()  // 198

  println("Time cost:" + Duration.between(start, Instant.now).getSeconds + "s")
  // about 108s
}
```

# References

<https://github.com/RedisLabs/spark-redis>
<http://redisdoc.com/>
