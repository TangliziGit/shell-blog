# [MIT6.824] Lecture6&7 -  Raft: Fault Tolerance

> - http://nil.lcs.mit.edu/6.824/2020/labs/lab-raft.html



## Paper：Raft

Raft是一个为了管理多副本日志的共识算法。一致性算法允许一组机器作为一致的组工作，同时可在一些成员出现故障时存活下来。它**强调容错性（崩溃容错和拜占庭容错，这里只讨论前者）。**

### 引入

Raft与其他共识算法相似（特别是VSR, viewstamped replication），区别在于：

- **Strong leader**：Raft使用了一种比其他共识算法更强的领导形式。
  - 提升系统性能：在一些无Leader的多副本系统（如Paxos）中，通常需要在一轮消息中确定一个临时Leader，然后在下一轮消息中再确认请求。这花费了两倍的时间。
  - 帮助理解Raft系统。
- **Leader election**：Raft使用随机计时器来选举领袖。这只在心跳包上增加了少量的机制，就能简单和迅速解决冲突。
- Membership changes：这允许集群在配置更改期间继续正常运行。

对比更复杂的Paxos，Raft通过下面的方法进行了简化：

- 分治：Raft将算法分解成**Leader选举、日志复制、安全性和成员变动**。
- 简化状态空间



### 复制状态机

与GFS和HDFS类似，Raft将上层应用程序作为复制状态机管理（见上文VMware FT）。Raft作为共识模块，<u>可以在节点失败的场景下，保证日志内容和顺序的最终一致性。</u>具体而言，Raft能够提供如下的属性：

- **安全性**（safety）：在**所有非拜占庭条件下（包括网络延迟、网络分区、丢包、包重复、包乱序）**，从不返回错误的结果。
- **可用性**：大多数节点正常运行并可相互通信情况下，系统保持正常运行。当节点失败重启后，能够恢复状态并重新加入集群。
- **不依赖时间**：错误的时钟和消息延迟在极端的场景下，会导致可用性问题。
- **性能**：只需过半的节点同步Log，那么就可以响应客户端。

Raft会以库的形式存在于服务中。从软件的角度来看，我们可以认为在某个节点的上层是有状态的应用程序代码，下层是raft模块。我们拿带raft的kv数据库做一个例子：

假设客户端将请求发送给Raft的Leader节点，在服务端程序的内部，应用程序只会将来自客户端的请求对应的操作向下发送到Raft层，并告知把这个操作提交到多副本的日志中，并在完成时通知我。**当且仅当Raft的Leader知道了过半节点的副本都有了这个操作的拷贝之后**。Raft层会向上发送一个通知到应用程序说：刚刚的操作，我已经提交给所有副本，现在你**可以真正的执行这个操作**了。

注意：

1. 对于client的读请求，client最少需要等待两倍RTT才能获得响应：一个RTT是client和leader间的往返，一个RTT是leader与半数follower并发AppendEntries的往返。
2. 如果客户端发送请求之后一段时间没有收到回复，它应该重新发送请求。



### Raft共识算法

![raft-fig-2](/static/image/2021-12-20/raft-fig-2.png)

每个节点都有一种临时角色：Leader、Follower和Candidate。他们的职能和状态转移如下：

- Leader：主要用于处理客户端请求、发送AppendEntries（心跳包&日志同步）
  - 变为Follower：受到高Term的VoteRequest & 高Term的AppendEntries & 高Term的AppendEntries响应
- Follower：（初始值）被动接受Leader的AppendEntries进行日志同步，接受Candidate的VoteRequest选主
  - 变为Candidate：选举定时器到期
- Candidate：选主时主动发送VoteRequest
  - 变为Leader：选举通过过半仲裁
  - 变为Follower：受到高Term的VoteRequest & 高Term的AppendEntries & 高Term的VoteRequestResponse

注意：

1. 其实<u>任何角色受到任何高任期的请求和响应，都会转变为对应节点的Follower</u>。
2. 每个RPC都会在响应超时情况下进行重试尝试，并且每个RPC都是并行执行。
3. 每个Term会将时间分割开来，它在整体上表现成逻辑时钟。



#### 选主流程

![request-vote-rpc](/static/image/2021-12-20/request-vote-rpc.png)

- **触发时机**：某个Follower / Candidate在选举定时内，没有收到Leader的AppendEntries
- **选主流程[发送者]**：
  1. 节点状态改变：`{Role=Candidate, Term+=1, VotedFor=self}`
  2. 并发发送`VoteRequest{Term, CandidateID, LastLogTerm, LastLogIndex}`给所有server
  3. 统计结果`VoteRequestResponse{Term, VoteGranted}`：

     - 确认选举没有结束（选举可被选举定时器和新的选举打断）
     - `Term > currentTerm` => 成为Follower：`{Role=Follower, Term, VotedFor=nil}`；<s>重置选举计数器</s>
     - `Votes == serverCount / 2 + 1`(为了不重复检查成为Leader，这里用相等判断) => 重置选举计数器；成为Leader：`{Role=Leader， []nextIndex, []matchIndex}`；初始化`{[]nextIndex=len(log), []matchIndex=-1}`
     - 选举超时 => 重新选主
- **选主流程[接收者]**：`VoteRequest{Term, CandidateID, LastLogTerm, LastLogIndex}`
  - `Term < currentTerm` => 反对票
  - `isTermEnough && isLogUpToDateEnough` => 支持票
    - 成为Follower：`{Role=Follower, Term, VotedFor=candidateID}`；重置选举计数器
  - `otherwise` => 反对票
    - 检查Term大小更新：`{Role=Follower, Term}`

**注意**：

- 这个RPC是幂等的。
- 最新的Log被定义为，有最高的Term和最高的Index。
- 一切带Term的节点交互过程里（req / resp）都需要检查并更新本节点`{Term, Role=Follower}`。
- 当Term相同时，对votedFor的检验才有意义。因为Follower当前Term的votedFor是上一次选举时的产物，而非本次选举的。
- 可以将选主流程认为是（在最终一致性角度）：在集群中识别`{LastLogTerm, LastLogIndex}`偏序集的过程。在实时情况上，你还需要判断`{Term, sendingTime}`。细节可见下一条。
- 其实，一个节点最终能不能成为Leader主要关注LogTerm&LogIndex。不关注Term的原因是，该节点在与高Term节点通信时会改变调整Term。同时因为节点拥有更新的Log，除非大多数节点没有更新的Log，那么该节点后续还是很可能成为Leader。（若没有成为Leader，那么这条Log后续很可能会被覆盖，客户端将收不到这条提交的Log）
- 会存在脑裂，但是不过半的小集群Leader不会提交Log。于是小集群Leader不会响应Client的请求，也不会执行Client的命令。
- 随机化的定时器可以解决Candidate同时请求投票，导致的系统一直投票死锁的情况。但是选举定时器的随机范围需要规范确定：
  - 下限：应当是心跳包时间间隔的n倍，这样能确保follower能够避免因为偶然的网络异常而触发选举
  - 上限：它影响了Leader失败后系统恢复速度。当故障频繁时，需要重点权衡。
    - 我们需要考虑在两个节点超时时间差之内，应当可容纳单趟VoteRequest RPC的时长。



#### 日志复制

![append-entries-rpc](/static/image/2021-12-20/append-entries-rpc.png)

每个Client请求包含由复制状态机执行的命令。Leader将命令附加到其日志作为新条目，然后与每个其他服务器并行的复制Log。当Log已提交时，Leader将Log应用于其状态机，并将该执行的结果返回给客户端。如果Followers崩溃或运行缓慢，或者如果网络数据包丢失，那么Leader将无限期重试。

- **触发时机**：定期AppendEntries & 客户端提交log接口

- **复制流程[发送者]**：
  1. 发送`AppendEntries `：注意需要根据nextIndex得到`prevLogTerm & prevLogIndex`

  2. 统计响应`AppendEntriesResponse{Term, Success}`：

     1. 确认自己不是Leader

     2. `Term > currentTerm` => 成为Follower：`{Role=Follower, Term}`

     3. 检查`success`：

        - `success` => 更新nextIdx成对方最后一个LogIndex+1；更新matchIndex为nextIndex-1；根据所有matchIdx更新commitIdx并提及Log给上层应用（后文讨论细节）

        - `!success` -> 对应server回退nextIndex（后文细节）

- **复制流程[接受者]**：

  - `Term < currentTerm` => 响应失败
  - 没有匹配PrevLogIndex的Log => 响应失败
    - 更新`{Term, Role=Follower}`重启计数器
    - 检查冲突Log，回复`{XTerm, XIndex, XLen}`
  - `otherwise` => 响应成功
    - 更新`{Term, Role=Follower}`重启计数器
    - Append对应Log
    - 更新CommitIndex，并提交Log给上层应用：`commitIndex = max(commitIndex, min(leaderCommit, lastLogIndex))`

**注意**：

- 这个RPC是幂等的。
- 对Leader控制的网络分区来说，当term和index一致，那么log一致。这是单节点leader能够保证的，它不会像多节点那样有冲突。
- 当prevLogTerm & Index不同时，不更新。一旦更新，那么从冲突点开始的所有log，复制append req的logs。与上一条结论结合，副本将类似区块链那样保证副本完全一致。
- 对于上层应用发来的**读命令请求**，仍需要Leader做一次半数以上的AppendEntries，验证当前Leader是系统唯一Leader，才能保证读到的commit操作结果不是脏数据（旧数据，线性一致性）。
- Leader只能commit与当前Term相等Term的Log：Leader只能提交当前Term的Log，这样可以**避免覆盖Leader更替间隙中已提交的日志**。
  - Leader只能提交当前Term的Log，即从接受该Log至今没有Leader更替。如果Leader（TermC）提交了之前TermA的Log，那么有可能导致覆盖TermB的已提交Log。有可能在TermA和TermB之间存在一些TermB的已提交Log，可以称为间隙已提交日志。

- 如果Follower的日志与Leader的日志不一致，在下一次AppendEntries中一致性检查将失败。RPC被拒绝后，Leader递减nextIndex并重试AppendEntries。最终，nextIndex将达到Leader和Follower日志匹配的点。
- 为了能够更快的恢复日志，Follower需要返回足够的信息给Leader，让Leader可以以一段Log为单位来回退，而不用每次只回退一条Log条目。优化方案有很多：
  - **按Term回退：**Follower返回xTerm、xIndex和xLen用来表示prevLogIndex的匹配情况（即冲突）
    - xTerm：指冲突Log的Term
    - xIndex：指xTerm最远冲突点，即xTerm的第一个LogIndex
    - xLen：指如果没有prevLogIndex对应的Log，那么缺多少补多少；此时xTerm=-1
    - **原理**：在Index一致情况下，如果Term不相等，那么Follower这一Term的Log都是错误Log，所以需要一次性退掉xTerm。如果对应Index没有Log，那么nextIndex回退xLen再检查
  - 二分查找冲突点：Leader首先携带空Entries选取中点进行查询，当Follower Reject后向前取二分，当Follower Accept时向后取二分。当冲突点找到后则可以携带数据。



#### 日志快照

![install-snapshot-rpc](/static/image/2021-12-21/install-snapshot-rpc.png)






#### 安全性

首先总结一下Raft的安全特性。这些特性在网络分区或故障中都能保证：

- **选举安全**：在任期内最多只能选出一位领导人。
  - 论证：因为Candidate是通过半数仲裁赢得选举，同时一个节点在同一个任期里只能投票给一个节点。在同一Term中选出两个Leader一定意味着至少存在一个节点给两个节点同时投同意票，这是不可能的。
- Leader仅能追加日志：Leader从不覆盖或删除其日志中的条目，它只添加新条目。
- **日志匹配**：如果两个日志包含一个具有相同Index和Term的条目，那么从零到给定Index的所有条目中的日志都是相同的。
  - 论证：
    1. 首先，当term和index一致，那么log内容一致。因为在相同Term中只能选出一个Leader，同时单节点Leader一定能够保证相同Index和Term下的Log内容一致。
    2. 其次，在Leader做AppendEntries时每个Log都有唯一的prevLogIndex和prevLogTerm，仅当被同步节点具有响应前置Log，它才能接受此Log并附加在前置Log之后（类似于区块链）。
    3. 所以，当两个节点（Leader节点同理）接受相同Index和Term的Log后，可以说明之前的所有Log都是相同的。
- **Leader完整性**：如果一个Log是已提交的，那么该Log将会出现在任何更高Term的Leader日志中。
  - 论证：
    1. 首先，已提交的Log是指大多数节点都存在此Log，称这个Log为LogX。
    2. 其次，成为Leader需要通过过半仲裁。那么持有LogX的节点和投赞成票的节点会**有交集**，至少存在一个拥有LogX的节点投赞成票。
    3. 所以可以认为当前Leader拥有LogX。
- **状态机安全**：如果一个服务器在给定Index上应用了一个Log到它的状态机，那么任何其他服务器会应用这个Index里相同的Log。（我的理解：在同一个已提交的Log的Index上，不会存在另一个不同内容的Log在相同Index上提交）
  - 论证：反证法
    1. 首先，应用相同Index而不同Log X的方式是获得半数以上Follower的AppendEntries同意。要获得同意，对Leader而言必须有：`{Term>=followerTerm, prevLogMatched}`。
    2. 所以，Follower的Term一定低于Leader的Term，同时此Log的Term一定小于等于Follower的Term。那么有`LeaderTerm > LogXTerm`。
    3. 但是，Leader完整性要求已提交Log会出现在任何高Term的Leader中。注意到`LeaderTerm > LogXTerm`同时LogX已提交。那么说明LogX的内容是没有改变的。推出矛盾，证明状态机安全成立。







## Lecture 6&7 - Raft(2014)

在之前的三个多副本系统中，都或多或少地使用了单节点去决定某个副本谁是主。

使用单节点来决定的优势在与，单节点的决策不会出现矛盾。但同时单节点又面临了单点故障的问题（Single Point of Failure）。正因为单节点没有决策矛盾，所以它被用来处理脑裂的场景：当局部故障出现后（主备heartbeat断开），应决定谁是主备份。

### 脑裂：为什么要用单节点决定主备份？

> - MapReduce复制了计算，但是复制这个动作，或者说整个MapReduce被一个单主节点控制。
>
> - GFS以主备的方式复制数据。它会实际的复制文件内容。但是它也依赖一个单主节点，来确定每一份数据的主拷贝的位置。
>
> - VMware FT，它在一个Primary虚机和一个Backup虚机之间复制计算相关的指令。但是，当其中一个虚机出现故障时，为了能够正确的恢复。需要一个Test-and-Set服务来确认，Primary和Backup虚机只有一个能接管计算任务。
>
> 这三个例子中，它们都是一个多副本系统（replication system），但是在背后，它们存在一个共性：它们需要一个单节点来决定，在多个副本中，谁是主（Primary）。

此处可以理解为如何设计分布式锁，首先回忆一下处理竞争的正确方式：让某个节点知晓自己优先请求到资源，而且其他节点也要能确认存在某个节点首先请求到资源。

假设设置一个多副本的TestAndSet服务（S1和S2），同时其中的多副本没有建立主备复制机制。即每次client（C1和C2）都要访问某个server去选主。这里我们考虑两个场景：

1. 每个client都需要访问所有server，如果响应一致则选主成功。这其实对提高系统容错性没有帮助，两台服务器都需要正常运行且网络链接正常。
2. 每个client只需要访问某个server，如果某个响应成功，则选主成功。但这又是一个典型设计错误：不安全请求顺序对多副本系统不确定。


上世纪80年代排除脑裂的两种技术：

1. 构建一个不可能出现故障的网络。不可能出现故障的网络一直在我们的身边：连接了CPU和内存的线路就是不可能出现故障的网络。如果客户端不能与一个服务器交互，那么这个服务器肯定是关机了。
2. 人工检查问题。



### 过半仲裁 Quorum

如果服务器的数量是奇数的，那么当出现一个网络分割时，两个网络分区将不再对称。且必然不可能有超过一个分区拥有过半数量的服务器。（所有服务器数量的一半，而不是当前开机服务器数量的一半）。如果系统有 2 \* F + 1 个服务器，那么系统最多可以接受F个服务器出现故障，仍然可以正常工作。

在过半票决这种思想的支持下，大概1990年的时候，有两个系统基本同时被提出。这两个系统指出，你可以使用这种过半票决系统，从某种程度上来解决之前明显不可能避免的脑裂问题。两个系统中的一个叫做Paxos，Raft论文对这个系统做了很多的讨论；另一个叫做ViewStamped Replication（VSR）。尽管Paxos的知名度高得多，Raft从设计上来说，与VSR更接近。



### 日志的作用

1. 保证操作顺序性：Log包含的term和index，对系统而言用于保证log的顺序性。每个log拥有唯一的term和index。
2. 临时存储：Follower需要在确定操作被committed，才能将其应用到副本状态机中。有些未提较的log可能在未来切换leader后，被替换而不复存在。
3. 备份重传：当新机器加入集群中时，状态中的log entry是空的。所以leader需要给它传输先前已有的数据。
4. 状态恢复：Log存储于非易失介质中，当节点重启后可以通过从头执行log，来恢复副本状态机的内存信息。



### 零散的注意点

1. 对于raft暴露出来的接口，事实上只有一个：Start(command)和对应的ApplyMsg{command, logIndex, valid?}响应通道。课程在这里选择使用异步的方式，让应用程序等待commited logs。



### 一些案例

- 假设Leader每秒可以执行1000条操作，Follower只能每秒执行100条操作，并且这个状态一直持续下去，会怎样？

Leader将会是系统中最新的副本，同时Follower的log将无限增长。**Raft中并无流控机制**（流量控制，并非拥塞控制）。所以生产环境下，如果需要最大化系统性能，Leader需要加入流控能力控制速度。

- **当系统所有节点宕机会发生什么？**

重启后，进行Leader选举。Leader会在下一个AppendEntries操作时，按照nextIndex（初始化为LastLogIndex）发送给Followers。此时Follower应当检查prevLogTerm和Index来确认最近一次Log是否为系统的最新Log。若是则发送success，Leader在多数成功下更新commitIndex。若否，则Leader**回退prevLog**Term和Index来检验该Follower最近Log，并计算最近的commitIndex。当commitIndex确认后，Raft将命令发送给上层应用。应用程序需要**按序进行每一个Log的操作**。（加粗的两个操作是最消耗时间的机制，Raft对这两种问题也提出了解决方案）

- 在单向的网络出现故障情况下，raft如何工作？

Leader将收不到自己心跳包响应，导致没有log可以提交，同时没有新的Leader被选举出来。解决这个问题的方法是，Leader在受不到一定的响应后自动卸任（不发送心跳包，可能是进入Follower状态）。这样新的Leader将会出现。

 

### 持久化与代价

- 持久化的存储可以确保当服务器重启时，服务器可以找到相应的数据，并将其加载到内存中。这样可以使得服务器在故障并重启后，继续重启之前的应用程序状态。
- 我们需要具备替换一个全新的空的服务器，并让该新服务器在集群内工作的能力。这是至关重要的，因为如果一些服务器遭受了不可恢复的故障，例如磁盘故障，你绝对需要替换这台服务器。同时，如果磁盘故障了，你也不能指望能从该服务器的磁盘中获得任何有用的信息。所以我们的确需要能够用全新的空的服务器替代现有服务器的能力。
- 在整个集群都同时断电停止运行的场景下，我们需要能够得到之前状态的拷贝，这样我们才能保持程序容错继续运行。

在论文中只有Log、currentTerm、votedFor三个状态需要持久化。下面依次介绍原因：

- Log：这是唯一可以用来恢复应用程序状态的记录，没有它不可能回复应用状态。
- votedFor：用以避免该节点在同一任期多次投票，导致集群中出现多个Leader的情况。
- currentTerm：当Leader挂掉后，其他节点如果根据本地Log最大Term来猜测本机Term的话，可能导致存在两个同一Term的不同Log的Leader。（？？？）

顺便再看一下不需要持久化的状态：

- commitIndex：Leader**必须**在恢复集群过程中通过matchIndex得到（因为Leader不知道数据是否同步）；Follower可以在下一次AppendEntries中同步得到。
- lastApplied：从头执行一遍Log，应用程序就不需要持久化。（存在压缩Log和快照优化）
- (Leader) nextIndex：从末尾开始匹配每个节点得到。
- (Leader) matchIndex：从0开始匹配每个节点得到。

![latency](/static/image/2021-12-20/latency.png)

在一个真实的Raft服务器上，这意味着将数据写入磁盘，所以你需要一些文件来记录这些数据。但向磁盘写数据是一个代价很高的操作（随机Seek：HDD 10ms，SSD 100 μs，DRAM 50ns）。所以这里的持久化操作的代价可能会非常非常高。10毫秒相比发送RPC（国内平均70ms）或者其他操作来说都太长了。如果你持久化存储在一个机械硬盘上，那么每个操作至少要10毫秒，这意味着你永远也不可能构建一个每秒能处理超过100个请求的Raft服务。这就是所谓的**synchronous disk updates**的代价。为了让磁盘的数据保证安全，同时为了能安全更新你的笔记本上的磁盘，文件系统对于写入操作十分小心，有时需要等待磁盘（前一个）写入完成。所以这（优化磁盘写入性能）是一个出现在所有系统中的常见的问题，也必然出现在Raft中。

如果你想构建一个能每秒处理超过100个请求的系统，这里有多个选择。

- 使用更快的非易失性存储设备。你可以使用SSD硬盘，或者某种闪存。SSD可以在0.1毫秒完成对于闪存的一次写操作，所以这里性能就提高了100倍。更高级一点的方法是，你可以构建一个电池供电的DRAM，然后在这个电池供电的DRAM中做持久化存储。
- 批量执行操作。如果有大量的客户端请求，或许你应该同时接收它们，但是先不返回。等大量的请求累积之后，再一次性持久化存储，发AppendEntries。注意，**每个节点在持久化数据后才能发送相关RPC**。这是因为不能在节点发送数据后节点宕机，导致忘记自己的数据。

> 1. 当你持久化存储一个Log或者currentTerm，如何确保这些数据实时的存储在磁盘中？（unix/linux下write函数写缓存&立即返回）
>    在一个UNIX或者一个Linux或者一个Mac上，为了调用系统写磁盘的操作，你只需要调用write函数，在write函数返回时，并不能确保数据存在磁盘上，并且在重启之后还存在。几乎可以确定（write返回之后）数据不会在磁盘上。所以，如果在UNIX上，你调用了write，将一些数据写入之后，你需要调用fsync。在大部分系统上，**fsync可以确保在正确返回时，所有之前写入的数据已经安全的存储在磁盘的介质上了**。之后，如果机器重启了，这些信息还能在磁盘上找到。fsync是一个代价很高的调用，这就是为什么它是一个独立的函数，也是为什么write不负责将数据写入磁盘，fsync负责将数据写入磁盘。因为写入磁盘的代价很高，你永远也不会想要执行这个操作，除非你想要持久化存储一些数据。



### 日志快照

快照解决的问题是：

- Log会持续增长。最后可能会有数百万条Log，从而需要大量的内存来存储。如果持久化存储在磁盘上，最终会消耗磁盘的大量空间。
- 服务器恢复时，需要通过重新从头开始执行这数百万条Log来重建自己的状态。当故障重启之后，遍历并执行整个Log的内容可能要花费几个小时来完成。

Raft会要求应用程序做一个快照，从Log中选取一个与快照对应的点，然后要求应用程序在那个点的位置做一个快照，同时赋予一个与最后一个Log相同的Term和Index。接下来Raft将会丢弃所有那个点之前的Log记录。注意：**快照是应用程序的产物，Raft并不理解其中的内容**。

这里会与无快照的Raft流程有冲突：

- 在节点重启后，需要将快照作为Log恢复。所以应用程序也需要能像Log那样读取并处理快照。
- 若某个Follower缺失Leader的快照之前的一些Log，同时Leader如果也要丢弃快照之前的Log。那么Follower将很难恢复并应用Log给应用程序。一种解决方式是Leader只能在matchIndex最小值前做快照，另一种解决方式是Leader不删除matchIndex最小值及其后的Log。但如果有一个Follower持续关机，那么Log快照将不能发挥其作用（Leader和其他节点都不能，因为其他节点也有成为Leader的可能）。所以InstallSnapshot被提出，它会清除落伍的Follower应用程序，并安装对应的快照，最后接受一系列AppendEntries。

> 1. 如果RPC消息乱序该怎么处理？
>    RPC系统不是完全的可靠和有序，同时Leader几乎肯定会并发发出大量RPC，其中包含了AppendEntries和InstallSnapshot。如果Follower收到了一条InstallSnapshot消息，但是这条消息看起来完全是冗余的，这条InstallSnapshot消息包含的信息比当前Follower的信息还要老，这时，Follower该如何做？
>    Raft论文图13的规则6有相应的说明。老师认为正常的响应是，Follower可以忽略明显旧的快照。

### 线性一致

通常来说，线性一致等价于强一致。一个服务是线性一致的，那么它**表现的就像只有一个服务器**，并且服务器**没有故障**，这个服务器每次执行一个客户端请求。

如何证明某个应用程序是线性一致地执行呢？**（我认为外在表现只能从客户端的历史请求入手）**：我们拿到所有历史请求（<u>注意这里是请求的接受和回复，而实际的处理过程会在这个请求的范围中某个点</u>）并对其排序，如果后继序列可以构建并且是无环的，那么说明系统是线性一致的。对于这个后继逻辑顺序，有两个限制条件：

1. 如果一个操作在另一个操作开始前就结束了，那么这个操作必须在执行历史中出现在另一个操作前面。
2. 执行历史中，读操作，必须在相应的key的写操作之后。

例子可见：https://zhuanlan.zhihu.com/p/208394772和后续。对于系统执行写请求，只能有一个顺序，所有客户端读到的数据的顺序，必须与系统执行写请求的顺序一致。

> - 所以说线性一致不是用来描述系统的，而是用来描述系统的请求记录的？
>   **线性一致的定义是有关历史记录或系统行为的定义，而不是系统的定义。**

**关于重传**

服务器处理重复请求的合理方式是，根据请求的唯一号或者其他的客户端信息来保存一个表。这样服务器可以记住是否执行过它，从而发送一个相同的回复，因为服务器不希望执行相同的请求两次（优化时间 / 读写幂等）。

