# ASC - LE 项目交接文档



##  背景

本项目是2020年1月份开始进行的，本人在当年6月份进行了最后一次的训练和测试。原团队里有张椿旭和张栩浩，本人负责数据爬取、数据预处理和模型预测优化全流程的调研、设计和代码实现，张栩浩负责模型选取和分布式训练的调研、设计和实现。此项目仍有一些优化空间，本人和张栩浩是由于课程和升学的事情而暂停了ASC的工作。

项目选取了`RoBERTa`模型，于2019年由`facebook`实现，基于`PyTorch`框架。在模型选取上，我们最终选择它的原因是在一些数据集上的正确率表现比较好。模型选取的细节在后文中会体现。

本项目现在存于`202.117.249.6`节点的`/public/home/asc03/ele`目录中。除了数据和日志等敏感数据，代码部分也可访问<https://gitee.com/TangliziGit/ASC20-ELE>。

在接手项目前，希望能阅读一些`RoBERTa`的文档，否则会难以理解接下来的说明。



### 一些参考文档

事实上不限于这些，遇到问题时任何渠道都可以提供参考。

- PyTorch和BERT模型的使用：可参考大多数博客
- 官方repo：https://github.com/pytorch/fairseq/，是一个NLP工具箱，我们只使用了其中的`roberta`。
- 需要重点阅读：https://github.com/pytorch/fairseq/tree/master/examples/roberta，其中数据集构建和微调等，本项目是没有变化的。
- 优化思路的原始记录：https://tanglizi.one/post.sh?name=2020-02-05_ASC-Supplementary_4.md
- 关于运行等的记录：https://tanglizi.one/post.sh?name=2020-03-05_ASC-Supplementary_6.md



## 已有的工作

1. 模型比较和选取
2. 数据爬取、清洗和结构化的工作
3. 各种数据集选取与测试
4. 各种优化思路、结果和分析：https://tanglizi.one/post.sh?name=2020-02-05_ASC-Supplementary_4.md
5. 微调脚本、评估代码和结果分析脚本的实现
6. 项目迁移的工作：https://tanglizi.one/post.sh?name=2020-02-21_ASC-Supplementary_5.md



## 项目结构

由于直接基于`RoBERTa`进行开发，所以有很多文框架提供的，实际上这里只要关注一些重要的文件即可。

下面是一个简化的目录结构，标星号的是需要经常使用的：

```
/public/home/asc03/ele
├── analysis/						# *存储分析结果用的ipynb文件
│   ├── log.ipynb
│   └── loss.ipynb
├── checkpoints/					# *用于存储训练结果的
│   ├── mask-exp10
│   ├── mask-exp11
│   ├── mask-exp12
│   ├── mask-exp13
│   ├── mask-exp13-2
│   ├── mask-exp13-3
│   ├── mask-exp13-4
|   ...
├── data/							# 存储数据集的，一些原始数据可以暂时存储在这里
├── data-bin/						# 用于训练的数据
│   ├── ele
│   ├── ele-clean-mtc
│   ├── ele-clean-tc
│   ├── ele-mofangge
│   ├── ele-mofangge-test
|   ...
├── draw.py							# *读取指定的训练产生的日志，进行绘图，包括loss和错误类型的统计图
├── evaluate_candidates.py			# *评估训练完成的模型
├── fig/							# *存储draw.py绘图的图像
│   ├── error.png
│   └── loss.png
├── find_prop.py					# 用于计算阈值的程序，注意它只能给出一个大概的范围，关于阈值的选取需要经验来敲定
├── finetune/						# 旧有的微调脚本，请直接忽略
├── log/							# *日志
│   ├── 0131_03:51.log
│   ├── 0131_13:40.log
│   ├── 0131_14:49.log
│   ├── 0131_15:17.log
|   ...
├── mask-finetune/					# *现有的微调脚本，包含了exp开头的文件
│   ├── base.sh
│   ├── exp10.sh
│   ├── exp11.sh
│   ├── exp12.sh
│   ├── exp13-2.sh
│   ...
├── models/							# 存储roberta基础的模型
│   ├── roberta.base
│   ├── roberta.large
│   ├── roberta.large.mnli
│   └── roberta.large.wsc
├── monitor.py						# 原本用于管理训练测试等模型生命周期的简单脚本，现被更轻量的函数取代，已弃用
├── pkl/							# *是评估程序的结构化的输出，可用于各种分析。
│   ├── mask_base-log.pkl			# 注意，本人在最后的几次测试中，忘了把一些pkl文件从项目根目录移动到这里了
│   ├── mask_base-slog.pkl
│   ├── mask-exp1-log.pkl
│   ├── mask-exp1-slog.pkl
│   ├── mask-exp2-log.pkl
│   ...
├── process.sh						# 用于构建数据集的脚本
├── raw/							# 是处理好的原始数据
├── send_email.py					# 发邮件的，结合log的shell函数使用，不需要直接运行
├── stat.py							# 对评估的日志，做一些简单的统计
└── test_near.py					# 基于评估脚本，在错误类型中做得一些尝试，可忽略
```



## 安装运行

> 请参考 https://tanglizi.one/post.sh?name=2020-03-05_ASC-Supplementary_6.md



### 安装

项目基于`fairseq`工具集，它的安装是极大依赖网络的，如果遇到此类问题参考：[内网环境下的依赖处理](https://tanglizi.one/post.sh?name=2020-01-16_%E5%86%85%E7%BD%91%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%9A%84%E4%BE%9D%E8%B5%96%E5%A4%84%E7%90%86.md)

最简单的解决方式是问老师要可以访问网络的节点。

建议直接使用原来的目录。



### 训练模型

需要明确的一点是，任何一个基于`BERT`的模型都需要大量时间进行训练。而为了提高复用性和训练效率，我们都是在官方提供的已训练好的模型上，进行微调。

微调只需要运行一个CLI命令，但是为了方便管理，所以我们通常写一个shell脚本：

```shell
# 以 mask-finetune/exp4.sh 为例

TOTAL_UPDATES=2400    # Total number of training steps
# WARMUP_UPDATES=100    # Warmup the learning rate over this many updates
PEAK_LR=0.000004          # Peak learning rate, adjust as needed
TOKENS_PER_SAMPLE=512   # Max sequence length
MAX_POSITIONS=512       # Num. positional embeddings (usually same as above)
# MAX_SENTENCES=16        # Number of sequences per batch (batch size)
# UPDATE_FREQ=16          # Increase the batch size 16x
MAX_SENTENCES=2        # Number of sequences per batch (batch size)
UPDATE_FREQ=64          # Increase the batch size 16x

SAVE_DIR=checkpoints/mask-exp4     # !!! 这里不要忘了改
ROBERTA_PATH=models/roberta.large/model.pt

DATA_DIR=data-bin/ele

# CUDA_VISIBLE_DEVICES=0 fairseq-train --fp16 $DATA_DIR \		# 这是开启半精度，仅0号GPU运行
fairseq-train $DATA_DIR \
    --task masked_lm --criterion masked_lm \
    --arch roberta_large --sample-break-mode complete --tokens-per-sample $TOKENS_PER_SAMPLE \
    --optimizer adam --adam-betas '(0.9,0.98)' --adam-eps 1e-6 --clip-norm 0.0 \
    --lr-scheduler fixed --lr $PEAK_LR \
    --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \
    --max-sentences $MAX_SENTENCES --update-freq $UPDATE_FREQ \
    --max-update $TOTAL_UPDATES --log-format simple --log-interval 25 \
    --ddp-backend=no_c10d \
    --restore-file $ROBERTA_PATH \
    --save-dir $SAVE_DIR \
    --skip-invalid-size-inputs-valid-test
```

然后运行`bash mask-finetune/exp4.sh`。

或者你如果想输出一个日志，并在结束时给自己发一封邮件，使用在`.bashrc`中实现的`log`命令:

```bash
log bash mask-finetune/exp4.sh
```

它会输出一个`log/0303_23:19.log`日志。

`log`函数是大概这样的，`send_mail.py`文件中请自己配置客户端密码。

```bash
function log(){
    dir=`pwd`
    filename=$dir/log/`date +%m%d_%H:%M.log`

    echo "$@" >> $filename
    $@ 2>&1 | tee -a $filename

    echo "" >> $filename
    date +%m%d_%H:%M >> $filename
    python $dir/send_email.py $filename
}
```





### 评估模型

```bash
python evaluate_candidates.py --questions data/ELE/valid.jsonl --model checkpoints/mask-exp4 --log mask-exp4
```

用于解答 `data/ELE/valid.jsonl`里的问题，使用 `checkpoints/mask-exp4/model.pt`模型，并且记录一些日志到`mask-exp4-log.pkl`。



### 分析



#### 分析微调日志

更新在`draw.py`文件中的`log_files`

```python
log_files = {
    "large": "0208_13:11.log",
    "exp1":  "0210_21:12.log",
    "exp2":  "0211_11:51.log",
    "exp3":  "0212_21:20.log",
    "exp4":  "0214_00:21.log",
    "exp6":  "0228_08:58.log",
    "exp7":  "0228_20:23.log",
}
```

然后运行`python draw.py`, 你可以得到 `fig/loss.png`。



#### 分析评估日志

##### 统计数据

评估之后，运行 `python stat.py pkl/mask-exp4` 可以得到一个简单的统计结果：

```bash
multi-words      0.0
unk            220.0
near           468.0
far            268.0
dtype: float64

max acc 1.0
min acc 0.25
var acc 0.01125261111111111
Accuracy: 0.8774044626827392
```



##### 错误类型的图像

如`分析微调日志`一样，修改`pkl_log_files`在`draw.py`中：

```python
pkl_log_files = {
    "#18": "mask-large-without_ul",
    "#19": "mask-large-without_ul-long_art",
    "#20": "mask-large-without_ul-long_art-unk",
    "#21": "mask-exp3-without_ul-long_art-unk",
    "#22": "mask-exp3-t4",
    "#23": "mask-exp4-t4",
    "#24": "mask-exp4-t3",
}
```

运行`python draw.py`得到`fig/error.png`。



### 建立新的数据集

请主要参照`RoBERTa`文档。



## 开发方式



## 优化思路



## 其他的一些想法

