# [CMU15-445] 数据库原理知识点总结

> - https://15445.courses.cs.cmu.edu/fall2021/schedule.html

这里将从lecture的顺序来做总结。下面是在并发控制中展示的图，它说明了整个数据库系统的大致组成部分：

![dbms-structure](/static/image/2022-02-19/dbms-structure.png)

# Storage / 存储

## Q：常见介质读写速度

![access-speed](/static/image/2022-02-19/access-speed.png)



## Q：数据库中讨论的Page是指什么？

![database-pages](/static/image/2022-02-19/database-pages.png)



## Q：DBMS如何存储各个Page？

一般而言有两种存储方式：

**Heap File**

堆文件是一个无序的页面集合，其中Tuple按随机顺序存储。而Heap File则通过使用页面链表或页面目录实现，用于给定页面id的时定位磁盘上的页面：

- Linked List：维护一个Header Page，里面存储free page和data page的位置；同时每个page带有双向链表的指针。
- Page Directory：维护一个目录页，存储每个page的位置。

![linked-list](/static/image/2022-02-19/linked-list.png)

![page-directory](/static/image/2022-02-19/page-directory.png)

**存储于索引**

B+Tree：Cluster Index聚簇索引，在叶节点上直接包含对应的数据页。

HashTable：哈希表也可以存储`<key, location>`，也可以存储`<key, tuple>`。相当于减少一次访存。



## Q：数据页的布局是什么样的？

**Slotted Pages**

这时当今dbms中最常用的方法。

- Page中的布局依次是头、空闲空间和Tuples数组。
- 其中Header记录了使用的槽数，最后一个使用的槽的起始位置，以及记录每个元组起始位置的槽数组。
- 对于Tuples数组部分，Tuples将从末尾到开始增长，响应的槽数组将从开始到结束增长。当槽数组和Tuple数据满足时，该页被认为是满的。

![slotted-pages](/static/image/2022-02-19/slotted-pages.png)

```
 table_page.h
 *  Slotted page format:
 *  ---------------------------------------------------------
 *  | HEADER | ... FREE SPACE ... | ... INSERTED TUPLES ... |
 *  ---------------------------------------------------------
 *                                ^
 *                                free space pointer
 *
 *  Header format (size in bytes):
 *  ----------------------------------------------------------------------------
 *  | PageId (4)| LSN (4)| PrevPageId (4)| NextPageId (4)| FreeSpacePointer(4) |
 *  ----------------------------------------------------------------------------
 *  ----------------------------------------------------------------------------
 *  | TupleCount (4) | Tuple_1 offset (4) | Tuple_1 size (4) | ...             |
 *  ----------------------------------------------------------------------------
```



**Log Structured**

一般DBMS只使用这种方法在存储日志记录。

- 为了读取记录，DBMS会扫描日志文件来重建日志。
- 适用于仅追加存储，因为DBMS不能返回和更新数据。所以写得快，读得慢。
- 为了避免长时间的读取，DBMS可以有索引，允许它跳转到日志中的特定位置。
- 可以周期性地压缩日志。但压缩的问题是DBMS最终会产生**写放大**。(Write Amplification，重复写相同的数据。)

![log-structured-file](/static/image/2022-02-19/log-structured-file.png)

![log-structured-snapshot](/static/image/2022-02-19/log-structured-snapshot.png)

## Q：什么是写放大？

> - [存储系统中的各种“写放大”](http://blog.jcix.top/2018-06-05/write_amplification/)



## Q：Tuple的内部结构是什么样的？

Tuple会存储两部分数据：

- Header：记录Tuple的大小、每个值是否为Null值的BitMap、并发控制的可见性信息等
- Payload：就是数据本身
  - 大部分数据库不支持Payload超过一个页
  - 可以将数据存储在一个特殊的“溢出”页面上，并让元组包含对该页面的引用的对象。这些溢出页可以包含指向其他溢出页的指针，直到可以存储所有数据为止。
  - 或者可以将数据存放在外部存储中，但这样就不能享受到DBMS提供的持久化和事务保证。

![overflow-page](/static/image/2022-02-19/overflow-page.png)

![external-file](/static/image/2022-02-19/external-file.png)

注意:

- 每个Tuple都有一个全局可访问的ID，一般而言是`page_id + offset`
- 如果两个表是相关的，那么**预连接**可能会有效：这两个表的数据最终会出现在同一个Tuple上。这使得读取速度更快，但更新成本更高



## Q：OLTP和OLAP工作负载区别？

OLTP：负载特征是快速且短暂的操作，典型的OLTP是写比读多的。

OLAP：负载特征是长时间且复杂的查询，定期会将数据从OLTP做ETL（提取转换加载）。

HTAP：二者混合。可直接在HTAP上做数据分析，也可以将数据转移在OLAP上。

![workloads](/static/image/2022-02-19/workloads.png)

## Q：数据页中Tuple的存储模型

这里的存储模型与数据页分布没有冲突，如Slotted Pages可以采用列式存储。不过列式存储的典型做法这里并没有介绍到，可以参考DDIA。

**N-Ary Storage Model (NSM） / 行存储**

DBMS将单个元组的所有属性连续存储在单个页面中。

这种方法对于OLTP工作负载非常理想：请求需要大量插入，且事务往往只操作单个实体。因为它只需要一次就能够获得单个Tuple的所有属性。

- **优势**：
  - 快速插入、更新、删除
  - 对查询Tuple所有属性的操作友好

- **缺点**：不适合做全表扫描、不适合做单独某些属性的查询（因为会查询到不需要的数据）

![nsm](/static/image/2022-02-19/nsm.png)

**Decomposition Storage Model (DSM) / 列存储**

DBMS将所有Tuple的列连续地存储在一个数据页中。

这个模型非常适合OLAP工作负载：大多数是只读查询，且常常做全表扫描 & 查询属性子集

- **优势**：
  - 减少了查询执行过程中浪费的工作量
  - 支持更好的压缩，因为相同属性的所有值都是连续存储的
- **缺点**：点查询、插入、更新和删除速度慢（因为需要多次随机查询，拼接出完整的Tuple）

![dsm](/static/image/2022-02-19/dsm.png)

拼接完整Tuple的方法有两种：

- **定长偏移量**：这是最常见的方法
  - 属性都是固定长度的，DBMS可以为每个元组计算属性的偏移量
  - 当拼接Tuple时，知道如何从offest跳转到文件中的该位置
  - 对于变长字段：维护`<offset, location>`的字典以便跳转；或者填充字段
- **内嵌ID**
  - 对于列中的每个属性，DBMS存储一个元组id。然后系统还会存储一个映射，告诉它如何跳转到每个具有该id的属性。
  - 这个方法有很大的存储开销，因为它需要为每个属性条目存储一个tuple id。

![dsm-history](/static/image/2022-02-19/dsm-history.png)



## Q：日志式数据库中的数据存储形式

> - https://zhuanlan.zhihu.com/p/181498475
> - https://www.cnblogs.com/siegfang/archive/2013/01/12/lsm-tree.html

**仅追加存储 & HashMap索引**

将日志分解成一定大小的段， 当文件达到一定大小时就关闭它， 并将后续写入到新的段文件中。 在内存中存储HashMap作为索引，映射键到某个段中特定的偏移量。

- **写入流程**：追加写入最新的段中，然后更新内存HashMap。
- **读取流程**：每个段都会有自己的内存哈希索引。数据查询首先检查最新段的索引，接着检查第二新的段，以此类推。 由于合并过程可以维持较少的段数量， 因此查找通常不需要检查很多段索引。
- **删除记录**：删除键时设置墓碑值。当合并日志段时， 一且发现墓碑标记， 则会丢弃这个已删除键的所有值
- **日志压缩**：可以在日志段上执行压缩和合并，新段会被写入另一个新的文件。
  - **并发压缩**：旧段的合并压缩可以在后台线程中执行，其他工作线程仍然可以使用旧段进行文件读写。当合并过程完成后， 将读取请求切换到新的合并段上， 而旧的段文件可以安全删除。
- **崩溃恢复**：可以通过从头到尾读取整个段文件，然后记录每个键的最新值的偏移量，来恢复每个段的HashMap。
  - Bitcask通过将每个段的hash map的快照存储在磁盘上， 可以更快地加载到内存中， 以此加快恢复速度。
  - **部分写入**：将记录追加到日志的过程中崩溃。通过校验值发现部分写入的日志，并直接丢弃。

下面讨论一些优缺点，便于引出LSM-Tree：

- **优点**

  - **更快的IO**：追加和分段合并主要是顺序写， 它通常比随机写入快得多， 特别是在旋转式磁性硬盘上。

  - **更简单的并发和恢复**：段文件是追加的或不可变的， 则并发和崩溃恢复要简单得多。不会有复杂的部分写入的问题。

  - **更少的碎片**：合并旧段可以避免随着时间的推移数据文件出现碎片化的问题

- **缺点**

  - 持久化的哈希表需要大量的随机访问I/O，增长代价昂贵。同时哈希冲突需要复杂的处理逻辑。

  - **区间查询**：区间查询效率低，只能扫索引扫表。

**SSTables/LSM存储 & LSM-Tree索引**

LSM-Tree的核心思想就是将写入推迟并转换为批量写：首先将大量写入缓存在内存，当积攒到一定程度后，将他们批量写入文件中，这样一次I/O可以进行多条数据的写入，充分利用每一次I/O。

在LSM存储中，段文件需要按键排序，称为排序字符串表SSTable。所以每个键在每个合并的段文件中只能出现一次。 

- **写入流程**：写入时，将KV添加到内存中的平衡树数据结构中，此结构有时被称为**内存表**。
  - **崩溃恢复**：维护内存表的日志，当崩溃重启后恢复即可，临时日志不需要保证顺序。
  - **RollingMerge**：当内存表大千某个阈值，将其作为SSTable文件写入磁盘。当SSTable写磁盘的同时， 写入可以继续添加到一个新的内存表实例。
    - 当C0大小达到某一阈值时或者每隔一段时间，将C0中记录滚动合并到磁盘C1中；对于多个存储结构的情况，当C1体量越来越大就向C2合并，以此类推，一直往上合并Ck。
- **读取流程**：首先尝试在内存表中查找键，然后是最新的磁盘段文件中的索引， 接下来是次新的磁盘段文件中的索引。
- **删除记录**：删除键时设置墓碑值。在最后一层Ck合并日志段时，再消除带有墓碑值的日志。 

相比哈希索引的日志段， 具有以下优点：

- **高效合并**：K路归并到输出文件。键相同时用新段的键覆盖。
- **稀疏索引**：不需要存储所有键，根据键排序可以查找到数据存在于一段offset中。
  - 这让文件中索引部分更小，并能二分加速查询。
  - 如果支持可变长Key，那么段上不支持二分查询。但仍然可以在内存中的索引上做二分，内存中可以存可变长Key的指针。

一些LSM-Tree的优化：

- **布隆过滤器**：查询不存在的Key会很有效
  - 在最优准确率下，$K = ln(2)\frac{m}{n}$。这里K是哈希函数个数，m是bit数组大小，n是插入元素个数。
  - 失误率为$p=(1-e^{-\frac{nk}{m}})^k$
- **大小分级**：[Cassandra, HBase] 较新的和较小的SSTables被连续合并到较旧和较大的SSTables
  - **缺点 - 更多冗余**：最底层的单个SSTable的大小会变得非常大。而且对于同一层的SSTable，每个key的记录是可能存在多份的。
- **分层压缩**：[LevelDB, RocksDB] 键的范围分裂成多个更小的SSTables, 旧数据被移动到单独的层级， 这样压缩可以逐步进行并节省磁盘空间。
  - leveled会将每一层切分成多个大小相近的SSTable。这些SSTable是这一层是全局有序的，意味着一个key在每一层至多只有1条记录，不存在冗余记录。
  - **压缩方法**：Lx层超过大小限制后，从Lx层选择一个文件与Lx+1层相交部分合并，输出到Lx+1层，并重复操作。多个不相交的合并可以并发执行。



## Q：SSTable的布局是什么样的？

> - https://leveldb-handbook.readthedocs.io/zh/latest/sstable.html

以下以LevelDB为例。注意除了Footer以外，其他所有类型数据的物理结构中都会做压缩和CRC循环冗余校验校验码：

- **DataBlock / KV数据**：会有多个DataBlock来存储kv对
  - 因为数据顺序存储，所以Key之间会有大量共享前缀。于是LevelDB存储时会共享前一个Key的一部分。
- **FilterBlock / 布隆过滤器**：用来存储布隆过滤器。若用户指定不使用过滤器，那么不会存储此Block。
  - 每个SSTable中会有多个bit数组，这可能是为了最优准确率？
- **MetaIndexBlock**：用来存储filter block的索引信息（索引信息指在该sstable文件中的偏移量以及数据长度）
- **IndexBlock / 索引**：存储每个索引信息，每个索引行对应一个DataBlock。
  - **MaxKey**：对应DataBlock中最大的Key。**Offset**，**Length**。
- **Footer**：用来存储meta index block及index block的索引信息，还有一个Padding



## Q：LevelDB的压缩过程是怎么样的？

> - https://zhuanlan.zhihu.com/p/46718964

LevelDB的存储过程如下：

![leveldb-arch](/static/image/2022-03-08/leveldb-arch.jpeg)

- **log**：为了应对故障，写操作首先写入到日志中，再响应客户端。对于Log而言，写操作都是顺序的，不需要考虑Key的排序。
- **memtable**：数据首先写入到跳表数据结构的内存表memtable中，所有的数据按**用户定义的排序方法**排序之后，到达阈值4MB转换为immutable memtable做下一步的存储。
  - 在其存储大小达到阈值时（默认4MB），便将其转换成一个**不可修改**的memtable。与此同时创建一个新的memtable，供用户继续进行读写操作。
- **immutable memtable**：在一定时机到达后，后台压缩进程会将其转换为sstable，随后持久化到磁盘文件中。
- **sstable**：sstable文件在逻辑上被分成若干层，由内存数据直接dump出来的文件称为level 0层文件，后期整合而成的文件为level i 层文件
  - 当 level i 到一定条件后（某个 level 中的数据量或者 sstable 文件数据等）就会和 level i+1 中的 sstable 进行 Compaction，合并成 level i+1 的 sst 文件


**压缩过程**

- 压缩优先级：**Minor > Manual > Size > Seek**。这里的优先级是调度时判断的顺序，而非抢占式。
- **Minor Compaction**：将 immutable memtable 持久化为 sst 文件。
  - **触发时机**：Write请求到来后，检查内存表占用大小，一旦超过默认4MB的写缓存大小就会触发
  - 新产生出来的sstable 并不一定总是处于level 0，需要计算到会产生重叠的层上。（有很多细节）

- **Major Compaction**：将不同层级的 sst 的文件进行合并。
  - **Manual Compaction**：人工触发，客户端调用接口`DBImpl::CompactRange(const Slice begin, const Slice end)`。
    - 检查所有重合区间的层，慢慢合这些层的区间
  - **Size Compaction**：<u>核心Compact过程</u>。为了均衡各个level的数据， 从而保证读写的性能均衡。
    - 根据每个level的总文件大小，计算score选择最大的level做合并。（$score = \frac{SumOfFileSize}{10^i}$）
    - 选择此层中需要合并的SSTable。（似乎是每层记录LastCompactedID，然后顺序选择下一个？）
    - 确定下一层需要合并的SSTable，做合并。
  - **Seek Compaction**：每个文件的 seek miss 次数都有一个阈值，如果超过了这个阈值，那么认为这个文件需要Compact。
    - 每次miss读操作都会递减SSTable的miss次数，当小于0时被标记为需要压缩。

# Buffer Pool / 缓存

## Q：Buffer Pool是什么？为什么需要它？为什么不用OS的缓存？

Buffer Pool是从磁盘读取页面的内存缓存。它本质上是在数据库内部分配的一个大内存区域，用于存储从磁盘获取的页面。Buffer Pool的中的数据页被成为frame。

Buffer Pool维护的也不仅仅是Tuple和Index，更多的还有：sorting & joining缓存、查询缓存、日志缓存等。

**意义**

- DBMS面临的一个障碍是如何最大限度地减少移动数据的耗时。理想情况下，数据理应全部在内存中。
- DBMS比OS更加了解磁盘操作执行顺序和时机。比如：数据页淘汰策略、预取、脏页写回、WAL和checkpoint等。
- DBMS更清楚数据的时空局部性：在磁盘上经常使用的页面在物理上尽可能靠近、尽量减少从磁盘读取数据的档位数量。

大多数DBMS不使用OS缓存（O_DIRECT可绕过缓存）。不过，Postgres是一个使用操作系统页面缓存的数据库系统的例子。



## Q：锁与闩的区别？

![lock-vs-latch](/static/image/2022-02-19/lock-vs-latch.png)

## Q：闩的实现

**OS Mutex**

Linux提供了futex(快速用户空间互斥锁)，它由**用户空间中的自旋锁存器**和**操作系统级的互斥锁存器**组成。尽管它包含两个内部锁存器，但它看起来就像DBMS的一个锁存器。一些编程语言中包装了它，如C++`std::mutex`。

如果DBMS可以获得用户空间锁存器，那么就设置锁存器。如果DBMS没有获得用户空间锁存器，那么它就会进入内核，试图获得更重的互斥锁。如果DBMS没有获得第二个互斥锁，那么线程通知操作系统它被阻塞在这个互斥锁上，然后它被重新调度。

- 优势：简单易用；常用于处理等待进程多的场景，释放进程资源占用

- 缺点：开销太大，且不可扩展

**Spin Latch**

一般是指原子性的CAS。

- 优势：上锁/解锁很快
- 缺点：不可扩展，只能用在等待进程少的场景，否则占用进程资源；缓存不友好，因为它会出现导致缓存一致性问题，因为线程在轮询其他CPU上的缓存。

**RW Latch**

读写锁跟踪有多少线程持有它并等待获得锁。它在本质上使用前面两种锁存器实现中的一种作为原语，同时额外维护读写者队列，并记录读写者个数。

- 优势：读读可并发
- 缺点：可能导致饥饿问题（如果持续有reader占用锁，那么writer将饿死）。一个解决方法是当writer到来时，停止接受reader。

![rw-latch](/static/image/2022-02-19/rw-latch.png)

## Q：Buffer Pool的运作方式

当DBMS请求一个页面时，一个精确的副本被放置到缓冲池的一个帧中。然后，当请求页面时，数据库系统可以首先搜索缓冲池。如果没有找到该页，则系统从磁盘中获取该页的副本。

![buffer-pool](/static/image/2022-02-19/buffer-pool.png)

**metadata / 元数据**

- **页表**：内存中的哈希表，用于将页面id映射到缓冲池中的帧位置。
  - 页表还需要维护每页的额外元数据：**脏标记**和 **pin / reference counter**。
- **脏标志**：是由线程在修改页面时设置的。这指示存储管理器后续必须将该页写回磁盘。
- **pin / reference counter**：跟踪当前访问该页面的线程数。
  - 线程在访问该页之前必须增加计数器。如果一个页的计数大于零，那么存储管理器就不允许从内存中删除该页。



## Q：Buffer Pool的优化

**Multiple Buffer Pool**

支持的数据库：MySQL、Oracle、SQL Server、DB2

为不同的目的维护多个Buffer Pool。每个Buffer Pool可以采用针对存储在其中的数据定制的本地策略。该方法有助于减少锁存争用并改善局部性。

**预取**

根据查询计划预取页面。这是在连续访问多个页面时常用的方法。

**Scan Sharing / 共享游标**

支持的数据库：Oracle、SQL Server、PostgreSQL、DB2

查询游标可以重用扫描中检索到的数据：如果存在一个查询开始扫全表，那么DBMS就可以连接这个scan到第二个查询上。

**Buffer Pool 绕过**

支持的数据库：Oracle、SQL Server、PostgreSQL、Informix

顺序扫描操作符不会将获取的页面存储在缓冲池中，以避免开销。

因为某些数据大概率只会读取一次，比如读取连续大量页面时、临时数据查询时(排序，连接)。

后续会讨论到sequential flooding的情况，也可以用这种方法解决。



## Q：Buffer Pool的换页策略

**Least Recently Used (LRU) / 至少最近使用 / 最久未使用**

维护page队列。

- 当使用某个页时，将它放置在队头。
- 当驱逐某个页时，排出队尾元素。

**CLOCK**

维护环形链。每个元素都有一个计数位。

- 当使用一个页时，计数位设置为1。
- 当驱逐某个页时，如果页面的位设置为1，设置为零；如果不是，则驱逐它。

**Sequential Flooding / 顺序泛洪**

LRU和CLOCK容易受到顺序泛洪的影响：缓冲池的内容会因为顺序扫描，而不断被驱逐。

因为顺序扫描读取的每个页面，实际上是最不需要的页面，因为未来的一系列访问都不会再访问扫描过的页。

解决方法如下：

- 更大的工作集。
- 更好的换页策略：如LRU-K，它将最后K个引用的历史作为时间戳跟踪，用于预测页面下次被访问的时间。
- 查询的本地化：DBMS在每个事务/查询的基础上选择退出哪些页面。
- 优先级提示：允许事务根据查询执行期间每个页面的上下文告诉缓冲池页面是否重要。

**关于脏页**

这里的脏页会在后续的WAL中的checkpoint处理，如果使用简单checkpoint会直接写入磁盘，但是fuzzy checkpoint将在后台空闲执行.

# Access Methods / 访问方法

## Q：Hash Table的设计应该注意什么？

**Hash Function**：如何将一个较大的键空间映射到一个较小的域

- 需要考虑**执行速度**和**冲突率（均匀分布）**之间的权衡。
- DBMS不需要使用加密安全的哈希函数，不需要担心保护键的内容。
- 目前最先进的哈希函数是Facebook XXHash3。

![hash-function-benchmark](/static/image/2022-02-19/hash-function-benchmark.png)

**Hash Scheme**：如何处理哈希后的键冲突

- 需要考虑在分配一个大哈希表以减少冲突，还是在冲突发生时必须执行额外指令之间的权衡。



## Q：什么是 Static Hash Scheme / 静态哈希？

Static Hash Scheme，即哈希表的大小是固定的。如果哈希表中的存储空间用光了，那么必须重建更大的哈希表（通常两倍），这是非常昂贵的。

**Linear Probe Hashing / 线性探查**

是最基本的哈希方案，通常也是**最快的**。

数据结构上，它使用循环数组：

- 当冲突发生时，线性搜索相邻的插槽，直到找到一个开放的插槽，然后插入
- 对于查找，我们可以检查键散列所指向的槽，并线性搜索，直到找到所需的条目(或者一个空槽，在这种情况下键不在表中)
- 删除操作比较困难。为了不打破查询规则，一般有两种解决方案：
  - 墓碑位：为删除的数据打上墓碑位。查询时可以跳过，插入时可以直接插入。
  - 移动数据：直接将最末尾的数据移动到空位上。

关于**非唯一键**，有两种解决方案：

- 拉链法：每个槽将存储一个链表，相同hash的key存储在一起。
- 冗余键：直接存储即可，这是最常见的方案。

**Cuckoo Hashing / 布谷鸟哈希**

维护多个具有不同哈希函数的表。这里的哈希函数是相同的算法，但使用不同的种子。

- 当插入时，需要检查每个表，并选择一个有空闲槽的表
  - 如果没有表有空闲槽，我们随机选择一个表，并驱逐旧的条目。然后将旧条目重新散列到另一个表中。
  - 在极少数情况下，我们可能会陷入一个循环。这时需要重建所有哈希表，或者使用更大的表(更常见)重建哈希表。
- 这种Hashing保证了`O(1)`的查找和删除，但插入成本更高。

![cuckoo-hashing](/static/image/2022-02-19/cuckoo-hashing.png)

## Q：什么是 Dynamic Hash Scheme / 动态哈希？

动态哈希方案能够根据需要调整哈希表的大小，而不需要重建整个哈希表。

**Chained Hashing / 链式哈希**

这是最常见的动态哈希方案。DBMS为哈希表中的每个槽维护一个桶的链表，散列到相同槽位的键被简单地插入到该槽位的链表中。

注意：
1. 这里的链表是block（或者说page，它可以内涵多个元素）的链表，而非元素链表。
2. buckets扩容仍然需要rehash

![cuckoo-hashing](/static/image/2022-02-19/cuckoo-hashing.png)

**Extendible Hashing / 可扩展哈希**

GFS和ZFS等大多数文件系统使用这种方式来做Hash索引。

这种方法允许哈希表中的多个槽位置指向同一个bucket。其核心思想是在拆分时移动桶项，并增加比特数来检查哈希表中的条目。这意味着DBMS只需要在分割链的桶中移动数据;所有其他桶都保持原样。

- DBMS维护一个全局和局部深度位计数，它决定了在槽数组中查找bucket所需的位数。
  - 当查询时，需要降Key的后N位与全局槽位中的数进行匹配，然后访问对应的bucket。
- 当存储桶满时，DBMS将存储桶拆分并重新shuffle它的元素。
  - 如果拆分桶的局部深度小于全局深度，则新桶将被添加到现有的槽位数组中。否则，DBMS将插槽数组的大小翻倍以容纳新的存储桶，并增加全局深度计数器。
- 当删除元素时，可以触发Shrink，具体见2018 / 2019 Homework2

![extendible-hashing](/static/image/2022-02-19/extendible-hashing.png)

## Q：B+Tree的定义

B+树是一种自平衡的树数据结构，它保持数据排序，允许在`O(log(n))`内进行搜索、顺序访问、插入和删除。它是针对读写大块数据的面向磁盘的DBMS进行优化的数据结构。

现代的B+Tree实现结合了其他B-Tree变体的特性，比如Blink-Tree中使用的兄弟指针。另外，作为参照，这里给出每个优化的大致发表年份：1971 B-Tree、1973 B+Tree、1977 B*Tree、1981 Blink-Tree。

**定义**

B+树是一棵M路搜索树，且具有以下属性:

- 每个叶节点都在相同的深度，即完美平衡。
- 除根节点外，每个内部节点至少满一半`M/2−1 <= num of keys <= M−1`。
- 每个有k个键的内部节点有k+1个非空子节点。

一下是BusTub的叶节点定义，主要包括页信息、兄弟节点ID、父节点ID、`<key, RID>`对：

```
 * Store indexed key and record id(record id = page id combined with slot id,
 * see include/common/rid.h for detailed implementation) together within leaf
 * page. Only support unique key.
 *
 * Leaf page format (keys are stored in order):
 *  ----------------------------------------------------------------------
 * | HEADER | KEY(1) + RID(1) | KEY(2) + RID(2) | ... | KEY(n) + RID(n)
 *  ----------------------------------------------------------------------
 *
 *  Header format (size in byte, 28 bytes in total):
 *  ---------------------------------------------------------------------
 * | PageType (4) | LSN (4) | CurrentSize (4) | MaxSize (4) |
 *  ---------------------------------------------------------------------
 *  -----------------------------------------------
 * | ParentPageId (4) | PageId (4) | NextPageId (4)
 *  -----------------------------------------------
```

![b-plus-tree-leaf-node](/static/image/2022-02-19/b-plus-tree-leaf-node.png)

**查询**

这里想讨论一种非最左匹配的查询。比如索引包括了A、B两个列的属性，那么如何查询`<*, b>`的所有值？

从最后一层中间节点开始扫描，优化掉不需要访问的叶节点（指不包含`B==b`的叶节点），最后再依次访问叶节点即可。

## Q：B+Tree如何处理非唯一键？

与Hash表相似，两种方法：冗余键和拉链法。

## Q：Clustered Indexes / 聚簇索引

![clustered-indexes](/static/image/2022-02-19/clustered-indexes.png)

聚簇索引的优点：

- 减少额外存储空间，减少一次访存操作（虽然二级索引需要回表）
- 多版本GC + 删除时的tuple物理地址移动只会做一次
- 二级索引回表时，可以排序主键的访问顺序，做到顺序访问存储

## Q：B+Tree的设计选择

**节点大小**

- 取决于**存储介质**
  - HDD存储通常使用MB级别大小，以减少查找数据时访盘次数。
  - 内存数据库则使用最小为512B的页面大小，以便将整个页面放入CPU缓存，并减少数据碎片。
- 取决于**工作负载**
  - 点查询希望页面越小越好，以减少不必要的额外信息加载量
  - 大型顺序扫描可能希望页面越大，以减少所需的读取次数。

**合并阈值**：拖延删除插入时的合并操作

**可变长键**

- **指针**：只存储指向键的指针，而不是直接存储键(`<key-ptr, value>`)。
  - 效率很低。嵌入式设备会在生产中使用，其微小的寄存器和缓存可以从这种节省的空间中受益
- **可变长节点**：允许可变长度的节点
  - 不可行。由于处理可变长度节点的内存管理开销很大，所以基本上不使用
- **填充**：把短数据填充到最长数据的空间
  - 不可行。是对内存/存储的极大浪费。
- **Key Map**：额外存储一个`<key-ptr, kv-offset>`的映射
  - 几乎所有人都在用。
  - 甚至有足够的空间放置每个键的一个前缀`<prefix, <key-ptr, offset>>`，可能允许一些索引搜索和扫描

**中间节点访问方式**

到达节点后，仍然需要在节点内进行搜索(要么从内部节点查找下一个节点，要么在叶子节点中查找键值)

- **线性扫描**
- **二分查找**



## Q：B+Tree的优化

**Prefix Compression / 前缀压缩**

大多数情况下，叶节点中的大多数键的前缀会有重叠。我们可以简单地在节点的开头存储该前缀一次，然后只在每个槽中包含每个键的剩下一部分。

![prefix-compression](/static/image/2022-02-19/prefix-compression.png)

**Deduplication / 解冗余**

在允许非唯一键的索引的情况下，冗余键会生成多个相同的key`(key1, val1), (key1, val2)`。这里优化的方法是只写一次Key，然后在它后面跟着它的所有相关值：`(key, val1, val2, ...)`

**Bulk Insert / 大量插入 / 快速初始化**

当最初构建B+树时，可以直接构造一个有序的叶节点链表，然后从下向上构建索引，那么数据的初始插入将更加高效。

如果Bulk insert带上faster merging的话，就可以做到非初始化时的大量插入。下面是快速合并的一些方法：
1. 离线：阻止所有操作去合并
2. eager / 早访问：同时从两个index里做访问，如果查询结果有交集，那么用最新的
3. 后台合并：分布式下空闲机合并 / 空闲时合并
4. 懒合并：如果leaf没有变化，那么不合并



## Q：Hash Table如何做并发？

**Static Hash Table**

所有线程都沿着相同的方向移动，且线程一次也只访问一个页面，所以死锁是不可能的。因为没有两个线程会竞争由另一个线程持有的锁。所以直接上读写锁即可并发。

**Dynamic Hash Table**

其实是与静态hash table相同的做法，只不过有更多状态需要锁存。顺便CAS天然支持Insert操作，只需保证元素的CAS是原子性的即可。



## Q：B+Tree 如何做并发？

B+Tree做并发的主要问题在于如何解决一个线程遍历树，同时另一个线程拆分/合并节点。

**Latch Crabbing / Latch Coupling**

这是一个允许多个线程同时访问/修改B+Tree的协议：

1. 在访问的同时，从上到下给路径上的每个节点加锁。

2. 如果当前节点是安全的，那么释放所有父节点的锁。这里的安全是指，该节点不会拆分（插入时不是满的）或不会合并（删除时超过一半满的）。

**Basic Latch Crabbing Protocal**

**搜索**：从上到下，反复地获取子结点的读锁，然后释放结点的闩锁。

**插入 / 删除**：从上到下，根据需要获得写锁。一旦发现操作是安全的，那么释放他所有祖先的写锁。

注意：这里释放锁的顺序没有特别的讲究，不过从根开始释放会更好，因为根有更多的访问量。

**Impoved Latch Crabbing Protocal / 乐观的Latch Crabbing Protocal**

因为在插入删除时总需要根节点上获得独占锁，所以这限制了并行性。我们假定到目标叶节点的路径是大多数安全的，所以这里用读锁访问到叶节点检查安全性。

**搜索**：以前一样。

**插入 / 删除**：设置读锁访问叶节点，如果叶节点安全，那么设置叶子上的写锁；如果叶子不安全，使用之前的插入删除协议重新启动事务



## Q：对比B-Tree和LSM-Tree？

> - DDIA Chpt.3
> - https://pingcap.com/zh/blog/10-questions-tidb-structure

根据经验，LSM-tree通常对于写入更快，而B-tree被认为对千读取更快。读取通常在LSM-tree上较慢，因为它们必须在不同的压缩阶段检查多个不同的数据结构和SSTable。下面从LSMT来做对比：

- **优点**
  - **更少的开销**：不需要两次写盘（日志&脏页）、不需要做叶节点分裂合并、不需要为部分更新做双写、不会出现随机写
    - **多页合并 & 批量写**：可以一次性从磁盘读出多个连续页做合并，然后一次性向磁盘顺序写回多个SSTable。
  - **更低的写放大**：以顺序方式写入紧凑的SSTable文件，而不必重写树中的多个页
  - **更好的压缩**：比BTree有更少的碎片，由千LSM- tree不是面向页的， 并且定期重写SSTables以消除碎片化， 所以它们具有较低的存储开销
- **缺点**
  - **读写放大**
    - 读取数据时实际读取的数据量大于真正的数据量。例如在LSM树中需要先在MemTable查看当前key是否存在，不存在继续从SSTable中寻找
    - 写入数据时实际写入的数据量大于真正的数据量。例如在LSM树中写入时可能触发Compact操作，导致实际写入的数据量远大于该key的数据量。
  - **磁盘负载**：虽然可以增量压缩，但由于磁盘的并发资源有限， 所以当磁盘执行昂贵的压缩操作时， 很容易发生读写请求等待的情况。
    - LSM存储的查询响应时间有时会很高（对于很早的写入）， 而B-tree的响应延迟则更具确定性
    - 数据库的数据量越大， 压缩所需的磁盘带宽就越多。
    - 如果写入吞吐量很高并且压缩没有仔细配置， 那么就会发生压缩无法匹配新数据写入速率的情况。因此需要额外的监控措施

用更简单的话来说：

1. 对于 B+Tree 来说每个写入，都至少要写两次磁盘： 1. 在日志里； 2. 刷新脏页的时候。即使你的写可能就只改动了一个 Byte，这个 Byte 也会放大成一个页的写 （在 MySQL 里默认 InnoDB 的 Page size 是 16K），虽然说 LSM-Tree 也有写放大的问题，但是好处是 LSM-tree 将所有的随机写变成了顺序写
2. LSMTree 对压缩更友好，数据存储的格式相比 B+Tree 紧凑得多

# Operator Execution / 操作符执行

## Q：Sorting的原理

排序可能会在ORDER BY、GROUP BY、JOIN和DISTINCT操作符中使用。对于太大而无法装入内存的数据，标准的排序算法是外部归并排序：

- **Phase #1 – Sorting**：首先，该算法对装入主内存的小块数据进行排序，然后将排序后的页面写回磁盘

- **Phase #2 – Merge**：然后，该算法将排序后的子文件合并成一个更大的文件

**二路归并**

这个算法最基本的版本是二路归并排序。算法在排序阶段读取每个页面，对其进行排序，并将排序后的版本写入磁盘。然后，在合并阶段，它使用三个缓冲页进行（两个输入page，一个输出page）。

![2-way-merge-sort](/static/image/2022-02-19/2-way-merge-sort.png)

**I/O Cost**： $2N*(1 + ⌈log_2N⌉) $

其中，N是数据页个数，`1 + ⌈log2(N)⌉`是指sort和merge的总次数，`2N`是指每次对一个页的读写次数。

**K路归并**

该算法的一般化版本允许DBMS利用三个以上的缓冲页。设B是可用缓冲页的总数。在排序阶段，算法可以一次读取B个页面，并将`⌈N/B⌉`排序运行写回磁盘。合并阶段也可以在每个通道中合并B−1运行，再次使用一个缓冲页来合并数据，并在需要时写回磁盘。

**I/O Cost**： $2N*(1 + ⌈log_{B-1}⌈\frac{N}{B}⌉⌉) $

其中，B-1是每次可以进行合并的page个数。



## Q：Sorting优化

**双缓冲优化**：外部归并排序的一种优化方法是在后台预取下一次运行，并在系统处理当前运行时将其存储在第二个缓冲区中。通过持续使用磁盘，这减少了每一步I/O请求的等待时间。

**Clustered B+Tree**：如果索引是聚集索引，DBMS可以遍历B+树。由于索引是聚集的，数据将按照正确的顺序存储，因此I/O访问将是顺序的。



## Q：Aggregations的原理

聚合操作符将多个Tuple的值折叠为单个标量值。一般有两种实现聚合的方法：

**Sorting**

DBMS首先在GROUP BY键上对元组进行排序。它可以使用内存中的排序算法(例如，快速排序)，或者使用外部归并排序算法(如果数据的大小超过了内存)。然后DBMS对排序的数据执行顺序扫描以计算聚合。

**Hashing**

对于计算聚合而言，Hashing比排序运算成本更低。DBMS在扫描表时填充一个哈希表。对于每条记录，检查哈希表中是否已经有一个条目，并执行reduce。

- **Phase #1 – Partition**：使用哈希函数，根据目标hash键将Tuple映射成磁盘上的分区。这将把所有带有相同键的Tuple放入同一个分区（即某个key不可能在别的分区存储）。
- **Phase #2 – ReHash**：对于磁盘上的每个分区，将其页面读入内存。使用另一个哈希函数，构建一个**内存哈希表**`<GroupByKey, Tuple>`。然后遍历这个哈希表的每个bucket，将同一个Key的Tuple放在一起来计算聚合。

注意：partition在这里是为了降低rehash的内存空间，因为相同的GroupKey已经在同一个bucket里，这个GroupKey的结果可以在本bucket扫描结束后直接提交。

![aggregations-partition](/static/image/2022-02-19/aggregations-partition.png)

![aggregations-rehash](/static/image/2022-02-19/aggregations-rehash.png)

![aggregations-hashing-summary](/static/image/2022-02-19/aggregations-hashing-summary.png)



## Q：Join的原理

这里的链接专指等值链接，非等值链接大差不差。

**运算符的输出**

联接操作符生成的输出Tuple的内容是不同的。它取决于DBMS的查询处理模型、存储模型和查询本身：

- **early materialization / 早实体化**：这种方法将两个表中的属性值复制到结果中。这里可以省略部分属性来减少内存消耗。
  - 优点：Query Plan中的操作符不需要回表来获取数据
  - 缺点：需要更多的内存。
- **late materialization / 晚实体化**：DBMS只复制连接键和匹配元组的记录id
  - 列存储天然支持这种方法，因为DBMS不会复制查询不需要的数据。

**Nested Loop Join / 嵌套循环**

在高层次上，这种类型的连接算法由两个嵌套的循环组成，判断Tuple匹配连接并输出。外层for循环中的表称为外层表，而内部for循环中的表称为内部表。

下面的开销分析中，外表R中有M个页面，其Tuple个数为m；内表S中有N个页面，其Tuple个数为n。

- **Simple Nested Loop Join / 简单嵌套**：对于外部表中的每个元组，将其与内部表中的每个元组进行比较
  - **IO Cost**：$M+(m*N)$

![nested-loop-join](/static/image/2022-02-19/nested-loop-join.png)

- **Block Nested Loop Join / 块嵌套**：对于外部表中的每个块，从内部表中获取每个块，并比较这两个块中的所有元组。
  - **IO Cost**：$M+(M*N)$
  - 如果DBMS有B个缓冲区可用来计算连接，那么它可以使用B - 2个缓冲区来扫描外部表（一个缓冲区扫描内表，一个缓冲区存储输出）。**IO Cost**：$M+(⌈\frac{M}{B-2}⌉*N)$

![block-loop-join](/static/image/2022-02-19/block-loop-join.png)

- **Index Nested Loop Join / 索引嵌套**：如果数据库已经在连接键上为内表建立了索引，那么它可以使用该索引来加速比较。
  - **IO Cost**：$M+(m*C)$

![index-loop-join](/static/image/2022-02-19/index-loop-join.png)

**Sort-Merge Join / 归并连接**

![sort-merge-join](/static/image/2022-02-19/sort-merge-join.png)

当一个或两个表已经**按照连接属性排序**(比如聚集索引)，或者**输出需要按照连接键排序**，那么这个算法是有用的。

对于这个算法，最坏的情况是两个表中所有元组的join属性包含相同的值，这在真实的数据库中是不太可能发生的。此时，合并成本为M·N。

- **Sort Cost**：$2N*(1 + ⌈log_{B-1}⌈\frac{N}{B}⌉⌉) + 2M*(1 + ⌈log_{B-1}⌈\frac{M}{B}⌉⌉) $
- **Mege Cost**：$M+N$

**Hash Join / 哈希链接**

哈希连接的思想是使用哈希表根据Tuple的连接属性将其分割成更小的块。这减少了DBMS需要对每个元组执行比较来计算连接的次数。**散列连接只能用于等值连接。**

- **Basic Hash Join**
  - **Phase #1 – Build**：扫描外表，并使用JoinKey做hash填充哈希表
  - **Phase #2 – Probe**：扫描内表，根据JoinKey跳转到外表的哈希表对应值上，然后做链接并输出
  - 如果DBMS知道外部表的大小，则第一步可以使用静态哈希表
  - 使用Bloom Filter可以加速第二步。它可以使用更少内存来判断不存在性，减少了Hash表的读入。
  - 当表不适合放在主内存中时，DBMS不得不基本上随机地进进出出读hash表，这将导致较差的性能。

![basic-hash-join](/static/image/2022-02-19/basic-hash-join.png)

- **Grace Hash Join**：它也将内部表散列到写入磁盘的分区，但是不会出现hash表的频繁读入。
  - **Phase #1 – Build**：扫描内外表，并使用JoinKey做hash填充哈希表分区。
    - 如果需要插入到一个已满的bucket，这时可以使用不同的哈希函数h2，使用递归分区来进一步划分bucket。这可以避免多对多的Nest Loop，减少频繁读入。
  - **Phase #2 – Probe**：对于每个层的Bucket，查询内外表的相应页面，然后执行嵌套循环连接。这些页面可以装入内存，因此这个连接操作将会很快。

**IO Cost**：$2(M+N) + (M+N)$

![grace-hash-join](/static/image/2022-02-19/grace-hash-join.png)

![grace-hash-join-recursive](/static/image/2022-02-19/grace-hash-join-recursive.png)

**总结**

![join-summary](/static/image/2022-02-19/join-summary.png)

在等值链接时，系统默认使用HashJoin。当有两个条件满足时用SortMerge。后续的基于cost的优化器可以来做选择。



## Q：操作符的处理模型是什么样的？

DBMS的处理模型定义了系统如何执行查询计划。这些模型也可以实现为从上到下或从下到上调用操作符。虽然从上到下的方法更为常见，但从下到上的方法可以更严格地控制缓存/寄存器。

**Iterator Model / 迭代器模型**

迭代器模型，也称为Volcano或Pipeline模型，是最常见的处理模型，几乎每个行存储DBMS都使用它。支持的数据库：SQLite、MongoDB、DB2、SQL Server、PostgreSQL、Oracle、MySQL。

迭代器模型通过为数据库中的每个操作符实现Next函数来工作。查询计划中的每个节点调用子节点上的Next，直到到达叶节点，叶节点开始向其父节点发出元组以进行处理。

![iterator-model](/static/image/2022-02-19/iterator-model.png)

查询计划中为给定Tuple执行的一系列任务称为**pipeline**。其中有些操作符会阻塞，直到子操作符发出它们的所有元组。此类操作符的例子包括<u>连接、子查询和排序(ORDER BY)</u>，他们被称为**pipeline breaker**。

**Materialization Model / 实体化模型**

支持的数据库：VoltDB、monetDB

在迭代器模型中，每个操作符同时处理其所有的输入，然后同时发出所有的输出。每个查询计划操作符都实现了一个Output函数。这个函数的返回结果是操作符将发出的所有元组。

![materialization-model](/static/image/2022-02-19/materialization-model.png)

- 它更适合于OLTP工作负载，因为查询通常一次只访问少量的元组。因此，检索元组的函数调用更少。
- 它不适用于具有较大中间结果的OLAP查询，因为DBMS可能不得不在操作符之间将这些结果溢出到磁盘上。

**Vectorization Model / 向量模型**

支持的数据库：Oracle、DB2、SQL Server、RedShift、snowflake、presto。

每个操作符发出的是一批数据(即向量)，而不是单个元组。它是为处理批量数据而不是一次处理单个项目而优化。

![vectorization-model](/static/image/2022-02-19/vectorization-model.png)

它适合OLAP查询，因为可以减少对Next函数的调用。



## Q：访问操作符

访问方法是DBMS访问存储在表中的数据的方式。一般而言只有两种：

**Sequential Scan / 顺序扫描**

顺序表扫描几乎总是DBMS执行查询时效率最低的方法。有许多优化可以帮助使顺序扫描更快：**预取**、**绕过Buffer Pool**、**并发扫描**、**晚实体化**

**Index Scan / 索引扫描**

- 选取哪个索引来做扫描是很关键的，这个过程中涉及到很多因素：索引包含什么属性、查询什么属性、属性的值域
  - 比如，在第一个场景中，最好在扫描中使用dept索引，因为它只有两个元组要匹配。在第二种情况下，age索引将消除更多不必要的扫描。
  - 后续讲到使用Histogram来优化查询。

![choosing-index](/static/image/2022-02-19/choosing-index.png)

- **多索引扫描**：使用每个匹配的索引计算ID集合，再根据查询的谓词取交集/并集。
  - DBMS可以使用位图、哈希表或布隆过滤器通过集合交集来计算记录id



## Q：where语句 / 谓词如何被执行？

DBMS将WHERE子句表示为一个表达式树，其中节点表示不同的表达式类型。

DBMS将遍历树来计算它的操作符并产生一个结果。以这种方式计算谓词很慢，一个优化是直接对表达式求值，减少节点个数。

![expression-evaluation](/static/image/2022-02-19/expression-evaluation.png)

# Query Plan / 查询计划

## Q：什么是Query Plan？生成Query Plan的步骤？

因为SQL是声明式的，所以查询只告诉DBMS要计算什么，而没有告诉DBMS如何计算。因此，DBMS需要将SQL语句转换成可执行的查询计划。查询计划也就是可以执行的Operator组成的树。

![query-plan-overview](/static/image/2022-02-19/query-plan-overview.png)

1. SQL查询：客户端向DBMS系统发送一个SQL查询。
2. SQL重写：该查询可能被重写为不同的格式（不过很少有人做）。
3. 生成AST：SQL字符串被解析为组成语法树的标记。
4. 生成逻辑计划：绑定器通过查询系统Catalog将AST中的命名对象转换为内部标识符（如表、属性等）。
5. 优化逻辑计划：这里Tree Rewriter将应用一些静态规则进行大致优化。
6. 生成物理计划：将逻辑计划交给优化器，优化器通过cost model选择最有效的过程来执行计划。



## Q：什么是Query Optimizer？

在查询计划中执行每个操作符有不同的方法(例如，连接算法)，这些计划之间的性能也会有所不同。查询优化器的工作是为任何给定的查询选择一个最佳的计划。一般来说，优化器有两种实现方法：

- **static rules / heuristics / 启发式方法**：将查询的部分与已知的模式进行匹配，以重组一个查询计划。一般用于优化逻辑计划。
  - 这些规则是为了优化效率低下的查询。
  - 尽管这些规则可能需要查看Catalog以了解数据的分布，但它们从来不需要检查数据本身。
- **cost-based search / 基于成本的搜索**：读取数据并估计执行等价计划的成本，寻找接近最低成本的查询计划。一般用于优化物理计划。
  - 大多数新的数据库系统只使用启发式，而不是复杂的成本模型来选择访问方法。

关于ML的优化器，学术界有很多研究，但是工业界没有用任何ML方法。



## Q：逻辑计划与物理计划是什么？

- **逻辑计划**：指逻辑代数表达式层面的计划。大致相当于查询中的关系代数表达式。
  - 这里的理论依据在于，如果两个关系代数表达式生成相同的元组集，则它们是等价的。
- **物理计划**：指物理操作符的特定执行策略。这取决于所处理数据的物理格式(即排序、压缩)。



## Q：常见逻辑查询优化有哪些？

**selection optimization / 谓词优化**

- **谓词下推**：尽可能早地执行过滤器。
- **谓词重排**：首先应用最具选择性的谓词。
- **谓词拆分**：拆分一个复杂谓词并将其向下推。
- **谓词合并**：比如合并多个相交的range谓词。

**projection optimization / 投影优化**

- **投影下推**：尽可能早地执行投影以创建更小的元组并减少中间结果。
- **按需投影**：只投影出要求的属性。

**subquery optimization / 子查询优化**

- **解除关联 / 扁平化**：将子查询变为join操作。
- **提出子查询**：如果每个子查询都不依赖外部查询，那么可以提出一个公共查询出来。

下面是一个逻辑计划优化的例子

![query-optimization-example](/static/image/2022-02-19/query-optimization-example.jpeg)



## Q：成本估计的过程是怎么样的？

这里的成本估计是指，根据内部统计信息，来预测谓词能够选择出多少Tuple。这些内部统计信息可以在后台更新。

**selection cardinality / 基于选择基数**

在计算谓词的选择基数时，会使用一些假设：数据分布均匀（每个key都出现N次）、属性间互相独立（名字与性别无关、或年龄与院系无关）等。虽然真实的数据往往不能满足这些假设。这样我们可以根据Tuple总数估计某些谓词能够产出多少Tuple。

**Histogram / 基于直方图**

真实的数据往往是扭曲的，很难做出假设。既然如此，不妨将真实数据分布做一些压缩。

- **Histogram / 直方图**：一种减少内存使用量的方法是用直方图存储数据。比如`sel(age < 50)`可以直接从直方图中读取出来。
  - **累计直方图**：这里更好的方法是用累计直方图，就像Promethues那样，可以支持`O(1)`的范围查询和点查询。

- **Quantiles / 分位图**：另一种方法是使用等深度直方图，它改变桶的宽度，以便每个桶的总出现次数大致相同

**Sampling / 基于采样**

DBMS可以使用抽样将谓词应用到具有类似分布的表的较小副本上（可能是10%）。这样也算压缩了真实分布。



## Q：基于成本的优化过程是什么样？计划枚举是什么？

在执行基于规则的重写之后，DBMS将枚举查询的不同计划，并估计它们的成本。然后在耗尽所有计划或某个超时后为查询选择最佳计划。

**Single-Relation Query Plans**

对于单关系查询计划，最大的障碍是选择最佳的访问方法(例如，顺序扫描、二分搜索、索引扫描等)。

- 对于OLTP查询，这特别容易。因为它们是sargable，这意味着存在一个可以为查询选择的最佳索引。

- sargable是指总可以使用索引来访问数据。一个non-sargable的例子是，where中的列值做不可逆函数，那么是算不出这个列值在索引中的key

大多数新的数据库系统只使用启发式，而不是复杂的成本模型来选择访问方法。

**Multi-Relation Query Plans**

随着连接数量的增加，可选计划的数量也迅速增加。我们选择left-deep joining tree来做这件事，这是因为左深连接树更适合管道模型，因为DBMS不需要具体化连接操作符的输出。

![left-deep-join-tree](/static/image/2022-02-20/left-deep-join-tree.png)

下面三种方法，都是为了确定连接顺序、每个操作符的实现（如HashJoin / SortMergeJoin）、表的访问方式（如index#1、index#2、seq scan等）。

- **DP**：对每个状态做最低cost的状态转移。

![dp-for-query-optimization](/static/image/2022-02-20/dp-for-query-optimization.png)

- **随机优化方法**：PostgreSQL在12个及以上的表join时，用遗传算法来做优化。
  - 一种蒙特卡罗算法，具有一类随机优化方法的特性：采样越多越接近最优解

![genetic-optimization](/static/image/2022-02-20/genetic-optimization.png)

- **枚举**：为每个步骤进行枚举，这个过程可以带剪枝。

![enum-step-1](/static/image/2022-02-20/enum-step-1.png)

![enum-step-2](/static/image/2022-02-20/enum-step-2.png)

![enum-step-3](/static/image/2022-02-20/enum-step-3.png)



# Concurrency Control / 并发控制

并发控制覆盖了操作符执行和访问方法，它本质上是控制事务中操作的调度。

## Q：ACID是什么？

- **原子性**：原子性确保事务中的所有动作都发生，或者不发生。
  - 关注点在<u>事务有回退的能力</u>，而且回退过程需要考虑故障回复。
- **一致性**：在事务开始时数据库是一致状态的，那么在事务完成时数据库是一致状态的。
  - 这里的一致性主要是通过用户定义，由DBMS来维护的。如一个人年龄不能是负数（数据库一致性），转账前后总钱数不变等（事务一致性）。
- **隔离性**：隔离意味着当一个事务执行时，它应该有一个与其他事务隔离的错觉。
- **持久性**：如果一个事务提交，那么它对数据库的影响应该持久。
  - 关注点在<u>崩溃或重新启动</u>后，提交的事务的所有更改必须是持久的。这里需要考虑内存中的脏页写回磁盘的问题。
  - 其实原子性的实现方式恰好能满足持久性：Redo Log / Shadow Paging



## Q：ACID - 原子性如何实现？

- **Undo Log**：记录所有操作，以便撤消中止事务的操作。几乎所有的现代系统都使用日志记录。
- **Shadow Paging**：复制被事务修改的页面，让事务对这些副本进行更改。当事务提交时，与主页面做切换。
  - 它在运行时的开销大于恢复的开销，而真实场景下故障并不场景，因此在实践中很少使用这种方法。
  - 支持的数据库：CouchDB、LMDB



## Q：ACID - 隔离性的理论依据？

DBMS执行操作的顺序称为**调度（execution schedule）**。并发控制协议的目标是生成一个与串行执行等价的执行计划：

- **Serial Schedule / 串行调度**：不交叉不同事务的行动的计划。指事务AB，要么A先做完B再开始，要么相反。
- **Equivalent Schedules / 等价调度**：对于任何数据库状态，如果执行第一个调度的效果与执行第二个调度的效果相同，那么两个调度是等价的。
- **Serializable Schedule / 可串行化调度**：可串行化调度是一类等价于串行调度的调度。

**Conflict Serializability / 冲突可串行化**

- 如果两个调度的每对冲突操作都以相同的顺序进行，那么它们就是**冲突等价（conflict equivalent）**的。

- 如果调度S与某个串行调度冲突等效，那么它就是**冲突可串行化（conflict serializable）**的。

- 验证冲突可串行化的方法是使用依赖关系图。如果依赖图是无循环的，则调度是冲突可串行化的。

![dependency-graph](/static/image/2022-02-20/dependency-graph.png)

![universe-of-schedules](/static/image/2022-02-20/universe-of-schedules.png)



## Q：phenomena / 事务异像是什么？有哪些？

> - A Critique of ANSI SQL Isolation Levels
>
> - [transaction phenomena](http://xianmu.github.io/posts/2019-01-19-transaction-phenomena.html)
>
> - [The SQL92 Standard](http://www.contrib.andrew.cmu.edu/~shadow/sql/sql1992.txt)

如果两个操作针对的是不同的事务，它们是在同一个对象上执行的，并且至少有一个操作是写操作，那么两个操作之间就会发生**冲突（conflict）**。当冲突发生时，可以说出现了**事务异像（phenomena）**。以下是SQL92定义的三种异常：

- **读写冲突 - Unrepeatable Reads / 不可重复读**：多次读取同一个对象时不能得到相同的值。
- **写读冲突 - Dirty Reads / 脏读**：可以看到在另一个事务之前的写。
- **Phantom / 幻读**：指读取两次记录的结果集合不同，原因是另一个事务在此期间insert / delete。

真实世界下还存在更多的异常，下面是`A Critique of ANSI SQL Isolation Levels`论文中提到的异常：

- **写写冲突 - Lost Updates / 更新丢失**：一个事务将覆盖另一个并发事务未提交的数据。
  - 更新丢失更多是指丢失的更新会造成一致性异常。如` R1[x], W2[x], C2, W1[x], C1`，这里事务1并不知道值改变了，如果事务2希望的是将存款清零，那么事实上事务1并未执行。
- **写写冲突 - Dirty Write / 脏写**：T2覆盖了未提交的T1的写，当T1回滚后，T2的更新丢失。
  - 区别于Lost Update，它是在某一个事务回滚后才出现问题。
- **Read Skew / 读偏移**：`R1(x)，W2(x), W2(y), R1(y)`，此时事务1读出来的x和y不满足一致性约束。
- **Write Skew / 写偏移**：两个并发事务都按照自己读到数据的一致性要求，去修改对方的数据。（会发生在快照隔离等级）



## Q：隔离等级有哪些？分别解决了什么异常？

> - http://www.bailis.org/blog/when-is-acid-acid-rarely/

下面是SQL92提出的一些隔离等级：

![isolation-levels](/static/image/2022-02-20/isolation-levels.png)

除此之外还有两个额外的隔离级别：

- **CURSOR STABILITY / 游标稳定性**：防止丢失更新（Lost Update）异常，是可重复读和读已提交之间的隔离性
  - 是IBM DB2中的默认隔离级别。
- **SNAPSHOT ISOLATION / 快照隔离**：保证在一个事务中进行的所有读取都能看到事务启动时存在的数据库的一致性快照。
  - 只存在写写冲突：事务只有在其写操作与该快照之后的任何并发更新不冲突时才会提交。
  - 容易导致Write Skew异常。

顺带一提，SQL92还提出了读写和只读的访问模式，只不过没有多少数据库实现了只读模式。

```
SET TRANSACTION [ READ WRITE | READ ONLY ];
BEGIN TRANSACTION [ READ WRITE | READ ONLY ];
```



## Q：如何实现隔离等级？哪些数据库支持什么等级？DBA常用哪个等级？

下面是一个提供SQL92各个隔离等级的一种方案：

![a-solution-to-isolation-levels](/static/image/2022-02-20/a-solution-to-isolation-levels.png)

一些DBMS支持的默认和最大隔离等级：

![isolation-levels-supporting](/static/image/2022-02-20/isolation-levels-supporting.png)

一个调查报告显示，DBA倾向于使用读已提交隔离等级。

![isolation-levels-survey](/static/image/2022-02-20/isolation-levels-survey.png)



## Q：幻读是如何解决的？

> - https://dev.mysql.com/doc/refman/8.0/en/innodb-locking.html#innodb-gap-locks

幻读是一种特殊的异常，因为其他异常都是能从调整调度到与串行化调度相同来解决的。或者说insert和delete是一种针对表的特殊写。一些解决方案如下：

- **表锁**：在2PL的场景下，插入删除操作需要申请表写锁。
- **提交前重扫描**：如果没有出现幻读情况，那么提交；反之则重启。
- **索引锁 / 间隙锁**：
  - 如果有针对某个属性的索引，那么锁住对应的Page；如果索引中没有记录，那么锁住可能存在的位置
  - 如果没有这种索引，那么锁住所有page / 锁表。

对于MySQL而言，有Record Lock & Gap Lock来处理这件事。下面是我在读文档时想弄清楚的一些问题：
- 为什么record lock要锁前后区间防止inserting and deleting？那么Gap Lock的意义在哪里？
- 为什么在快照隔离等级里引入 NextKey Lock 来预防幻读？快照隔离是否天然可防止幻读？
- 为什么record lock要在非索引下创建索引，然后锁住索引record？不会导致索引变多么？如果是多个谓词条件查询情况下呢？



## Q：悲观 / 乐观并发控制协议是什么？

并发控制协议是DBMS如何在运行时从多个事务中决定操作的适当交错。并发控制协议有两类:

- **悲观并发控制**：假定事务之间经常发生冲突，所以它从事务开始就使用严格的方法，不让问题出现。
- **乐观并发控制**：假定事务之间的冲突很少发生，因此它选择在事务提交时发送冲突再处理问题。



## Q：2PL和S2PL是什么？执行过程如何？

两阶段锁定是一种悲观并发控制协议，它使用锁来确定是否允许事务动态地访问数据库中的对象。协议不需要提前知道事务将执行的所有查询，所以它提供了可交互的能力：

- **Phase #1– Growing**：每个事务向锁管理器请求它需要的锁。
- **Phase #2– Shrinking**：事务在释放第一个锁后立即进入收缩阶段。在收缩阶段，事务只允许释放锁。

![2pl-locking](/static/image/2022-02-20/2pl-locking.png)

注意：

- 2PL本身就足以保证冲突可串行化。它生成的调度优先图是非循环的。
- **死锁**：两个事务互相获取对方持有的锁。

- **级联回滚（cascading aborts）**：当一个事务中止时，现在必须回滚另一个事务，这会导致浪费。
  - 当T1进入释放阶段后执行某些操作时，T2获得锁，并在T1的修改上进行读写。当T1最终abort，T2的读写也失效了，于是也需要abort。
  - 其实就是由于脏读的产生导致级联回滚，下面的S2PL会解决这个问题。
  - 事实上，DBMS很难从用户得知什么时候应该进入Shrink阶段，所以大多数系统使用S2PL？

**Strict Two-Phase Locking / 严格两段锁**：S2PL是2PL的一种变体，事务只在提交时释放锁。

![s2pl-locking](/static/image/2022-02-20/s2pl-locking.png)



## Q：2PL下死锁如何解决？

**Deadlock Detection / 死锁检测**

系统在后台线程中周期性地检查事务等待图。当DBMS检测到死锁时，它将中止其中一个事务以打破这个循环。

- **终止因素**：选择哪一个事务来终止呢？常见的考虑因素有：年龄、锁个数、等待它的事务数等。许多系统使用这些因素的组合。
- **回滚范围**：在选择一个受影响的事务中止之后，DBMS还可以决定回滚该事务的更改的范围。它可以回滚整个事务，也可以回滚到足以打破死锁的某个操作。

![deadlock-detection](/static/image/2022-02-20/deadlock-detection.png)

**Deadlock Prevention / 死锁预防**

死锁预防是指在申请锁时如果已有事务占用，那么依据事务优先级来抢占或正常等待锁，即在发生死锁之前阻止事务。这里的优先级需要保证它总是偏序的，一般而言，将根据时间戳分配优先级（旧的事务具有更高的优先级）。

- **Wait-Die (“Old waitfor Young”)**：如果请求事务的优先级更高，则等待。否则，终止请求事务并抢占锁。 
- **Wound-Wait (“Young for Old”)**：如果请求事务的优先级更高，终止持有锁的事务并抢占。否则，请求事务将等待。

（不少系统使用了后者，比如Spanner）

![deadlock-prevention](/static/image/2022-02-20/deadlock-prevention.png)



## Q：锁的多粒度如何实现？

多数DBMS实现了表锁和行锁，并提出了意向锁。意向锁的意义在于提供一种优化：当申请表锁时可以不用扫描检查每一个行锁是否冲突。如果节点处于意图模式，则显式锁定将在树的较低级别执行。

- **Intention-Shared (IS) / 意向共享锁**：当事务给行加共享锁前，必须先取得该表的IS锁。表示使用共享锁在较低级别上显式锁定。
- **Intent-Exclusive (IX) / 意向独占锁**：当事务给行加独占锁前，必须先取得该表的IX锁。表示使用排他锁或共享锁在较低级别上显式锁定。
- **Shared+Intention-Exclusive (SIX) / 共享+意向独占**：根节点的子树在共享模式下是显式锁定的，显式锁定是在较低级别上用排他模式锁定完成的。
  - 本质上是原子性的申请了S和IX锁，常用于Update扫描表，一边扫描一边修改。

![intent-locking-compatibility-matrix](/static/image/2022-02-20/intent-locking-compatibility-matrix.png)

![explicitly-locking](/static/image/2022-02-20/explicitly-locking.png)



## Q：乐观并发控制协议有提到哪些？

**Basic Timestamp Ordering (BASIC T/O) / 基本时间戳排序**

每个数据库对象X都存在一个最后更新的事务时间戳，如读操作（记`R-TS(X)`）或写操作（记`W-TS(X)`）。

如果一个事务试图以一种违反时间戳顺序的方式访问一个对象，该事务将被中止并重新启动。潜在的假设是，违规行为将是罕见的，因此这些重启也将是罕见的。

- **读操作**
  - 当`TS(Ti) < W-TS(X)`，则违反时间戳顺序，所以事务将被中止，并使用一个新的时间戳重新启动。
  - 否则，Ti被允许读取X。然后更新`R-TS(X) = max(R-TS(X), TS(Ti))`。同时在事务过程中保存X的副本，以确保可重复读。
- **写操作**
  - 如果`TS(Ti) < R-TS(X) || TS(Ti) < W-TS(X)`，则重启Ti。
  - 否则，DBMS允许Ti写X并更新`W-TS(X)`。同时生成X的副本，以确保Ti的可重复读。

注意：

- **复制开销**：将数据复制到事务工作区和更新时间戳带来的高开销。
- **长事务饥饿**：长时间运行的事务可能会饿死。
- **时间戳瓶颈**：在高并发系统上受时间戳分配瓶颈。
- 所以基本没人用它。



**Optimistic Concurrency Control (OCC) / 乐观并发控制**

这里的OCC只是一种协议而已，跟乐观并发控制协议这个分类无关。

每个事务将保存一个**私有工作空间**。任何读写的对象都被复制到工作区中，并在那里修改。当事务提交时，DBMS会比较事务的工作区写操作，以查看它是否与其他事务冲突。如果没有冲突，则将数据写入全局数据库中。这个过程会有三步：

1. **Read Phase**：跟踪事务的读写，并将它们的写存储在一个私有工作区中。
2. **Validation Phase**：当一个事务提交时，DBMS检查它是否与其他事务冲突。
   - 在这个过程中，当前事务会与它开始之前或者提交之后的事务的时间戳和读写集做比较。
   - 如：T1在T2开始之前提交、T1和T2在第一阶段之前提交并且二者没有写集交集等。
3. **Write Phase**：如果验证成功，DBMS将私有工作区更改应用到数据库。否则，它将中止并重新启动事务

注意：

- **复制开销**：将数据复制到工作区带来的高开销。
- **时间戳瓶颈**：在高并发系统上受时间戳分配瓶颈。
- 似乎也没人用它。

![occ-example](/static/image/2022-02-20/occ-example.png)



## Q：MVCC是什么？设计MVCC需要考虑什么？

多版本并发控制（Multi-Version Concurrency Control）涉及DBMS设计和实现的各个方面。MVCC是dbms中应用最广泛的方案，几乎10年内所有新的DBMS都使用它。

MVCC使得数据库中单个逻辑对象会对应多个物理版本，读写操作都会根据一定的规则来寻找或创建该对象的物理版本，所以只会存在写写阻塞。每一种MVCC的设计都需要考虑四个部分，并选择对应的可选方案：**并发控制协议**、**版本存储**、**GC**、**索引管理**。

![mvcc-impl](/static/image/2022-02-20/mvcc-impl.png)



**Concurrency Control Protocol / 并发控制协议**

这里的并发控制是指在两个事务在同一个对象上发生了写写冲突时的解决方案，因为版本链是不能有分叉的。之前的悲观乐观控制协议都可以作为解决方案，比如MySQL-InnoDB使用了MV-2PL协议。



**Version Storage / 版本存储**

每个Tuple会带有一个上一个版本指针，它们形成了一个**版本链（version chain）**。索引总是指向链的Header，线程将遍历链，直到找到正确的版本。不同的存储方案决定了每个版本的存储位置和内容：

- **Approach #1: Append-Only Storage**：Tuple的所有物理版本都存储在同一个Page空间中。
  - 每次更新只是将元组的一个新版本添加到Page中，并更新版本链。
  - 这个链可以是最老到最新的排序(O2N)，这需要在查找时遍历链；也可以是最新到最老的排序(N2O)，但这需要为新版本更新所有索引指针

![append-only-storage](/static/image/2022-02-20/append-only-storage.png)

- **Approach #2: Time-Travel Storage**：维护一个单独的时间旅行表，用来存储较旧版本的元组。
  - 每次更新时，DBMS将主表中旧版本Tuple复制到时间旅行表中，并用新数据覆盖主表。同时主表中的元组指针指向时间旅行表中的过去版本。

![time-travel-storage](/static/image/2022-02-20/time-travel-storage.png)

- **Approach #3: Delta Storage**：在时间旅行表中，只存储Tuple之间的增量或变化。
  - 事务可以通过遍历增量来重新创建旧版本。这导致了比时间旅行存储更快的写操作，但更慢的读操作

![delta-storage](/static/image/2022-02-20/delta-storage.png)



**Garbage Collection / 垃圾回收**

随着时间的推移，DBMS需要从数据库中删除**可回收（reclaimable）**的物理版本。如果没有活动事务能看到该版本，或者该版本是由已中止的事务创建的，则该版本可回收。

- **Approach #1: Tuple-level GC**：直接检查元组来查找旧版本
  - **Background Vacuuming / 后台清理**：后台线程定期扫描表并寻找可回收的版本。一个简单的优化是维护一个脏页BitMap，跟踪自上次扫描以来哪些页面被修改过。
  - **Cooperative Cleaning / 合作清理**：运行事务的线程在遍历O2N版本链时，顺带做可回收检查。
- **Approach #2: Transaction-level GC**
  - 每个事务维护自己的写对象的旧版本。当提交时，可以使用它来确定要回收哪些元组。

![tx-level-gc](/static/image/2022-02-20/tx-level-gc.png)



**Index Management / 索引管理**

所有索引中的Tuple数据总是指向版本链头，所以DBMS需要在创建新版本后维护所有索引。注意，二级索引因为需要回表所以不必做更新，同时Delta和TimeTravel存储方法因为本身不用改变地址所以也不需要更新。

- **逻辑指针**：做一个物理指针映射中间层，索引总是指向中间层，而中间层做指针改变。这需要两次访盘。
- **物理指针**：每个索引存物理位置，所以新版本到来时都要更新所有索引。

注意，MVCC中的插入和删除会引起索引中的**冗余键**问题：

- T1在读A对象，此时T2想删除A，同时有T3想插入新的A。
- 那么旧A不能删除（也不能打墓碑位），而新A需要添加在index，导致了冗余键。那么如何处理？

需要注意的是，我们只能当逻辑上没有Tx访问该tuple时物理上删除它。所以这里必须引入<u>逻辑上删除</u>的概念。当逻辑删除后需要插入时，必须锁住插入的Tx：
1. Delete Flag：可以放在tupler header或者一个新column
2. Tombstone Tuple：创建新的版本，指明该tuple需要被删除

其实MySQL在这里的MV2PL策略是锁住T2和T3，没有这个问题。

![mvcc-dup-key](/static/image/2022-02-20/mvcc-dup-key.png)

# Crash Recovery / 故障恢复

## Q：故障有那些类型？

**Transaction Failures / 事务失败**

事务失败发生在事务出现错误且必须中止事务时：**逻辑错误**（比如一致性、约束条件等）、**内部状态错误**（比如死锁）。

**System Failures / 系统失败**

系统故障是硬件或软件的偶然故障，在崩溃恢复协议中也必须考虑到这些故障：**软件故障**（如除零异常）、**硬件故障**（如电源断开）

**Storage Media Failure / 存储介质失败**

指物理存储损坏时发生的不可修复的故障。当存储介质发生故障时，DBMS必须从备份恢复。



## Q：Buffer Pool策略？脏数据能否写入磁盘？

Buffer Pool需要保证：任何已提交的事务都是持久化的，而且任何回退的事务操作不可持久化。这就要说道两个写回策略：

- **Steal Policy / 偷取策略**：是否允许未提交的事务，或脏数据写入磁盘。
- **Force Policy / 强制写回策略**：是否要求在事务提交之前，强制所有更新都写入磁盘。

![no-steal-plus-force](/static/image/2022-02-20/no-steal-plus-force.png)



## Q：Shadow Paging如何进行故障恢复？

在Shadow Paging下，DBMS需要维护两种空间：**主页表**只包含来自已提交的txns的更改、**影子页表**存储未提交事务修改的临时数据库（每个事务一个影子空间）。

更新只在影子页表中进行。当事务提交时，影子页表将自动切换为新的主页表。这就是一种NO-STEAL + FORCE的例子：

- **Undo / 回滚**：删除影子页表，并确保磁盘中的Root Ptr指向主页表。
- **Redo / 重放**：没必要。

![shadow-paging](/static/image/2022-02-20/shadow-paging.png)

NO-STEAL + FORCE是最简单的处理策略：

- **优点**：回退事务不需要撤销，因为更改没有写盘。已提交事务也不需要重做，因为所有更改都保证在提交前被写入磁盘。
- **缺点**：每个事务都需要一个私有空间拷贝，这会消耗大量内存来存储运行中的事务；或者一次只能运行一个事务。

注意:

- shadow page 不仅在内存，还在Disk中存在副本。它是写时复制的。
- DB Root Ptr是需要存在磁盘上的。
- 当commit且DB root更改后，原先的master page都是可删除的，或者说可以当作新的影子页表。

几乎没有系统使用它，SQLite初期在用类似的方法称为Journal File，但2010年之后更换了其他方法。



## Q：Journal File如何进行故障恢复？

写事务T1来到后：
1. 将Buffer Pool中原先的干净页，写到Journal File中。
2. 在Buffer Pool中做数据修改。
3. 当commit时，先强制写回数据到磁盘，然后删除所有T1相关的Journal File，最后通知客户端事务已提交。

此时如果出现宕机，那么sqlite读取所有Journal File，写入disk，相当于回复了原始数据。

![sqlite-journal-file](/static/image/2022-02-20/sqlite-journal-file.png)



## Q：WAL如何进行故障恢复？

在将数据库对象刷新到磁盘之前，DBMS必须将与数据库对象所做的更改相对应的日志文件记录写入磁盘。WAL是STEAL + NO-FORCE系统的一个例子。预写日志允许DBMS将随机写转换为顺序写，以优化性能。几乎每个DBMS都使用预写日志(WAL)，因为它有最快的运行时性能。但是恢复故障需要重放log，这会比较耗时。

具体而言，每次写操作都要先写WAL缓冲区，每次commit都要先保证WAL写入磁盘。每个日志条目包含对单个对象的更改信息：事务ID、对象ID、Before Value(用于UNDO)、After Value(用于REDO)。

- 系统可以使用“组提交”优化来批量处理多个日志刷新，以分摊开销。但需要等待组WAL写回后才能响应commit。

- DBMS可以随时将脏页写到磁盘上，只要它是在刷新了相应的日志记录之后。

![wal-example](/static/image/2022-02-20/wal-example.png)



**Logging Schemes / 日志格式**

- **Physical Logging**：记录对数据库中特定位置所做的字节级更改。
- **Logical Logging**：记录高层操作，如SQL本身。
  - 使用逻辑日志需要更长的时间恢复，因为必须重新执行每个语句。
- **Physiological Logging**：根据页面中的槽号标识元组，而不必精确指定更改在页面中的位置。
  - 是最常用的方法。

![logging-schemes](/static/image/2022-02-20/logging-schemes.png)



## Q：ARIES 故障恢复算法包含什么概念？

**Algorithms for Recovery and Isolation Exploiting Semantics (ARIES) **是IBM在20世纪90年代早期为DB2系统开发的一种恢复算法。在ARIES恢复协议中有三个关键的概念：

- **WAL / 预写日志**：这里的WAL与之前不同的是，这里每个Log都有序号，称为LSN（log sequence number）。
- **恢复崩溃前状态**：在重启时，回溯动作和恢复数据库到崩溃前的确切状态。
- **记录回滚操作**：以确保在重复失败的情况下，操作不会重复。

所有日志记录都有一个LSN。每次事务修改页面中的记录时，都会更新`pageLSN`。每次DBMS将WAL缓冲区写入磁盘时，内存中的`flushedLSN`就会更新。所以可以保证在磁盘中，`pageLSN <= flushLSN`。

![log-sequence-numbers](/static/image/2022-02-21/log-sequence-numbers.png)

**运行时流程**

- **事务提交**
  - **`<COMMIT>`**：首先将commit记录写入内存中的日志缓冲区。然后，DBMS将所有的日志记录刷新到磁盘，包括事务的`COMMIT`记录。一旦`COMMIT`记录安全地存储在磁盘上，DBMS就向应用程序返回事务已提交的确认信息。
  - **`<TXN-END>`**：在以后的某个时刻，DBMS将写入一个特殊的`TXN-END`记录。这表明事务在系统中已经完全完成，不再有日志记录。

![aries-tx-commit](/static/image/2022-02-21/aries-tx-commit.png)

- **事务回滚**
  - **`prevLSN`**：是一个日志记录的属性，对应于当前事务的前一个日志的LSN。prevLSN为每个事务维护事务链表，这样可以更容易地遍历日志。
  - **`<CLR: obj, before, after, undoNext>`**：描述了回滚前一个更新记录的操作所采取的操作。更新日志记录的所有字段以及undoNext指针(即下一个要被撤消的LSN)。
  - **`<ABORT>`**：要中止一个事务，首先将一条`ABORT`记录追加到内存中的日志缓冲区中，然后反向撤销事务。对于每个要回滚的更新，DBMS在日志中创建`CLR`并恢复旧值。所有更新被回滚后，写入一个`TXN-END`日志记录。

![aries-tx-abort](/static/image/2022-02-21/aries-tx-abort.png)

![aries-tx-abort-clr](/static/image/2022-02-21/aries-tx-abort-clr.png)



**Checkpointing / 检查点**

DBMS定期在将Buffer Pool中的脏页写的地方设置checkpoint，用于在恢复时最小化重放日志量。当checkpoint结束时，需要在数据库的磁盘中记录**`MasterRecordLSN`**指示<u>最后一次checkpoint开始时</u>的日志位置。

- **Blocking Checkpoints**：需要暂停事务执行。会停止事务和任何查询的执行，以确保能将数据库的一致性快照写入磁盘。
  - 停止任何新交易的开始，并等待所有活动事务执行完毕。所以脏页里只有已提交事务的更新。
  - 将脏页刷新到磁盘。



- **Better Blocking Checkpoint**：需要暂停事务执行。停止开始任何新事务，但不需要等待所有活动事务执行完毕，虽然仍需要暂停活动事务。
  - 为保证这种方法能够在故障恢复时，能够<u>通过ARIES重建当前活动事务和内存中的脏页以便回滚</u>，下面两种表被提出：
  - **Active Transaction Table (ATT) / 活动事务表**：记录checkpoint瞬间数据库的活动事务状态。事务的记录会在该事务的提交或终止会后被删除。每个事务包含信息：**事务ID**、**状态**（运行中、正在提交、需要回滚）、**`lastLSN`**（该事务的最近一次日志LSN）
    - 注意：ATT中只排除了已经有`TXN-END`记录的事务，其他事务都需要记录，包括正在提交的事务。
  - **Dirty Page Table (DPT) / 脏页表**：记录checkpoint瞬间数据库的脏页状态。每个脏页记录都只包含**`recLSN`**(第一次导致脏页的日志LSN)
  - 注意，这种方法不直接将脏页写回磁盘？



- **Fuzzy Checkpoint**：不需要暂停事务执行，也不需要等待所有活动事务执行完毕。但需要停止开始任何新事务？
  - **`<CHECKPOINT-BEGIN>`**：表明检查点建立开始。此时获取当前ATT和DPT的快照，将在`<CHECKPOINT-END>`里存储。
  - **`<CHECKPOINT-END>`**：表明检查点建立完成，它包含检查点开始时捕获的ATT和DPT。

![fuzzy-checkpointing](/static/image/2022-02-21/fuzzy-checkpointing.png)



## Q：ARIES故障恢复是什么样的流程？

ARIES协议由三个阶段组成：

- **Analysis**：读取WAL以重建崩溃时内存中的脏页和活动事务。同时计算下面两个步骤读取日志的开始位置。
  - ATT表明了在崩溃时哪些事务是活动的，而DPT显示哪些脏页可能没有到达磁盘。
- **Redo**：从WAL的一个合适的位置，开始重做所有操作。
- **Undo**：回滚不是`commiting`状态的所有事务。

![aries-overview](/static/image/2022-02-21/aries-overview.png)



**Analysis Phase / 分析阶段**

这个阶段的目的是恢复ATT和DPT，二者将在后面两个阶段里恢复数据库状态并回滚。从数据库的`MasterRecordLSN`开始（即最后一个checkpoint begin）扫描日志记录：

- 重建ATT：
  - 如果当前日志类型是`TXN-END`，就从ATT中删除它。
  - 所有其他记录，覆写记录`<Tx, LastLSN, Statue>`到ATT。这里的状态有：`commiting`（当前事务日志类型为`commit`）、`undo`（默认回滚这个Tx记录）
- 重建DPT：
  - 对于非查询的日志记录，如果操作的页P不在DPT中，则将P添加到DPT`<PageNum, recLSN>`。并将P的`recLSN`设置为日志记录的LSN（注意这里的页是从磁盘读出，已经丢失了崩溃前的内存中的状态）。

![aries-analysis](/static/image/2022-02-21/aries-analysis.png)



**Redo Phase / 重放阶段**

这个阶段目的是从造成脏页的第一条操作开始，重做事务，以求回复到崩溃前的buffer pool状态。

DPT中最小的`recLSN`是指造成脏页的第一条操作，从这里开始向下扫描，并在BufferPool中执行对应的每一条操作。需要注意有些操作不用执行：

- 该记录没有影响DPT中的page
- 该记录对应的`pageLSN` > 当前LSN
- 该记录对应的page `recLSN` > 当前LSN

最后给所有commit的事务写上`TXN-END`记录（日志先不要写回磁盘），然后从ATT中删除这个事务。



**Undo Phase / 回滚阶段**

目的是回滚掉所有非已提交的事务。说起来此时ATT中所有的事务都是需要undo的。回滚步骤如下：

- 每次找出ATT中最大的`lastLSN`，开始回滚：
  - 如果该记录是`CLR`，并且`nextUndoLSN`是空，那么写入`<TXN-END>`。（在不是空的情况下，跳过这个记录，因为前面的redo已经执行过它了）
  - 对于其他的操作日志，undo它，并写入`CLR`，并按照`prevLSN`更新ATT中的`LastLSN`


注意：

- 重放阶段，日志缓冲区里只会添加` <TXN-END>`记录；回滚阶段，日志缓冲区里只会添加 `<TXN-END>`和`<CLR>`记录
- 在前两个执行阶段，所有脏页和日志都没有写入磁盘，直到回滚阶段才开始写日志和脏页。所以前两个阶段是无状态的，而回滚写日志和脏页可以保证是对下次恢复没有影响，那么可以说在恢复过程中崩溃是不影响下一次恢复的。

![aries-crash-when-recovery](/static/image/2022-02-21/aries-crash-when-recovery.png)

![aries-crash-when-recovery-2](/static/image/2022-02-21/aries-crash-when-recovery-2.png)



我在学习ARIES时有些疑问：

- 为什么`<CHECKPOINT-BEGIN>`时不携带ATT和DPT，而是在`<CHECKPOINT-END>`时存储呢？那么重建它们的步骤里，什么时候需要读`<CHECKPOINT-END>`的ATT和DPT？怎么合并？
  - 我能不能认为运行时在`<CHECKPOINT-BEGIN>`和`<CHECKPOINT-END>`之间的这段时间里，DBMS停止新事务并查询活动事务和脏页？
    - 因为停止新事务开始，所以在checkpointing过程中的活动事务只减不增，所以`<CHECKPOINT-END>`里存储的是开始时的ATT和DPT是有意义的。
    - 所以我能否认为`<CHECKPOINT-BEGIN>`的目的是，记录停止新事务开始并计算ATT和DPT的时刻？而APT和DPT存储在`<CHECKPOINT-END>`是因为二者才刚刚建立好，或者说`<CHECKPOINT-END>`的目的是存储计算好的ATT和DPT？
  - 如果是这样的话，我认为一开始就应该先扫描查找`<CHECKPOINT-END>`，先去恢复APT和DPT。然后再跳到`<CHECKPOINT-BEGIN>`去恢复checkpointing期间缺失的信息（即有事务提交）。
- ARIES执行结束后，是否不需要写回恢复好的状态？我认为写不写似乎都可以，不过日志缓冲区是一定要保留好的。

![aries-crash-issue-1](/static/image/2022-02-21/aries-crash-issue-1.png)

![aries-crash-issue-2](/static/image/2022-02-21/aries-crash-issue-2.png)

# Distributed System / 分布式系统

## Q：分布式系统架构有哪些？

![distributed-system-arch](/static/image/2022-02-20/distributed-system-arch.png)

**Shared Memory / 共享内存**

CPU可以通过网络访问公共内存地址空间。大多数dbms都不使用这个体系结构，因为它需要在内核级别提供一定支持。

微软的研究系统FaRM使用这个架构，利用RDMA和NVRAM来提高性能，使用一些乐观并发控制来支持事务。

**Shared Disk / 共享磁盘**

![shared-disk-arch](/static/image/2022-02-20/shared-disk-arch.png)

在共享磁盘架构中，所有CPU可以通过网络链接读写公共逻辑磁盘。这种方法在基于云的DBMS中更为常见。Spanner、Aurora在使用这种方式：

- Spanner使用了同步时间来有效提高只读事务并发度，在Paxos Group中结合了2PC。
- Aurora抽离了存储节点让更多应用变得无状态化、利用变种的Qourum提供了很棒的容错性、在与存储节点传输信息使用更小的Log提高了吞吐量。

这种方式总是有很高的扩展性。可以添加无状态应用提高吞吐，也可以添加更多存储节点提高容错。

![shared-disk-example](/static/image/2022-02-20/shared-disk-example.png)

**Shared Nothing / 无共享**

![shared-nothing-arch](/static/image/2022-02-20/shared-nothing-arch.png)

在无共享环境中，每个节点都有自己的CPU、内存和磁盘。这是一种强调分片的架构：转发请求、扩容时迁移数据（一系列一致性hash）。这将很难扩展容量，也很难保证数据库一致性。但它提供了很好的性能。

有大量的DBMS是这种架构：TiDB、Cassandra、mongoDB、Couchbase、etcd、clickHouse、redis、RethinkDB

（其实如果磁盘是吞吐量或时延瓶颈，那么增加磁盘个数即可（RAID）；仅当网络成为瓶颈后，分节点负责一些磁盘可能意义更大？）

![shared-nothing-example](/static/image/2022-02-20/shared-nothing-example.png)



## Q：Sharding vs. Partitioning / 分片和分区的区别？

> - https://stackoverflow.com/questions/20771435/database-sharding-vs-partitioning

分区是跨表或数据库划分数据的通用术语。分片是一种特定类型的分区，是水平分区的一部分。



## Q：Partitioning Schemes / 分区策略

**Horizontal Partitioning / 水平分区**：就是一种Sharding，将表的元组分割成不相交的子集。

- 选择根据大小、负载或使用情况对数据库进行平均划分的列，称为Partition Key。
- DBMS可以通过哈希分区或范围分区对数据库进行物理分区。

![horizontal-partitioning](/static/image/2022-02-20/horizontal-partitioning.png)

**Consistent Hashing / 一致性哈希**：通过Hash环来做到更好的扩展性。

将节点分配到环上的某个位置，让节点负责左边的Hash区域。当添加或删除一个节点时，键只在新/删除节点的相邻节点之间移动。

![consistent-hashing](/static/image/2022-02-20/consistent-hashing.png)



## Q：副本系统有哪些注意的点?

**Propagation Scheme / 传播模式**

- **同步**：当主节点确保从节点收到副本后，再响应客户端。
- **异步**：响应客户端不会等待从节点传播副本。

**Propagation Timing / 传播时机**

- **continuous / 持续**：每个操作立即发送。即使未提交或回退。
- **on commit / 提交时**：当事务提交时才发送。只会传递提交事务信息。



## Q：2PC的优化有了解么？

- **预投票**：如果节点指导当前请求是最后一个操作，那么该节点直接返回准备阶段的投票和查询结果。
- **准备提交**：当所有节点都可以commit时，TC可以提前向客户端发送commit成功响应。



## Q：知道Muti-Paxos么？

在Paxos的基础上，只允许一个节点作为`proposer`。系统选举出Leader并在一段时间内进行变更，那么它可以跳过提议阶段。系统定期更新领导使用另一轮Paxos的人。当出现故障时，DBMS可以退回到完全的Paxos。

Spanner使用的Paxos会在10s后重新选举新的leader，（或许这称为leader lease？）新的leader选举过程类似一轮paxos。

下面是普通Paxos的截图，放在这里单纯认为讲的不错：

![paxos-example](/static/image/2022-02-20/paxos-example.png)



## Q：什么是Federated Database / 联邦数据库 ？

将多个dbms连接到一个逻辑系统中：

- 由于不同的数据模型、查询语言和每个DBMS的限制，这很难做到。
- 没有简单的方法来优化查询。
- 需要进行大量的数据复制。

PostgreSQL是使用其**外部数据包装器（foreign data wrappers）**部署联邦数据库的最好选择。它允许用户在给定的Postgres会话中使用来自其他系统的数据。

![federated-db](/static/image/2022-02-20/federated-db.png)



## Q：OLAP - 什么是Star Schema / Snowflake Schema？

**Star Schema / 星型模式**

星型模式包含两种类型的表：事实表和维度表：

- **事实表**：包含应用程序中发生的多个“事件”，它将包含每个事件的最小惟一信息，然后其余的属性将是对外部维度表的外键引用。

- **维度表**：包含跨多个事件重用的冗余信息。

在星型模式中，事实表只能有一个维度级别。由于数据只能有一级维度表，因此它可以有冗余信息。对星型模式的查询(通常)将比雪花模式更快，因为连接更少。

![star-schema](/static/image/2022-02-20/star-schema.png)

**Snowflake Schema / 雪花模式**

雪花模式与星型模式类似，不同的是它们允许从事实表中输出多个维度。它们占用更少的存储空间，但需要更多的连接来获得查询所需的数据。

![snowflake-schema](/static/image/2022-02-20/snowflake-schema.png)



## Q：OLAP - Query Planning如何进行？

分布式查询优化较单机更加困难，因为它必须考虑数据在集群中的物理位置和数据移动成本。目前有两种做法：

- **分发操作符**：生成单个全局查询计划，然后将物理操作符分发到节点，将其分解为特定于分区的片段。大多数系统都实现了这种方法。

- **改写分区SQL**：另一种方法是采用SQL查询并将原始查询重写为特定于分区的查询。这允许在每个节点上进行局部优化。SingleStore和Vitess就是使用这种方法

![sql-fragments](/static/image/2022-02-20/sql-fragments.png)



## Q：无共享架构下分布式Join如何执行？

在Shared Nothing的场景下，数据散落在各个节点上。协调节点只能通过网络传输数据来做Join。下面对四种场景进行讨论：

![dist-join-scenario-1](/static/image/2022-02-20/dist-join-scenario-1.png)

- **场景1**：表A在每个节点上进行复制，表B跨节点分片。
  - **方案**：节点间并行连接其本地分片，最后将其结果发送给协调节点。（不需要数据拷贝）
  - 这些表通常是较小或者较冷

![dist-join-scenario-2](/static/image/2022-02-20/dist-join-scenario-2.png)

- **场景2**：两个表在Join属性上分片，所以表AB的ID范围在本地已经互相匹配。
  - **方案**：与上一个场景相同。在本地可以直接按主键join，每个节点分别处理后在协调节点上取并集，转发给client。（不需要数据拷贝）

![dist-join-scenario-3](/static/image/2022-02-20/dist-join-scenario-3.png)

- **场景3**：其中一个表B没有按JoinKey分片，所以对某个节点来说，表B的数据可能散落在所有节点上。
  - **方案**：**广播链接** - 将较小的表广播给所有节点。然后做#2。
  - 其中一个表没有按joining key分片，所以需要广播AB中较小的表，让所有节点得到完整的joining id range，然后做#2的处理。（需要对所有节点拷贝对应范围的较小的表，**可以filter范围**）

![dist-join-scenario-4](/static/image/2022-02-20/dist-join-scenario-4.png)

- **场景4**：表AB都没有在JoinKey上做分片。
  - **方案**：DBMS通过跨节点重新洗牌来复制这些表。即让每个节点重新建立表AB在JoinKey上的分片，然后执行#2。
  - 如果没有足够的磁盘空间，失败是不可避免的。这称为shuffle join。



## Q：什么是云数据库系统？

较新的系统开始模糊无共享和磁盘共享架构之间的界限。如，Amazon S3允许在将数据复制到计算节点之前进行简单的过滤。目前有两种类型的云系统：

**Cloud-Native DBMS / 云原生DBMS**

原生云系统被明确设计为在云环境中运行，通常基于共享磁盘架构。

这种方法在Snowflake、BigQuery、Amazon Redshift和Microsoft SQL Azure中都使用过。

**Serverless Databases**

Serverless DBMS不总是进行计算资源计费，而是在服务空闲时将其逐出，并将系统中的当前状态写入到磁盘。这更多是一种计费方式。

![serverless-dbms](/static/image/2022-02-20/serverless-dbms.png)



## Q：分布式系统组件有哪些？

许多现有的项目实现了分布式数据库的单个组件，可以重用组件构建分布式数据库：

- **System Catalogs**：HCatalog，Google Data Catalog，Amazon Glue Data Catalog
- **Node Management**： Kubernetes，Apache YARN，Cloud Vendor Tools
- **Query Optimizers**：Greenplum Orca，Apache Calcite



## Q：一些通用的存储文件格式有哪些？

大多数DBMS使用专用的二进制文件格式，但在DB之间共享数据的唯一方法是将数据转换为基于文本的通用格式（CSV、JSON之类），这需要一定开销。

现在，一些云供应商和分布式数据库系统支持新的开源二进制文件格式，这使得跨系统访问数据更加容易：

- **Apache ORC**：Apache Hive的压缩列存储。
- **HDF5**：用于科学工作负荷的多维数组。
- **Apache Arrow**：Pandas / Dremio的内存压缩列式存储。
