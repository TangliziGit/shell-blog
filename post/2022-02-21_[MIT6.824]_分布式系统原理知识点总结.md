# [MIT6.824]分布式系统原理知识点总结

> - http://nil.lcs.mit.edu/6.824/2020/schedule.html
> - https://timilearning.com/
> - https://www.zhihu.com/column/c_1273718607160393728

# Introduction

## Q：分布式系统想要解决什么问题？有什么挑战？

当需要设计一个系统时，如果你发现可以在一台计算机上就能解决问题，那你就应该用一台计算机解决它。有很多的工作都可以在一台计算机上完成，并且通常比分布式系统简单很多。那么什么时候选择分布式？它用来解决什么问题？

- **提升性能**：将计算任务分发给大量计算机，使用大量的CPU、内存和磁盘，通过并发来提升性能。如果大规模的集群比一台超级计算机成本更低。
- **错误容忍**：单机执行任务不可避免要考虑故障问题，通过冗余计算或冗余存储，宏观上降低故障的概率。
- **系统安全**：拜占庭问题。系统中可能有恶意代码执行，但又需要开放系统。

为什么说分布式让系统变得复杂？

- **复杂的交互**：系统中多个节点需要并行合作来解决一系列公共的问题，这使得并发和交互变得十分复杂。
- **需要容错**：不可避免要局部故障，系统如果不考虑容错，那么故障概率按节点个数指数级上升。
- **扩展性问题**：并非机器加更多机器就有更好性能。



## Q：一致性模型是什么？有哪些？

> - https://int64.me/2020/%E4%B8%80%E8%87%B4%E6%80%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0.html
> - https://zhuanlan.zhihu.com/p/48782892

- **一致性模型**：系统总是能够保证操作执行，满足某种时序关系的规则。我们把这样的规则称为一致性模型。
  - **系统的操作**：用户与系统交互过程可以分为三个时刻：用户请求发送、系统执行操作、用户接受响应。
  - 因为用户与系统交互过程中，会因为传输消息带来的延迟和不同消息传播的快慢，而导致预期外的事件顺序发生。所以，为了让系统能确保一种操作执行顺序，我们必须为系统定义一致性模型。
  - 一致性保证的代价是需要系统间协调来达成对顺序的强制约束。不严格地说，执行越多的记录，系统中的参与者就必须越谨慎且通信频繁。
- **Linearizability / 线性一致性**：对与<u>所有客户端</u>而言，系统总能保证<u>读写都在一个单调递增的时间上串行执行</u>，<u>所有的读总能返回最近的写</u>。
  - **注意**：所有变更都对其他参与者可见。且当某个操作响应到来后，任何之后的操作一定会被此操作影响。 
  - 由于这些强约束条件的存在，可线性化的系统变得更容易推理，这也是很多并发编程模型构建的时候选择它作为基础的原因。
- **Sequential consistency / 顺序一致性**：对<u>单个客户端</u>而言，系统保证<u>操作一定按照用户的逻辑顺序发生</u>
  - **注意**：不要求操作按照真实的时间序发生。保证所有进程看到的操作顺序一致，但不保证同一时刻进程间看到的数据是一样的。
- **Casual consistency / 因果一致性**：保证因果相关的操作必须按顺序发生。也就是说按因果关系隔离，无因果关系的操作可以并发执行
  - 如果我们将这些因果关系编码成类似“我依赖于操作X”的形式，作为每个操作明确的一部分，数据库就可以将这些操作延迟直到它们的依赖都就绪后才可见。
- **Eventual consistency / 最终一致性**：指对于已改变写的数据的读取，最终都能获取已更新的数据，但不完全保证能立即获取已更新的数据。



## Q：CAP定理和BASE定理是什么？

CAP理论声明给定一致性（**C**onsistency），可用性（**A**vailability）和分区容错性（**P**artition tolerance），任何系统**至多**能保证以上三项中的**两项**而不可能满足全部三项。这是Eric Brewer的CAP猜想的非正式说法，以下是准确的定义：

- **一致性（Consistency）**意味着线性化，具体说，可以是一个可线性化的寄存器。寄存器可以等效为集合，列表，映射，关系型数据库等等，因此该理论可以被拓展到各种可线性化的系统。
- **可用性（Availability）**意味着向非故障节点发出的每个请求都将成功完成。因为网络分区可以持续**任意长的时间**，因此节点不能简单地把响应推迟到分区结束。
- **分区容错性（Partition tolerance）**意味着分区很可能发生。当网络**可靠**的时候，提供一致性和可用性将变得十分**简单**，但当网络**不可靠**时，同时提供一致性和可用性将变得**几乎不可能**。然而网络总不是完美可靠的，所以我们不能选择CA。所有实际可用的商用分布式系统至多能提供AP或CP的保证。

BASE是大多数NoSQL数据库的设计思路，是追求高可用的权衡：

- **Basically Available / 基本可用**：基本可用是指分布式系统在出现故障的时候，允许损失部分可用性，即保证核心可用
- **Soft State / 软状态**：允许一些不一致状态暴露出来, 即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时
- **Eventually consistent / 最终一致性**：指系统中的所有数据副本经过一定时间后，最终能够达到一致的状态，但不保证能立即获取已更新的数据。



## Q： 分布式数据库系统中会存在什么故障？

这里不考虑拜占庭问题。

**Network Failures / 网络故障**

与单机数据库不同，分布式数据库中的网络通信问题不能被忽视：**网络延迟**、**网络分区**、**丢包**、**包重复**、**包乱序**。

**Transaction Failures / 事务失败**

事务失败发生在事务出现错误且必须中止事务时：**逻辑错误**（比如一致性、约束条件等）、**内部状态错误**（比如死锁）。

**System Failures / 系统失败**

系统故障是硬件或软件的偶然故障，在崩溃恢复协议中也必须考虑到这些故障：**软件故障**（如除零异常）、**硬件故障**（如电源断开）

**Storage Media Failure / 存储介质失败**

指物理存储损坏时发生的不可修复的故障。当存储介质发生故障时，DBMS必须从备份恢复。



## Q：当谈到Replica和Shard时到底在讨论什么？

- **Replicating**
  - **同步模型**：主备同步模型、同步/异步复制
  - **读写分离**：主从一致性如何
  - **故障恢复**：故障发现（单节点故障处理 / 多节点共识机制） + 主备切换/选举主节点 + 脑裂解决方式
- **Sharding**：
  - **分片策略**：水平扩缩容的方案
  - **分布式事务**：原子提交协议
  - **重定向策略**：客户端缓存 / 服务端转发

# Replicating / 副本复制

## Q：副本同步方法可以分为哪些类？

在VMwareFT的演讲里介绍了两种复制方法：

- **State Transfer**：Primary将自身内存中的状态信息定期发送给Backup存储作为存储冗余。
  - 需要注意的是，每次状态迁移需要进行一次大拷贝，但也可以做diff来减少传输量。
  - **优势**：只需要暴力存储状态，所以较为直白。
  - **缺点**：数据传输量很大，需要更多的成本来做拷贝。

- **Replicated State Machine**
  - 将系统受到的一系列**外部输入**发送给backup进行存储。
  - 这其实对系统提出了一个假设：两个相同状态的系统在受到完全一致的外部输入后将保持一致、互为副本。
  - **优势**：数据传输量很小
  - **缺点**：要对系统提出很多假设，针对不满足假设的情况需要一一处理。



## Q：采用RSM/副本状态机模型需要考虑什么？

一些主备同步中的重要问题：

- **同步层级**
  - **内核级别**：在VMware中，主备的状态是指primary内存中的每个bit，即主备在底层也是完全一致的。很少有系统如此设计备份方案，因为它过于困难（甚至需要考虑中断在主备中同一个位置进行）。他的优点是，在VMware FT支持的微处理器上，任何一个可运行的软件都可以具备容错性。你不需要考虑软件的任何逻辑和源码。
  - **应用程序级别**：然而大部分的系统类似GFS，备份数据是指应用程序级别数据chunk，每个针对chunk都保存有默认3个副本在不同的机架和服务器上。GFS只需要保证chunk副本的一致即可。
- **主备切换**：当系统primary故障后进行主备切换，在理想情形是应当没有任何客户端会注意到这里的切换。但在切换过程中，必然会有异常。
- **备份故障**：当两个备份其中一个故障后，应当尽快上线新的备份避免所有备份宕机。
  - 创建新的副本需要较大代价。因为此时新的副本没有一致的内部状态，所以创建时只能进行状态迁移，不能采用复制状态机。



**Non-Deterministic Events / 非确定事件**

通常情况下，代码执行都是直接明了的，但并不是说计算机中每一个指令都是由计算机内存的内容而确定的行为。这一节，我们来看一下不由当前内存直接决定的指令。这些指令在Primary和Backup的运行结果可能会不一样。这些指令就是所谓的非确定性事件。非确定性事件可以分成几类：

- **客户端输入**

  - 当我们说输入的时候，我们实际上是指接收到了一个网络数据包。而一个网络数据包对于我们来说有两部分，一个是数据包中的**数据**，另一个是提示数据包送达了的**中断**。

  - 当网络数据包送达时，通常网卡的DMA（Direct Memory  Access）会将网络数据包的内容拷贝到内存，之后触发一个中断。操作系统会在处理指令的过程中消费这个中断。对于Primary和Backup来说，这里的步骤必须看起来是一样的，否则它们在执行指令的时候就会出现不一致。所以，这里的问题是，**中断在什么时候，具体在指令流中的哪个位置触发**

- **怪异指令**：有一些指令在不同的计算机上的行为是不一样的

  - 随机数生成器

  - 获取当前时间的指令，在不同时间调用会得到不同的结果

  - 获取计算机的唯一ID

- **多核并发**
  - 当服务运行在多CPU上时，指令在不同的CPU上会交织在一起运行，进而产生的指令顺序是不可预期的。



## Q：共识机制是什么？有哪些？

分布式系统强调，在存在大量故障节点的情况下，实现系统的整体可靠性。这需要在节点间对一些必要的数据达成一致。<u>共识算法本质上就是在维护一些数据副本，而副本本质上就是提供数据冗余的，它们的作用就是为了保证数据可靠，即**数据不丢失、请求不乱序**</u>。达成共识的例子包括数据库的事务顺序、状态机复制和原子广播等。这里我们只讨论非拜占庭共识机制，即**崩溃容错协议**。

常见的一些崩溃容错协议包括Paxos、Raft、Viewstamped replication、ZAB、Gossip等。



## 共识机制 Raft

### Q： Raft与其他共识算法的区别？

Raft与其他共识算法相似（特别是VSR, viewstamped replication），区别在于：

- **Strong Leader**：Raft使用了一种比其他共识算法更强的领导形式。
  - 提升系统性能：在一些无Leader的多副本系统（如Paxos）中，通常需要在一轮消息中确定一个临时Leader，然后在下一轮消息中再确认请求。
- **Leader Election**：Raft使用随机计时器来选举领袖。这只在心跳包上增加了少量的机制，就能简单和迅速解决冲突。
- **Membership Changes**：这允许集群在配置更改期间继续正常运行。

对比更复杂的Paxos，Raft通过下面的方法进行了简化：

- 将算法分解成**Leader选举、日志复制、安全性、成员变动**四个模块。
- 简化状态空间。



### Q：Raft复制状态机能提供什么能力？

与GFS和HDFS类似，Raft将上层应用程序作为复制状态机管理。具体而言，Raft能够提供如下的属性：

- **安全性**：在所有非拜占庭条件下（包括网络延迟、网络分区、丢包、包重复、包乱序），从不返回错误的结果。
- **可用性**：大多数节点正常运行并可相互通信情况下，系统保持正常运行。当节点失败重启后，能够恢复状态并重新加入集群。
- **无时间**：错误的时钟和消息延迟在极端的场景下，会导致可用性问题。
- **性能**：只需过半的节点同步Log，那么就可以响应客户端。
- **一致性**：强一致性。



### Q：Raft如何工作？

![raft-fig-2](/static/image/2021-12-20/raft-fig-2.png)

每个节点都有一种临时角色：Leader、Follower和Candidate。他们的职能和状态转移如下：

- **Leader**：主要用于处理客户端请求、发送AppendEntries（心跳包&日志同步）
  - 变为Follower：受到高Term的VoteRequest & 高Term的AppendEntries & 高Term的AppendEntries响应
- **Follower**：（初始值）被动接受Leader的AppendEntries进行日志同步，接受Candidate的VoteRequest选主
  - 变为Candidate：选举定时器到期
- **Candidate**：选主时主动发送VoteRequest
  - 变为Leader：选举通过过半仲裁
  - 变为Follower：受到高Term的VoteRequest & 高Term的AppendEntries & 高Term的VoteRequestResponse

注意：

1. 其实<u>任何角色受到任何高任期的请求和响应，都会转变为对应节点的Follower</u>。
2. 每个RPC都会在响应超时情况下进行重试尝试，并且每个RPC都是并行执行。
3. 每个Term会将时间分割开来，它在整体上表现成逻辑时钟。



**选主流程**

![request-vote-rpc](/static/image/2021-12-20/request-vote-rpc.png)

- **触发时机**：某个Follower / Candidate在选举定时内，没有收到Leader的AppendEntries
- **选主流程[发送者]**：
  1. 节点状态改变：`{Role=Candidate, Term+=1, VotedFor=self}`
  2. 并发发送`VoteRequest{Term, CandidateID, LastLogTerm, LastLogIndex}`给所有server
  3. 统计结果`VoteRequestResponse{Term, VoteGranted}`：

     - 确认选举没有结束（选举可被选举定时器和新的选举打断）
     - `Term > currentTerm` => 成为Follower：`{Role=Follower, Term, VotedFor=nil}`；<s>重置选举计数器</s>
     - `Votes == serverCount / 2 + 1`(为了不重复检查成为Leader，这里用相等判断) => 重置选举计数器；成为Leader：`{Role=Leader， []nextIndex, []matchIndex}`；初始化`{[]nextIndex=len(log), []matchIndex=-1}`
     - 选举超时 => 重新选主
- **选主流程[接收者]**：`VoteRequest{Term, CandidateID, LastLogTerm, LastLogIndex}`
  - `Term < currentTerm` => 反对票
  - `isTermEnough && isLogUpToDateEnough` => 支持票
    - 成为Follower：`{Role=Follower, Term, VotedFor=candidateID}`；重置选举计数器
  - `otherwise` => 反对票
    - 检查Term大小更新：`{Role=Follower, Term}`

**注意**：

- 这个RPC是幂等的。
- **任期**：一切带Term的节点交互过程里（req / resp）都需要检查并更新本节点`{Term, Role=Follower}`。
  - `isTermEnough`判断是否有最高的Term，当Term相等时检查是否已投票；`isLogUpToDateEnough`是判断是否有最高的Term，如果Term相同则判断最高的Index。
  - 当Term相同时，对votedFor的检验才有意义。因为Follower当前Term的votedFor是上一次选举时的产物，而非本次选举的。
- **偏序集**：在长时间内的选主流程可以认为是：在集群中识别`{LastLogTerm, LastLogIndex}`偏序集的过程。在短时间内，你需要先判断`{Term, sendingTime}`。细节可见下一条。
  - 其实，一个节点最终能不能成为Leader主要关注LogTerm&LogIndex。不关注Term的原因是，该节点在与高Term节点通信时会改变调整Term。同时因为节点拥有更新的Log，除非大多数节点没有更新的Log，那么该节点后续还是很可能成为Leader。（若没有成为Leader，那么这条Log后续很可能会被覆盖，客户端将收不到这条提交的Log）
- **脑裂**：会存在脑裂，但是不过半的小集群Leader不会提交Log。于是小集群Leader不会响应Client的请求，也不会执行Client的命令。
- **定时器**：随机化的定时器可以解决Candidate同时请求投票，导致的系统一直投票死锁的情况。但是选举定时器的随机范围需要规范确定：
  - 下限：应当是心跳包时间间隔的n倍，这样能确保follower能够避免因为偶然的网络异常而触发选举
  - 上限：它影响了Leader失败后系统恢复速度。当故障频繁时，需要重点权衡。
    - 我们需要考虑在两个节点超时时间差之内，应当可容纳单趟VoteRequest RPC的时长。



**日志复制**

![append-entries-rpc](/static/image/2021-12-20/append-entries-rpc.png)

每个Client请求包含由复制状态机执行的命令。Leader将命令附加到其日志作为新条目，然后与每个其他服务器并行的复制Log。当Log已提交时，Leader将Log应用于其状态机，并将该执行的结果返回给客户端。如果Followers崩溃或运行缓慢，或者如果网络数据包丢失，那么Leader将无限期重试。

- **触发时机**：定期AppendEntries & 客户端提交log接口
- **复制流程[发送者]**：
  1. 发送`AppendEntries`：注意需要根据nextIndex得到`prevLogTerm & prevLogIndex`
  2. 统计响应`AppendEntriesResponse{Term, Success}`：
     1. 确认自己不是Leader
     2. `Term > currentTerm` => 成为Follower：`{Role=Follower, Term}`
     3. 检查`success`：
        - `success` => 更新nextIdx成对方最后一个LogIndex+1；更新matchIndex为nextIndex-1；根据所有matchIdx更新commitIdx并提及Log给上层应用（后文讨论细节）
        - `!success` -> 对应server回退nextIndex（后文细节）
- **复制流程[接受者]**：
  - `Term < currentTerm` => 响应失败
  - 没有匹配PrevLogIndex的Log => 响应失败
    - 更新`{Term, Role=Follower}`重启计数器
    - 检查冲突Log，回复`{XTerm, XIndex, XLen}`
  - `otherwise` => 响应成功
    - 更新`{Term, Role=Follower}`重启计数器
    - Append对应Log
    - 更新CommitIndex，并提交Log给上层应用：`commitIndex = max(commitIndex, min(leaderCommit, lastLogIndex))`

**注意**：

- 这个RPC是幂等的。
- 当prevLogTerm & Index不同时，不更新。一旦更新，那么从冲突点开始的所有log，复制append req的logs。
- 对于上层应用发来的**读命令请求**，仍需要Leader做一次半数以上的AppendEntries，验证当前Leader是系统唯一Leader，才能保证读到的commit操作结果不是脏数据（旧数据，线性一致性）。
- **当前任期提交**：Leader只能commit与当前Term相等Term的Log
  - Leader只能提交当前Term的Log，即从接受该Log至今没有Leader更替。如果Leader（TermC）提交了之前TermA的Log，那么有可能导致TermB的Leader重新选举出来，用自己的TermB的Log，去覆盖TermC提交的TermA的Log。那么就会出现已提交的Log消失的问题。
  - 只提交当前Term的原因是，当前Term的Log一定不会被覆盖。被覆盖需要满足：新Leader的日志Term一定要高于他的Term，这个Leader才能被选出来；这个新Leader是不能被选举出来的。具体看选举安全。
- **批量回退**：为了能够更快的恢复日志，Follower需要返回足够的信息给Leader，让Leader可以以一段Log为单位来回退，而不用每次只回退一条Log条目。优化方案有很多：
  - **按Term回退**：Follower返回xTerm、xIndex和xLen用来表示prevLogIndex的匹配情况（即冲突）
    - xTerm：指冲突Log的Term
    - xIndex：指xTerm最远冲突点，即xTerm的第一个LogIndex
    - xLen：指如果没有prevLogIndex对应的Log，那么缺多少补多少；此时xTerm=-1
    - **原理**：在Index一致情况下，如果Term不相等，那么Follower这一Term的Log都是错误Log，所以需要一次性退掉xTerm。如果对应Index没有Log，那么nextIndex回退xLen再检查
  - **二分查找冲突点**：Leader首先携带空Entries选取中点进行查询，当Follower Reject后向前取二分，当Follower Accept时向后取二分。当冲突点找到后则可以携带数据。



### Q：Raft的安全保证是什么？如何证明？

这里总结一下Raft的安全特性。这些特性在网络分区或故障中都能保证（排除了**Leader Append-Only**）：

- **Election Safety / 选举安全**：在任期内最多只能选出一位领导人。
  - 论证：因为Candidate是通过半数仲裁赢得选举，同时一个节点在同一个任期里只能投票给一个节点。在同一Term中选出两个Leader一定意味着至少存在一个节点给两个节点同时投同意票，这是不可能的。
- **Log Matching / 日志匹配**：如果两个日志包含一个具有相同Index和Term的条目，那么从零到给定Index的所有条目中的日志都是相同的。
  - 论证：
    1. 首先，当term和index一致，那么log内容一致。因为在相同Term中只能选出一个Leader，同时单节点Leader一定能够保证相同Index和Term下的Log内容一致。
    2. 其次，在Leader做AppendEntries时每个Log都有唯一的prevLogIndex和prevLogTerm，仅当被同步节点具有响应前置Log，它才能接受此Log并附加在前置Log之后（类似于区块链）。
    3. 所以，当两个节点（Leader节点同理）接受相同Index和Term的Log后，可以说明之前的所有Log都是相同的。
- **Leader Completeness / Leader完整性**：如果一个Log是已提交的，那么该Log将会出现在任何更高Term的Leader日志中。
  - 论证：**TODO**：第三点是错误的。
    1. 首先，已提交的Log是指大多数节点都存在此Log，称这个Log为LogX。
    2. 其次，成为Leader需要通过过半仲裁。那么持有LogX的节点和投赞成票的节点会**有交集**，至少存在一个拥有LogX的节点投赞成票。
    3. 所以可以认为当前Leader拥有LogX。
- **State Machine Safety / 状态机安全**：如果一个服务器在给定Index上应用了一个Log到它的状态机，那么任何其他服务器会应用这个Index里相同的Log。（我的理解：在同一个已提交的Log的Index上，不会存在另一个不同内容的Log在相同Index上提交）
  - 论证：反证法
    1. 首先，应用相同Index而不同Log X的方式是获得半数以上Follower的AppendEntries同意。要获得同意，对Leader而言必须有：`{Term>=followerTerm, prevLogMatched}`。
    2. 所以，Follower的Term一定低于Leader的Term，同时此Log的Term一定小于等于Follower的Term。那么有`LeaderTerm > LogXTerm`。
    3. 但是，Leader完整性要求已提交Log会出现在任何高Term的Leader中。注意到`LeaderTerm > LogXTerm`同时LogX已提交。那么说明LogX的内容是没有改变的。推出矛盾，证明状态机安全成立。



### Q：脑裂如何处理？Raft在Leader上如何解决脑裂？

**单节点**

现代系统处理脑裂的方法非常一致，就是通过单节点来决定哪个副本是主。正因为单节点没有决策矛盾，所以它被用来处理脑裂的场景：当局部故障出现后（主备heartbeat断开），应决定谁是主备份：

- MapReduce：它复制了计算，但是复制过程是被一个单主节点控制。
- GFS：以主备的方式复制数据，客户端和存储节点都依赖一个单主节点，来确定每一份数据的主拷贝的位置。
- VMwareFT：通过复制状态机的原理，Primary虚机和Backup虚机之间会传递客户端的指令。当主虚拟机宕机，一个TestAndSet服务出现，来决定是否切换。

**Quorum**

而Raft处理脑裂是通过定时器+Qourum来完成的，它本质上也是一种共识机制。



### Q：Raft的日志作用是什么？

- **保证操作顺序**：Log包含的term和index，对系统而言用于保证log的顺序性。每个log拥有唯一的term和index。
- **临时存储**：Follower需要在确定操作被committed，才能将其应用到副本状态机中。有些未提较的log可能在未来切换leader后，被替换而不复存在。
- **备份重传**：当新机器加入集群中时，状态中的log entry是空的。所以leader需要给它传输先前已有的数据。
- **状态恢复**：Log存储于非易失介质中，当节点重启后可以通过从头执行log，来恢复副本状态机的内存信息。



### Q：Raft如何实现成员变更？

> - https://zhuanlan.zhihu.com/p/359206808

**Joint Consensus / 联合共识**

- **同步Cold,new**：Leader收到成员变更请求后，先向Cold和Cnew同步一条Cold,new日志，此后所有日志都需要Cold和Cnew**两个多数派的确认**。
- **同步Cnew**：Cold,new日志在Cold和Cnew都达成多数派之后才能提交，此后Leader再向Cold和Cnew同步一条只包含Cnew的日志，此后日志**只需要Cnew的多数派确认**。
- Cnew日志只需要在Cnew达成多数派即可提交，此时成员变更完成，不在Cnew中的成员自动下线。

成员变更过程中如果发生Failover，老Leader宕机，Cold,new中任意一个节点都可能成为新Leader，如果新Leader上没有Cold,new日志，则继续使用Cold，Follower上如果有Cold,new日志会被新Leader截断，回退到Cold，成员变更失败；如果新Leader上有Cold,new日志，则继续将未完成的成员变更流程走完。

**注意**：

- 新成员先加入再同步数据还是先同步数据再加入：这是在可用性和简单快速之间的取舍
- 成员变更日志使用什么配置：任何处于ColdCnew的节点都使用两个Qourum。
- 成员变更日志什么时候生效：Leader在同步配置前生效，Follower在持久化之后生效
- 只有少数成员存活时怎么恢复服务：无法读写，所以只能提供强制更改成员配置的接口。

**单步成员变更**

实现单步的成员变更，关键在于限制Cold与Cnew，使之任意的多数派交集不为空。方法就是每次成员变更只允许增加或删除一个成员。

增加或删除一个成员时的情形，如图3所示，可以从数学上严格证明，只要每次只允许增加或删除一个成员，Cold与Cnew不可能形成两个不相交的多数派。因此只要每次只增加或删除一个成员，从Cold可直接切换到Cnew，无需过渡成员配置，实现单步成员变更。

单步成员变更一次只能变更一个成员，如果需要变更多个成员，可以通过执行多次单步成员变更来实现。



### Q：Raft如何实现日志压缩？

> - https://tangwz.com/post/raft-extension/
> - https://web.stanford.edu/~ouster/cgi-bin/papers/OngaroPhD.pdf

随着时间推移，存储的日志会越来越多，不但占据很多磁盘空间，服务器重启做日志重放也需要更多的时间。所以日志压缩是必要的。

日志压缩的原则是：日志中的许多信息随着时间推移会变成过时可以丢弃的。一旦日志记录被提交并应用于状态机，那么用于到达当前状态的中间状态和操作就不再需要了。

- 系统一般不将压缩集中在 Leader 上，而是每个服务器独立地压缩其已提交的日志。
- **状态机的责任**
  - 只有状态机才知道那些日志可以压缩、怎么压缩。
  - 状态机需要主动查询Raft日志的大小来决定什么时候压缩日志，并传递快照给共识层。
- **共识层的责任**
  - 服务器重启后，需要用快照来做状态机恢复
  - 向慢Follower发送快照

![raft-log-snapshot](/static/image/2022-03-06/raft-log-snapshot.png)

**基于内存的快照**

对内存的数据结构(树形或哈希等)进行序列化并存储，同时存储 Raft 重启需要的状态：`lastLogTerm`和`lastLogIndex` 以及状态机快照。之后此index之前的日志和快照都可以丢弃了。

Leader可能需要把快照发送给慢 Followers 或新加入集群的服务器。快照信息通过 InstallSnapshot RPC 来传输。快照安装RPC主要包括：Term、LastIndex、LastTerm、offset和data。响应只有term。

![raft-install-snapshot](/static/image/2022-03-06/raft-install-snapshot.png)

- **并发生成快照**：序列化和写快照都要与常规操作并发进行，避免服务不可用
  - **写时复制**：可以利用OS的fork来做快照生成（redis和LogCabin在用）；其他的实现机制，需要与状态机强相关。（如不可变数据结构、MVCC等）
  - **Leader接替**：当Leader想做快照的时候，可以让其他服务器选出另一个Leader接替工作。这样就可能消除快照的并发，但服务器在快照期间可用性会降低。
- **快照触发时机**：太过频繁地做快照，将会浪费磁盘带宽和其他资源；太不频繁地做快照，则有存储空间耗尽的风险，并且重启服务需要更长的重放日志时间。
  - **阈值**：当日志大小超过阈值则做快照。
  - **日志快照比值**：当日志大小超过之前的快照的大小乘以扩展因子，触发快照。

**基于磁盘的快照**

对于上百GB级别的状态机，使用基于内存的方法显然是不合适的。对于每一个Log entry，当其被提交并应用到状态机后，该entry就可以被丢弃。因为磁盘可以永久存储状态，即可以理解为每有一个Log entry被写入，磁盘就会有一个新的Snapshot。

Snapshot由于需要继续提供服务，仍然需要Copy-on-write的帮助。由于磁盘是天然分块的，可以分块的进行操作。Linux的LVM(logical volume management，逻辑卷管理)也可以提供Snapshot镜像的操作。

- **增量式快照方法 / 递增清理方法**：增量的方法做压缩如 log cleaning 或 LSM tree是可能的。

  - 他们快照的实现会更复杂，但有如下**优点**：

    - 压缩的负载随着时间来看是均匀的。

    - 相比创建新的快照，删除不需要的日志会写入更少的数据。

    - 不会原地修改磁盘的区域。

  - **Log-Structured / 日志结构压缩**：见DDIA日志式索引

  - **LSM-Tree / 日志结构合并树压缩**：见DDIA日志式索引



### Q：介绍一些Raft的优化

> - https://zhuanlan.zhihu.com/p/464258409
> - https://zhuanlan.zhihu.com/p/25735592
> - https://www.cockroachlabs.com/blog/scaling-RAFT/
> - https://zhuanlan.zhihu.com/p/104651506
> - [TiKV - Lease Read](https://mp.weixin.qq.com/s?__biz=MzI3NDIxNTQyOQ==&mid=2247484499&idx=1&sn=79acb9b4b2f8baa3296f2288c4a0a45b&scene=0#wechat_redirect)

- **Leader并行磁盘写**：Leader 将日志写到磁盘后，再将该日志复制到它的 Follower，然后等待 Follower 将该日志写到他们的磁盘上。这里出现了两次连续的磁盘写入等待，这将导致显著的延迟。
  - Leader 可以在向 Follower 并行复制日志的同时写入自己的磁盘。

![optimized-raft-pipeline](/static/image/2022-03-06/optimized-raft-pipeline.png)

- **Batch & Pipeline**
  - **Batch**：Leader 可以一次收集多个客户端 requests，然后一批发送给 Follower。需要阈值来限制每次最多可以发送多少数据，LogCabin 使用 1M 大小
  - **Pipeline**：认为网络是稳定互通的。所以当 Leader 给 Follower 发送了一批 log 之后，它可以直接更新 nextIndex，并且立刻发送后面的 log，不需要等待 Follower 的返回。如果网络出现了错误，或者 Follower 返回一些错误，Leader 就重新调整 nextIndex，然后重新发送 log。
  - **多链接Pipeline**：如果 Leader 与一个 Follower 共用一个连接使用 pipeline 的话, 那么效果和 Batch 没有多大区别，tcp 层面已经是串行。
    - 即使因为在多个连接中不能保证有序，但是大部分情况还是先发送的先到达；
    - 即使后发送的先到达了，由于有 AppendEntries RPC 一致性检查的存在，后发送的自然会失败，失败后重试即可。
  
- **Pre-vote**：网络分区会导致某个节点的数据与集群最新数据差距拉大，而且term因为不断尝试选主而变得很大。网络恢复之后，Leader 向其进行日志复制时，就会导致 Leader 因为 term 较小而下台。
  - Follower 在转变为 Candidate 之前，先与集群节点通信询问Leader是否存活。如果当前集群Leader存活，Follower 就不会转变为 Candidate，也不会增加term。
  
- **MultiRaft**：CockroachDB的优化，他的背景是一个节点会持有多个Raft实例做range。当节点的Raft实例过多时，节点间会有相当多的通信量。
  - 范围的数量远大于节点的数量（保持范围小有助于降低恢复时间）。不允许每个范围独立运行RAFT，而是管理整个节点的范围作为一个组。每对节点只需要在固定周期内让所有范围共用一个心跳包解决。
  
- **只读优化**：只读操作没必要做集群共识，这会引入多节点网络和log持久化等开销。

  - **原理**：只要客户端请求的是现集群真正的leader，那么获取到的数据就不会有错误。

    - leader必须拥有最新被提交的日志的信息
    - 在处理只读请求前，leader必须确认自己是否已经被替换掉了

  - **ReadIndex**：虽然避免不了网络请求的开销，但减少了raft的log，也避免了读操作落磁盘的开销。

    1. **心跳包**：将最新的commitIndex做为readIndex。发起一次心跳，确认leader拥有<u>最新的commitIndex</u>且<u>自己就是集群中的合法leader</u>。
    2. **读命令**：当Leader的状态机至少执行到readIndex后，向状态机发送只读命令，最后给客户端发送响应。
  
    - 注意：
      - Follower也能够响应读请求：重点在于ReadIndex必须由Leader来确定。Follower只能向Leader请求ReadIndex执行1和2步骤，然后Follower执行最后一步即可。
      - 为什么只要状态机执行到ReadIndex任何角色就都可以响应？日志匹配保障状态机会拥有这个Index下的共识状态。且客户端希望读到的内容是请求发送和响应到来之间，任意时刻的状态机最新状态。（这是系统强一致性的要求）所以ReadIndex下的状态机，持有的就是这个时间段中任意时刻的最新状态，也就是客户端想要读取的内容。
      - 当一个leader刚被当选时，无法确认自己的commitIndex是最新的。所以leader会发一个`no-op`的空白log去做共识，目的是保证commitIndex为最新

  - **Lease Read**：在Leader租期未过期时，集群中一定没有其他Leader，那么只需要保证Leader选举出来时commitIndex是最新的即可。

    - TiKV使用了写请求一并做续租，并且也为CPU时间偏差大的集群提供ReadIndex方法。



## 共识机制 Chain Replication / CRAQ

### Q：目标

CRAQ采用的方式与Zookeeper非常相似，它通过将读请求分发到任意副本去执行，来提升读请求的吞吐量，所以副本的数量与读请求性能成正比。CRAQ有意思的地方在于，它在任意副本上执行读请求的前提下，还可以保证线性一致性（Linearizability）。

提醒：CR & CRAQ的目的是提升吞吐量。二者在响应时延的节点扩展性上，与Raft类算法相比显然会差不少。



### Q：工作原理

**Chain Replication**

在链式复制中，存在多个副本节点首尾链接，可以认为是节点的链表。对于写请求而言，顺序的从head处理完毕后再到tail，最终返回给客户端写请求的响应。对应读请求，则客户端应请求tail节点由它直接响应。

在没有故障时，从一致性的角度来说，整个系统就像只有TAIL一台服务器一样，TAIL可以看到所有的写请求，也可以看到所有的读请求，它一次只处理一个请求，读请求可以看到最新写入的数据。

![chain-replication](/static/image/2022-01-06/chain-replication.png)



**Chian Replication with Apportioned Queries**

![chain-replication-with-apportioned-queries](/static/image/2022-01-06/chain-replication-with-apportioned-queries.png)

与CR相比，CRAQ使得每个节点都能处理读请求。他们的做法如下：

1. 所有节点都带有一个存储对象的版本，和版本对应的可读状态。这里的可读状态是dirty / clean，具体指这条记录是否被committed
2. 当一个节点收到写请求后，立即将数据以新版本号存储。
   - 如果该节点是Tail，那么设置状态为干净，并响应客户端。同时Tail节点会通知前面的节点更新干净状态。（具体是链式通知，还是并发通知论文2.3并没有说。我认为是链式通知，否则会引入Tail的通知瓶颈）
   - 如果不是Tail，那么设置为脏状态，转发请求给后继节点。
3. 当一个节点受到读请求后，如果最新版本是干净的，那么立马回复最新版数据；如果是脏的，那么请求Tail节点询问对应数据的最新版本号，再回复对应版本数据。
4. 当节点收到Tail的状态通知后，更新对应版本为干净状态，并删除前面的所有版本的数据。

注意：在CRAQ中，读写仍然是强一致性的。写请求返回后的读写请求一定会感知到新的数据。



TODO：稍微提一下FAWN：https://medium.com/coinmonks/chain-replication-how-to-build-an-effective-kv-storage-part-1-2-b0ce10d5afc3



### Q：故障恢复

关于故障回复，链复制则非常简单。当写请求到达Head但Head故障后，那么下一个节点则成为新的Head；当写请求到达Tail但Tail故障后，则前一个节点则充当Tail。当写请求到达中间节点时，故障节点将被去除，前一节点将传递一些最新的写请求给新后继节点。

注意：

- 单独的链复制方案很难解决脑裂：Head与下一个节点的网络通信断开，那么二者都会认为自己是Head让客户端发送写请求给自己。所以一般而言会存在Configuration Manager来作为外部权威机构，来检测节点活性从而更新链的配置。（CM认为挂掉的节点，无论它是否真的宕机，只要所有节点都有共识那么就是有效的，因为脑裂被单点解决了。加入第二个节点宕机，那么HEAD需要不停的尝试重发请求。**节点自己不允许决定其他节点的死活**。）
- 但是，如果某两个相邻节点间网络不能通信，那么Configuration Manager的信息会导致两者不停重发请求。这里就需要Configuration manager来考虑节点间的通信情况。

对比Raft类算法，Chain Replication有这些差异：

- 链复制的网络瓶颈会比Raft更高。Raft中Leader负责给所有Folliower发送AppendEntries，而链复制中每个节点只会与下一个节点做交互。
- 读写负载会落在不同节点上。（可能会有更高的吞吐量）Head只会处理写请求，而只有Tail能处理读请求。
- 更简单的故障恢复。
- （更慢的写请求处理时间。）实际上Raft除了要2倍RTT的处理时长外，还要有更复杂的选主、恢复节点等的延时。如果考虑更高的写性能，你可以用更短的链来做复制。
- （存在队头阻塞问题。）当某一个节点运行缓慢时，链复制会消耗更多的时间与Raft。因为Raft可以多数通过commit。Raft在抵御短暂的慢响应方面表现的更好。

带有Configuration Manager的Chain Replication架构极其常见，这是正确使用Chain Replication和CRAQ的方式。在这种架构下，像Chain  Replication一样的系统不用担心网络分区和脑裂，进而可以使用类似于Chain  Replication的方案来构建非常高速且有效的复制系统。比如在上图中，我们可以对数据分片（Sharding），每一个分片都是一个链。其中的每一个链都可以构建成极其高效的结构来存储你的数据，进而可以同时处理大量的读写请求。同时，我们也不用太担心网络分区的问题，因为它被一个可靠的，非脑裂的Configuration Manager所管理。



# Sharding / 分片

## Q：分片策略

> - https://zhuanlan.zhihu.com/p/107618160
> - https://pingcap.com/zh/blog/10-questions-tidb-structure

- **Hash-based**
  - **Hash Slots / 哈希槽**
    - 支持数据库：Memcached、Redis
    - 其中每个键经过Hash取模映射到某个槽上，一个节点负责一系列槽。
  - **一致性哈希**：提供一定的动态伸缩能力。在移除或者添加一个服务器时，能够尽可能小地改变已存在的服务请求与处理请求服务器之间的映射关系。
    - 支持数据库：Cassandra、DynamoDB
    - **数据结构**
      - 哈希环：将数据hash编码成一个哈希环。将个服务器抽象地扩充为M个虚拟节点：`[A1, A2, ..., B1, B2, ...]`。将虚拟节点分散地分布（可能根据UUID取模计算）于哈希环上，每个虚拟节点负责存储相邻的左边区间，比如`A2`负责`[A1对应位置, A2对应位置]`的数据。
      - BST：对于查询某个hash对应的服务器，这里使用了BST记录server对应的环位置和serverID。如果没有后继server，那么使用第一个server。
    - **查询操作**：根据数据hash计算对应哈希环上的位置，通过BST得到某个虚拟节点对应的真实服务器，再进行数据访问。
    - **修改操作**：
      - 加入数据。与查询过程相同，找到对应真实服务器后，向服务器请求添加数据。具体实现可能是拉链法。
      - 删除数据。与查询过程相同，由服务器来执行删除。
    - **服务器数量变化**
      - 扩展服务器。如添加一台服务器E在`[A, B, C, D]`的服务器中，E的UUID取模后如果是1，那么将在AB中添加一台服务器E`[A, E, B, C, D]`。现在B服务器变为负责EB之间的hash，而E服务器则会负责AE间hash。于是转移者部分数据到E上。
      - 缩容服务器。遇上相同，只是将废弃服务器数据转移到其他服务器上。
- **Range-based**：
  - **优势**：对于OLTP的关系型数据库，使用Hash分片实现有序的扫描会更困难：一个顺序扫描即使几行都可能会跨越不同的机器。
  - **静态Range分片**：将数据分片和物理机一一对应的分片策略
    - **缺点**：动态添加节点必然需要做动态的数据迁移；静态分片对于根据 workload 实时调度是不友好的，热点数据需要快速迁移来做分散
  - **动态Range分片**
    - **缺点**：顺序写入热点；小表热点问题（用于调度的最小单位Region中存在大量热点，建议Redis缓存）



## Q：原子提交协议是什么？与共识算法的区别？

> - https://www.inlighting.org/archives/distributed-atomic-commit-protocols/

在一个分布式环境中，数据被分割在多台机器上，如何构建数据库或存储系统来应付局部错误以支持事务，就是原子性提交协议的任务。原子提交协议是将一组操作组成单个操作的协议，它**提供事务的原子性性质**，并这确保了系统始终处于一致的状态。在分布式事务之外，我们也要确保当出现错误时，事务仍然具有原子性或者说具有**故障恢复**的能力。

常见的原子提交协议包括：2PC、3PC、TCC、Saga、Percolator。

TODO：简单看看后面几个，终点看看Percolator。

**2PC与Raft**

注意2PC与Raft是本质上不同的协议：

- Raft 的意义在于，将数据复制到多个参与者得到高可用，即使部分参与的服务器故障了或者不可达，系统仍然能工作。Raft 能做到这一点是因为**所有的服务器都在做相同的事情**。
- 两阶段提交中，每个参与者都在做事务中的不同部分，**参与者完全没有在做相同的事情**。所有的参与者都必须完成自己那部分工作，这样事务才能结束，**所以这里需要等待所有的参与者。**

## 原子提交协议 2PC

### Q：2PC的工作原理是什么样的？

两阶段提交中存在一种角色，**事务协调者（Transaction Coordinator）**。他们接受客户端的事务请求，并将具体命令发送给各个事务参与者，最后由协调者来判断事务提交或回退。所以2PC能够保证原子性操作的主要原因是：作为单独节点的TC不会发出不一致的Commit/Abort消息。

类似于两段锁，事务参与者会维护一个锁的表单，用来记录锁被哪个事务所持有。两阶段提交的完整过程如下：

1. 客户端发送一个事务开始请求给事务协调者TC。
2. 事务协调者生成TID，并将每条客户端交互的命令依次发送给参与者执行。

   1. 参与者根据TID和具体命令在某个粒度上上锁
   2. 参与者会在这里实际执行命令，并结果返回给事务协调者。


- **Phase#1 - Voting / Preparing**：当客户端请求Commit时，TC将发送Prepare消息给所有参与者，询问能否准备提交。
  1. 参与者在这里可以返回No来一票否决。于是TC将触发事务回退abort。
     - 否决的原因，比如它们因为这个事务会引起死锁，或许它们在故障重启后并完全忘记了这个事务。
  2. 参与者进行持久化：事务的修改、该TID的所有的锁、和事务的prepare状态。这会在故障恢复时起作用。
  3. 响应Yes给TC。
- **Phase#2 - Commiting**：当所有参与者返回同意后，TC先持久化事务信息，再发送Commit消息给所有参与者；或者一票否决后发送Abort。
  - 参与者此时开始持久化所有变更（或者回退时根据Log回退事务），释放所有TID对应的锁，最后返回ACK作为响应。
  - 参与者在这里可以将TID对应的持久化日志删除。
  - （提交阶段的意义在于通知每一个节点：其他节点都能够保证提交，所以你这个节点可以提交；或其他某个节点不能保证提交，所以你要回滚）

当所有ACK被返回后（或者在abort事务后），TC将事务结果返回给客户端，并将TID对应的持久化信息删除。



### Q：2PC的故障恢复如何进行？

**对于参与者**

- 它可能在回复 Prepare 消息之前就崩溃了。
  - 那么它可以单方面的回复No来撤销事务

- 它可能在回复Prepare Yes消息之后崩溃。这说明无论是否存在任何外部意外事件，参与者保证都可以提交该事务。
  - 在回复 Prepare 之前，它必须确保记住当前事务的中间状态，记住所有要做的修改，记住事务持有的所有的锁，这些信息必须在磁盘上持久化存储。
  - 当它重启恢复时，通过查看自己的 Log，它可以发现自己正在一个事务的中间，并且对一个事务的 Prepare 消息回复了 Yes。
  - 当参与者最终收到了 Commit 而不是 Abort，那么通过读取 Log，参与者就知道如何完成它在事务中的那部分工作。
- 它可能在收到 Commit 之后崩溃了。
  - 参与者有可能在处理完 Commit 之后就崩溃了。但是这样的话，它就完成了修改，并将数据持久化存储在磁盘上了。这样的话，故障重启就不需要做任何事情，因为事务已经完成了。
  - 但因为TC没有收到 ACK，那么TC会再次发送 Commit 消息。当参与者重启之后，收到了 Commit 消息时，它可能已经将 Log 中的修改写入到自己的持久化存储中、释放了锁、并删除了有关事务的 Log。这里参与者可以记住事务的信息，但是这会消耗内存，所以实际上它会完全忘记已经在磁盘上持久化存储的事务的信息。那么**对于一个它不知道事务的 Commit 消息，参与者会简单的 ACK 这条消息。**

**对于协调者**

- 如果事务协调者在发送 Commit 消息之前就崩溃。
  - 在重启后不需要任何操作，TC默认这个事务被撤回了。因为没有一个参与者会 Commit 事务。
  - 但参与者可以在自己的 Log 中看到事务，又从来没有收到 Commit 消息。那么事务的参与者会向事务协调者查询事务，**事务协调者会发现自己不认识这个事务，它必然是之前崩溃的时候 Abort 的事务，响应abort**即可。
- 协调者在发送完一个或者多个 Commit 消息之后崩溃。这里不允许TC忘记相关的事务，因为这里需要最终决定是否应该提交或回退事务。
  - 在发送任何 Commit 消息之前，它必须先将事务的信息持久化。
  - 当事务协调者故障重启时，对于执行了一半的事务，事务协调者会向所有的参与者重发 Commit 消息或者 Abort 消息，以防在崩溃前没有向参与者发送这些消息。这就是为什么参与者需要准备好接收重复的 Commit 消息的一个原因。

**考虑网络问题**

- 事务协调者发送了 Prepare 消息，但是并没有收到所有的 Yes/No 消息
  - **重新发送一轮 Prepare 消息**。这表明自己没有收到全部的 Yes/No 回复。事务协调者可以持续不断的重发 Prepare 消息。但是如果其中一个参与者要关机很长时间，我们将会在持有锁的状态下一直等待。
  - **撤销事务**。如果一个崩溃了的参与者重启了，向事务协调者发消息说，我并没有收到来自你的有关事务 95 的消息，事务协调者会发现自己并不知道到事务 95 的存在，因为它在之前就 Abort 了这个事务并删除了有关这个事务的记录。这时，事务协调者会告诉参与者说，你也应该 Abort 这个事务。
- 如果参与者等待 Prepare 消息超时
  - **撤销事务**。如果事务协调者上线了，再次发送 **Prepare 消息，B 会说我不知道有关事务的任何事情并回复 No**。
- 如果参与者等待 Commit / Abort 消息超时
  - 或许TC在很久一段事件中断电，但其他事务在等待锁的释放。所以我们应该尽早的 Abort 事务，并释放锁。但参与者不允许 Abort 事务，它必须无限的等待 Commit 消息，这里通常称为 Block。
  - **Block行为是两阶段提交里非常重要的一个特性**。在特定的故障中，你会很容易的陷入到一个需要等待很长时间的场景中，因为参与者会一直持有锁，并阻塞其他的事务。所以各种2PC的变种会尽量让这部分尽可能轻量化，甚至对于一些变种的协议，对于一些特定的场景都不用等待。（这里可以让TC作为副本系统，降低宕机可能性）
- 如果TC没有受到所有的ACK
  - 它会假设丢包了并**重发 Commit 消息**。如果一个参与者收到了一个 Commit 消息，但是它并不知道对应的事务，因为它在之前回复 ACK 之后就忘记了这个事务，那么参与者会再次回复一个 ACK。（因为它之前已经完成对这个事务的 Commit 或者 Abort，然后选择忘记这个事务了）
  - （或许这里不需要接受ACK？必须接受，因为这能确保参与者接受到Commit信息）



### Q：2PC的缺点和优化？

下面是2PC的两个问题：

- **锁的性能问题**：本地事务在prepare阶段锁定资源，造成了延迟和性能下降。

- **协调节点单点故障**：协调节点如果发生故障，整个事务会一直阻塞。

下面是一些简单优化：

- **提前准备投票**：在投票阶段，如果参与者节点知道这时最后一个执行的操作，那么该节点也将返回准备阶段的投票和查询结果。
- **提前响应客户端**：如果所有节点都投票提交一个事务，协调器可以在提交阶段结束之前向客户端发送一个事务成功的确认

# System Study / 系统研究



## MapReduce [2004]

### Q：执行流程

**编程模型**

首先，系统的用户会将数据按照kv的形式存储，比如`<filename, content>`的形式。然后用户需要编写一些逻辑：

- **Map**：`map(k1, v1) -> list(k2, v2)`
  - 是指拆分的过程。将`v1`根据`k1`进行某种拆分，得到一系列的kv对。
- **Reduce**：`reduce(k2, list(v2)) -> list(v2)`
  - 是指聚合的过程。这里MapReduce将Map出来的kv对进行合并，然后交由用户来处理聚合的逻辑。



**执行流程**

![mapreduce](/static/image/2021-08-30/mapreduce.jpeg)

1. 用户程序首先将输入文件分割成若干份确定大小的分区，然后在worker节点和master节点执行用户程序。
2. master节点分配map和reduce任务
3. map节点读取分区数据，产生**中间kv，存储在内存中**
4. map节点**周期性存储中间数据于磁盘**，在map完成后向master通知输出R个文件的位置和大小。master更新数据结构，通知reduce处理新的文件
5. reduce节点受到通知后，通过RPC读取map本地数据。当读取完所有map数据后（全部结束？），所有数据按key排序。
6. 用户程序的reduce函数开始执行。reduce的输出以追加方式写入结果文件，当reduce结束则原子性修改文件名。
7. 全部map reduce执行完毕后，master开始执行用户程序

**注意**：

1. map本地文件会生成几个？

   1. R个。当所有map结束后，reducer依次拉取各个map本地数据做reduce。

2. map和reduce函数是同时执行么？即reduce读map的一部分周期提交数据进行处理？

   - 阅读后文发现，不是同时执行的。

     > When a map task completes, the worker sends a message to the master and includes the names of the R temporary files in the message.

   - 注意map函数不可以与reduce节点的排序同时执行，因为reduce函数需要等待某个key结束了map，然后再做数据的排序。

3. 为什么reduce函数执行前必须要排序？

   - 排序后的kv对，可以在同一时刻只reduce一个list。当所有list被reduce后，一次性写入输出文件并原子命名。
   - 如果不进行排序，将同时处理大量不同的key，会需要大规模的线程（内存、切换时间）、内存（极端情况是所有数据）

4. master维护什么状态？

   - 各个节点的状态机模型：idle、in-progress、completed
   - 节点的identy信息
   - 每个完成的map产出的文件位置和大小

### 

### Q：容错机制

**Worker 容错**

master会周期性ping各个worker，如果失败则说明节点失败。

| 任务类型 \ 崩溃时当前任务状态 | 正在执行                         | 执行完成            |
| ----------------------------- | -------------------------------- | ------------------- |
| map                           | 重新分配节点，该节点返回idle状态 | 重新分配节点执行map |
| reduce                        | 重新分配节点，该节点返回idle状态 | 不做处理            |

**注意**：

1. 为什么map在失败后，不能使用当前任务输出的中间kv？
   - 因为中间kv是存于内存，同时周期性写入的文件只是本地文件。也因此map节点失败后，只能重新分配并执行。
2. 为什么reduce在执行完成后，不用处理失败？
   - 因为reduce输出文件是全局公用的，reduce节点失败不影响存储。



**Master 容错**

- **方案一：恢复数据** - 将所有状态存储checkpoint，失败后恢复即可
- **方案二：let it crash** - 单节点的master不太可能失败，若失败则用户自行处理



**原子性提交**

- 当map和reduce函数是确定性的（纯函数），那么节点通过提交原子性commit，来达到无错误的顺序执行。
  - map节点执行成功后，向master发送信息。master将忽略已经执行完的任务的信息，以避免冗余。
  - reduce节点执行成功后，依靠操作系统的原子性重命名（mv）来提供原子性。
- 当map和reduce函数是非确定性的（非纯函数），那么会提供更弱的方式。具体是什么，论文似乎没有提到。



### Q：优化

- **备份任务 / 冗余计算**：一个使运行时间变长的常见原因是某个任务掉队。原因比如硬盘速度太慢、CPU缓存禁用等。
  - 增加副本机制，当某些任务执行时长太久，则新分配一个节点给此任务。当主备某一个完成后，则其他任务终止。



## VMwareFT

### Q：目标

VMware FT(Fault Tolarence)是指两个虚拟机的主备容错。它需要两个物理服务器，Primary虚机在其中一个物理服务器上，Backup在另一个物理服务器上。（将Primary和Backup运行在一台服务器的两个虚拟机里面毫无意义，因为容错本来就是为了能够抵御硬件故障）



### Q：工作原理

在真实场景下，一个局域网中存在着primary宿主机和backup宿主机，他们分别运行着primaryVMM和backupVMM (VMM，Virtual Machine Monitor)，用于监控主备VM。同时这个局域网中还存在者一些client（此处用于VM存储的disk server也可以算为client）。

**主备同步时机与流程**

首先讲<u>主备同步的时机，并如何进行主备同步</u>。当client向primary发送一个请求分组后，将触发**primary host的中断，之后这个中断将数据送给VMM**。此时VMM可以发现该分组是需要发送给primary vm的，于是VMM开始如下两个操作：

1. 向**本地（primary）的vm模拟网络请求中断**，将数据发送给primary vm的应用程序中。（当primary VMM收到处理完的primary输出，并收到到backup的ACK后，再发送响应。这个过程被称为输出控制）
2. **向backup host发送一个相同的网络请求**，之后backup VMM将可以受到此分组。

此时主备vm都受到了相同的外部输入，他们会以相同的方式处理外部输入，并最终达到状态一致。最终，primary将网络响应发送给client，backup因为知道自己是备份所以丢弃响应。（论文中的Log Channel就特指局域网中priamry向backup VMM发送外部输入的信道）

**主备切换流程**

其次，讲<u>主备切换</u>的过程。实际场景中，backup能够在一秒内受到很多条log，有一部分是primary的定时器中断（大概100次每秒）。当backup没有在一段时间内受到primary的定时器中断后，说明primary出现失败，需要主备切换了。

1. 首先backup不再接受来自primary VMM的log，而是接受网络输入作为外部输入源，同时不再丢弃输出分组。
2. backup在网络中做一些处理（？），使得client都转而访问backup。



### Q：容错机制

**主备切换后的重复响应**

当Backup的Log缓冲区仍有很多剩余未处理时，Primary由于故障而Backup触发接管机制。这时Backup的外部输入Log将引导vm发出重复的响应。

但是需要注意到，副本在TCP层面也进行了复制。这说明Backup知晓链接的TCP序列号信息，这样重复响应会被Client的TCP栈抛弃。同时，Backup和Client之间并没有真正保持TCP链接，所以Backup应该会受到Client的TCP Reset响应。（我猜测Backup可以不做处理，这样Client没有受到响应。那么client会有两种反应：请求没有到达primary，所以系统没有状态变化；或者响应没有到达client，所以系统已经发生变化。这本来就是client需要考虑的事情。所以backup在这个情况下，完全可以不处理Reset。）

通常而言，<u>分布式系统基本不可能保证不产生重复输出</u>。这需要其他机制来处理，一种可能是在应用层面设计序列号。

**Split Brain / 脑裂**

> 分布式场景下的定理：你无法判断一个节点是否真的挂了

VMware的解决方法是主备依赖第三方的TestAndSet服务。当主备的网络通信断掉后，双方都会申请成为primary。TestAndSet服务相当于一个锁，它决定了主备之中谁应该成为priamry。

这里TestAndSet服务存在单点故障问题。但VMware肯定也考虑到这点，所以它应当也是有主备的。



## ZooKeeper  （TODO）

### Q：目标

**公共组件**

ZooKeeper 的目标是为大型分布式计算提供开源的分布式配置服务、同步服务和命名注册。它将复杂且容易出错的分布式一致性服务封装起来，构成一个高效可靠的原语集，并以一系列简单易用的接口提供给用户使用。

一个典型的分布式数据一致性的解决方案，分布式应用程序可以基于它实现诸如数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master 选举、分布式锁和分布式队列等功能。



**一致性保证： 写线性一致性 & 客户端顺序一致性**

与线性一致一样，这些保证与序列有关。Zookeeper有两个主要的保证，它们在论文的2.3有提及。总的说来，Zookeeper提供了客户端顺序一致性和线性一致的写。

- **写请求线性一致**
  - 如果一个写请求在另一个写请求开始前就结束了，那么Zookeeper实际上也会先执行第一个写请求，再执行第二个写请求。
- **FIFO客户端顺序保证**：任何一个客户端的请求，都会按照客户端指定的顺序来执行
  - **写请求**：会以这个客户端发送的相对顺序，加入到所有客户端的写请求中。所以，如果一个客户端顺序完成三个写操作，那么在最终整体的写请求的序列中，可以看到这个客户端的写请求以相同顺序出现。对于异步的请求，可以假设，客户端实际上会对它的写请求打上序号。
  - **读请求**：**需要注意只经过副本去处理读**。读请求也会像上文的样子，客户端顺序完成三个读操作，最终会看到副本会依次做读请求。注意，处理读请求时Log的长度也会增长，后来的读请求会比之前的能看到更多的Log。

**FIFO客户端请求序列是对一个客户端的所有读请求，写请求生效**。所以，如果我发送一个写请求给Leader，在Leader  commit这个请求之前需要消耗一些时间，所以我现在给Leader发了一个写请求，而Leader还没有处理完它，或者commit它。之后，我发送了一个读请求给某个副本。这个读请求需要暂缓一下，以确保FIFO客户端请求序列。读请求需要暂缓，直到这个副本发现之前的写请求已经执行了。这是FIFO客户端请求序列的必然结果，（对于某个特定的客户端）读写请求是线性一致的。

这里还需要提一下`sync(path)`操作，它是指等待在`sync`操作前的所有写操作，本质上是写请求。是一个弥补非线性一致的方法。一般场景是这样的：客户端先发送`sync`再做读请求，那么副本会先sync请求后，再返回读请求。这个读请求可以保证看到sync对应的状态，所以可以合理的认为是最新的。



### Q：工作原理





### Q：API与应用场景

它的API中定位结构类似znode组成的树，znode会有三种类型

- **Regular znode**：永久的znode，除非删除掉。
- **Ephemeral znode**：短暂/会话znode，当会话结束时撤销。
- **Sequentail znode**：顺序znode，在实际创建时会在名字后添加序号。当有多个客户端创建同一个名字znode时，ZooKeeper保证没有命名冲突。

API包括3个写操作和3个读操作，详情可见论文2.2。下面是一些需要注意的地方：

- `create`是排他的操作，当多个客户端创建同一个名字znode时，保证只有一个客户端能受到创建成功的消息，其他客户端会受到失败消息。
- 所有写操作都带有`version`信息（`create`默认0），所以都是基于key级别的原子CAS操作模式。几乎所有读操作都有`watch`选项（除了`list`），当客户端受到文件变动时，需要做同步操作。（？）

这种API可以实现一系列有用的模式：

- **自旋锁**：原子CAS提供的getData+setData
  - 需要注意的是这个方式针对客户端数量是`O(n^2)`的。如100个客户端同时请求修改，那么一共需要`n^2`的通信次数
  - 高负载的情况需要合理的等待机制。如raft中的随机等待时间，这时在处理未知数量客户端的合理方式。或者直接用下面的lock。
- **非扩展锁**：利用`create(ephemeral=true)`和`exist(watch=true)`。当create成功则获得锁，不成功则exist监听删除变动做等待
  - 注意一定要用会话级别的znode文件。防止客户端宕机不释放锁。
  - 在`create`和`exist`之间如果锁已释放，那么不需要做watch等待，只需要判断exist

Zookeeper的API能够解决什么样的事情？

- Test-and-Set
- 集群通用配置信息：比如发布Master的地址
- 选举Master
- 服务注册



## Aurora

Aurora的目标（解决云存储下的写放大、容错）& 工作原理（变种Qourum）

### Q：目标

在MySQL基础上，结合Amazon自己的基础设施RDS。对于RDS来说，有且仅有一个EC2实例作为数据库。这个数据库将它的数据页和WAL存储在EBS云存储上。当数据库执行写操作时，实际上请求通过网络发送到了EBS服务器。所有这些服务器都在一个AZ中。下面是MySQL镜像方案的架构：

![mirrored-mysql](/static/image/2022-01-09/mirrored-mysql.png)

Aurora的出现在于解决两个问题：

- **写放大**：RDS的写操作代价极高
  - 每一次写操作（如写日志、写Page），数据需要发送给AZ1的两个EBS副本（EBS通过Chain Replication），还需要发送AZ2的副数据库。而副数据库又会将数据再发送给AZ2的两个独立的EBS副本。之后，副数据库给主数据库响应成功，才会认为写操作完成了。

- **更合理的容错**

  - **写操作**：如果只有一个AZ挂了，那么写操作不受影响。

  - **读操作**：当一个AZ加上另一个处于其他AZ的服务器挂了之后（即AZ+1台服务器），读操作不受影响。

  - **容忍慢副本**：Aurora期望能够在出现短暂的慢副本时，不像Chain Replication那样阻塞，而是仍然能够继续执行操作。

  - **快速副本**：如果一个副本挂了，备份数据是争分夺秒的。
    - 通常来说服务器故障不是独立的，如果一个服务器挂了，通常意味着有很大的可能另一个服务器也会挂，因为它们有相同的硬件，或许从同一个公司购买，来自于同一个生产线。如果其中一个有缺陷，非常有可能会在另一个服务器中也会有相同的缺陷。所以，当出现一个故障时，第二个故障可能很快就会发生。



### Q：工作原理

![aurora](/static/image/2022-01-09/aurora.png)

1. 现在替代EBS的位置，有6个数据的副本，位于3个AZ，每个AZ有2个副本。所以现在有了超级容错性，并且每个写请求都需要以某种方式发送给这6个副本。
   - 数据库服务发送给存储服务的数据只有Log，而非data page等大数据。这节约了相当多的带宽用量。
   - Aurora抛弃了Chain Replication，它并不需要6个副本都确认了写入才能继续执行操作。这里是一个变种的Quorum，具体而言是Write Quorum为4、Read Qourum为。那么只要4个副本写入成功，数据库就可以继续执行操作。
2. 存储服务器内存最终存储的还是数据库服务器磁盘中的page。
   - 当一个新的写请求到达时，新的Log条目，它会立即被追加到影响到的page的Log列表中。
   - 当读请求到来时，才立即执行这个Log，来更新EBS本地的data page。
3. Aurora不仅有可读写的主数据库实例，同时还有多个数据库的只读副本。
   - 只能支持一个写入者的原因是，Log需要按照数字编号。如果有多个数据库以非协同的方式处理写请求，那么为Log编号将会非常非常难。
   - 当向只读数据库发送读请求后，只读数据库需要弄清需要查询哪些data page，之后直接从存储服务器读取，同时会缓存读取到的page。
   - 只读数据库也需要更新自身的缓存，所以主数据库也会将它的Log的拷贝发送给每一个只读数据库。只读数据库用这些Log来更新它们缓存的page数据，进而获得数据库中最新的事务处理结果。（这意味着只读数据库会落后主数据库一点，通常来说不会是一个大问题。）（对只读服务器而言，是否是最终一致性呢？只读数据库是否应该作为Read Qourum或者是N的一员？）
   - 数据库背后的B-Tree结构非常复杂，可能会定期触发rebalance。中间状态的数据是不正确的，只有在rebalance结束了才可以从B-Tree读取数据。但是只读数据库直接从存储服务器读取数据库的page，它可能会看到在rebalance过程中的B-Tree。这时看到的数据是非法的，会导致只读数据库崩溃或者行为异常。

**Qourum Replication**

Aurora使用的是一种经典quorum思想的变种。Quorum系统背后的思想是通过复制构建容错的存储系统，并确保即使有一些副本故障了，读请求还是能看到最近的写请求的数据。

假设有N个副本。为了能够执行写请求，必须要确保写操作被W个副本确认，W小于N。所以你需要将写请求发送到这W个副本。如果要执行读请求，那么至少需要从R个副本得到所读取的信息。这里的W对应的数字称为Write Quorum，R对应的数字称为Read Quorum。这是一个典型的Quorum配置。这里的关键点在于，W、R、N之间的关联。Quorum系统要求，任意你要发送写请求的W个服务器，必须与任意接收读请求的R个服务器有重叠。这意味着，R加上W必须大于N（ 至少满足R + W = N + 1 ），这样任意W个服务器至少与任意R个服务器有一个重合。

这里还有一个关键的点，客户端读请求可能会得到R个不同的结果。现在的问题是，客户端如何知道从R个服务器得到的R个结果中，哪一个是正确的呢？**通过不同结果出现的次数来投票（Vote）在这是不起作用的**，因为我们只能确保Read Quorum必须至少与Write  Quorum有一个服务器是重合的，这意味着客户端向R个服务器发送读请求，**可能只有一个服务器返回了正确的结果**。对于一个有6个副本的系统，可能Read  Quorum是4，那么你可能得到了4个回复，但是只有一个与之前写请求重合的服务器能将正确的结果返回，所以这里不能使用投票。在Quorum系统中使用的是**版本号**（Version）。所以，每一次执行写请求，你需要将新的数值与一个增加的版本号绑定。之后，客户端发送读请求，从Read Quorum得到了一些回复，客户端可以直接使用其中的最高版本号的数值。

如果你不能与Quorum数量的服务器通信，不管是Read Quorum还是Write Quorum，那么你只能不停的重试。

Aurora采用的变种Qourum是指N=6，W=4，R=3。它能够实现上一节描述的Aurora的容错目标：

- 对于写操作。W等于4意味着，当一个AZ彻底下线时，剩下2个AZ中的4个服务器仍然能完成写请求。
- 对于读操作。R等于3意味着，当一个AZ和一个其他AZ的服务器下线时，剩下的3个服务器仍然可以完成读请求。
- 对于慢副本。Quorum系统本身可以剔除暂时的慢副本。
- 对于快速生成副本。当3个服务器下线了，系统仍然支持读请求，但是却不能支持写请求。所以当3个服务器挂了，变种Quorum系统有足够的服务器支持读请求，并据此重建更多的副本，但是在新的副本创建出来替代旧的副本之前，系统不能支持写请求。



## Frangipani

Frangipani是一个分布式网络文件存储的客户端，它在系统内核中实现了文件系统。对应的，Petal作为它的服务端，是共享虚拟磁盘服务。Petal包含文件系统的数据结构，例如文件内容、inode、目录、目录的文件列表、inode 和块的空闲状态。

![frangipani-layering](/static/image/2022-01-10/frangipani-layering.png)

这个系统在工作站（或者说系统中）里面做了大量的缓存，并且文件的修改可以在本地缓存完成。因为这样数据可以在微秒级别读出来，而不是毫秒级别的从文件服务器获取它们。注意这里的缓存是支持Write-Back的。

（缓存有两种：Write-Direct在数据更新时同时写cache和存储、Write-Back在数据替换出cache时更新存储）



### Q：目标

在这个系统中，有三个主要的挑战：

- **缓存一致性**：当有人在大厅里说自己在文件系统里面做了修改，其他人应该能看到这个修改。缓存一致性，在这里我认为就是在缓存的系统中的读写强一致性。
- **原子性更新**：我们希望写操作不会与相同时间其他工作站的操作相互干扰。即使对于复杂的操作，涉及到修改很多状态，我们也希望这些操作表现的好像在一个时间点发生。
- **故障恢复**：由于 Write-Back 缓存，可能会在本地的缓存中堆积了大量的修改。如果我的工作站崩溃了，但是这时这些修改只有部分同步到了 Petal，还有部分仍然只存在于本地。不管如何，其他客户端/应该看到一个一致的文件系统，而不是一个损坏了的文件系统数据。

需要注意的是：

- 本论文是1997年发布的，所以硬件资源不高，而且它的目标是50人的小工作组。所以这不是分布式存储的主要应用场景。真正的应用场景是一些大型的数据中心、大型网站、大数据运算，在这些场景中，文件系统的接口相比数据库接口来说，就不是那么有用了。
- 而且，在大数据的场景下，缓存就显得累赘。如果你读取 10TB 的数据，缓存基本上没什么用，并且会适得其反。所以，随着时间的推移，Frangipani 在一些场合还是有用的，但是并不符合在设计新系统时候的需求。



### Q：工作原理

**锁服务：缓存一致性 & 原子更新**

在 Frangipani 系统中还有第三类服务器：锁服务器。在锁服务器里面，有一个locks表单。我们假设对于每一个文件都有一个锁，它可能会被某个工作站所持有。同时，每个工作站都会记录跟踪它所持有的锁，和锁对应的文件内容。所以在每个工作站中，Frangipani 模块也会有一个 lock 表单，表单会记录文件名、对应的锁的状态和文件的缓存内容。

当一个 Frangipani 服务器决定要读取文件，比如读取目录 `/`、读取文件 `A`、查看一个 inode，首先，它会向一个锁服务器请求文件对应的锁，之后才会向 Petal 服务器请求文件或者目录的数据。收到数据之后，工作站会记住，本地有一个文件 X 的拷贝，对应的锁的状态，和相应的文件内容。

每一个工作站的锁至少有两种模式。工作站可以读或者写相应的文件或者目录的最新数据，可以在创建，删除，重命名文件的过程中，如果这样的话，我们认为锁在 Busy 状态。只要系统调用结束了，工作站会在内部释放锁，现在工作站不再使用那个文件。但是从锁服务器的角度来看，工作站仍然持有锁。工作站内部会标明，这是锁是 Idle 状态，它不再使用这个锁。

这里的锁在使用上有一些规则：

- 工作站不允许持有缓存的数据，除非同时也持有了与数据相关的锁。实际上，我认为这里的**锁是请求对某个文件的缓存能力**。仅当持有写锁时，客户端才有一份可写的数据缓存；同样，只有获得读锁才能有读的缓存。
- 释放锁之前，应先向 Petal 存储系统写数据，再从客户端 lock 表单中删除锁和缓存。这里可以说释放缓存的能力，但在这之前需要将缓存写回。

下面介绍客户端与锁服务之间的缓存一致性协议 / 接口：

- **Request**：指客户端请求锁服务获得文件的锁。
- **Grant**：指锁服务授权客户端这个文件锁。注意这里Request和Grant是异步的，锁服务接受Request后会再请求其他客户端释放锁。
- **Revoke**：指锁服务请求客户端释放锁。
  - 如果工作站收到 Revoke 消息时，它还在使用锁（Busy状态）。那么直到它完成了相应的文件系统操作，才会放弃锁。
  - 如果是Idle状态，那么首先客户端要同步元数据的WAL到Petal，接着将文件数据写入Petal，最后再释放锁。
  - 工作站会每隔 30 秒会将所有修改了的缓存写回到 Petal 中
- **Release**：客户端根据锁的状态来放弃锁，是Revoke的响应。二者异步。



### Q：故障恢复

**元数据WAL**

Frangipani 与其他的系统一样，需要通过预写式日志WAL，实现故障可恢复的事务：（似乎原子性的逻辑(事务、分布式锁)都需要提供WAL来恢复）

- 当工作站向 Petal 写入任何数据之前，它会在自己的Petal Log列表中追加一个 Log 条目。此Log会描述整个的需要完成的操作。
- 只有当这个描述了完整操作的 Log 条目安全的存在于 Petal 之后，工作站才会开始向 Petal 发送数据。

所以如果工作站可以向 Petal 写入哪怕是一个数据，那么描述了整个操作、整个更新的 Log 条目必然已经存在于 Petal 中。

除了经典的WAL外，Frangipani还提到Petal的Log列表是一个环形空间，且每个Log都需要自增的序号（用于检测Log结尾）。同时每个Log还会描述每个操作，包括数据块号、版本号和一些数据。注意，Log 只包含了对于元数据的修改，比如说文件系统中的目录、inode、bitmap 的分配。Log 本身**不会包含需要写入文件的数据**。（所以故障恢复是不会恢复文件内容的，这与现代Unix文件系统缓存一样。引用见FAQ。）

**故障恢复代理人**

Frangipani 总是会先将自身的 Log 先写入到 Petal。这意味着如果发生了故障，那么发生故障时可能会有这几种场景：

- 要么工作站正在向 Petal **写入 Log**，所以这个时候工作站必然还没有向 Petal 写入任何文件或者目录。
- 要么工作站正在向 Petal **写入修改的文件**，所以这个时候工作站必然已经写入了完整的 Log。

假设一个其他的工作站需要崩溃了的工作站所持有的一个锁，锁服务器会发出 Revoke 消息，但是锁服务器永远也不会从崩溃了的工作站收到 Release 消息。Frangipani 出于一些原因对锁使用了租约，当租约到期了，锁服务器会认定工作站已经崩溃了，之后它会初始化恢复过程。实际上，锁服务器会通知另一个还活着的工作站说：看，工作站 1 看起来崩溃了，请读取它的 Log，重新执行它最近的操作并确保这些操作完成了，在你完成之后通知我。

那么详细来谈故障场景：

- 当WAL未写入时崩溃。当其他代理人 WS2 执行恢复，查看崩溃了的工作站的 Log 时，发现里面没有任何信息，自然也就不会做任何操作。之后 WS2 会释放 WS1 所持有的锁。
- 当WAL部分写入时崩溃。执行恢复的工作站 WS2 会检查每个 Log 条目，并重新向 Petal 执行 WS1 的每一条 Log。当 WS2 执行完 WS1 存放在 Petal 中的 Log，它会通知锁服务器，之后锁服务器会释放 WS1。注意这里会检查Log完整性，存在校验和机制。
  - 这里说明在事务层面故障回复不是原子性的，或者说针对每个文件是原子性的。
- 当块数据写入时崩溃。执行恢复的工作站 WS2 并不知道 WS1 在哪个位置崩溃的，它只能看到一些 Log 条目。所以WS2 会以相同的方式重新执行 Log。尽管部分修改已经写入了 Petal，WS2 会重新执行修改。（这里是怎么知道要从哪个操作开始执行恢复？应该是每次释放锁只有一个Log，它包含了所有操作，也就是为什么需要递增序号）

这里有一个时序的例子能够说明当前的方法还有缺陷：

```
WS1:   delete(/a)                 CRASH
WS2:                create(/a)
WS3:                                        RECOVER
```

当WS3恢复WS1的WAL时，不能直接执行delete操作，因为对应文件已经不是原有的文件了。所以我们不能只是不经思考的重新执行 WS1 的 Log，WS1 的 Log 在我们执行的时候可能已经过时了，其他的一些工作站可能已经以其他的方式修改了相同的数据，所以我们不能盲目的重新执行 Log 条目。Frangipani 是这样解决这个问题的，通过对每一份存储在 Petal 文件系统**块数据增加一个版本号**，同时将**版本号与 Log 中描述的更新关联**起来。如果一个工作站没有故障，并且成功的将数据写回到了 Petal。这样元数据的版本号会大于等于 Log 条目中的版本号。如果有其他的工作站之后修改了同一份元数据，版本号会更高。



## Spanner

### Q：目标

Google Spanner，它是一个非常少见的既提供了**分片的分布式事务**，又提供了强一致的**外部一致性**的数据库系统。

在Spanner的设计中，有两个非常巧妙的想法：

- 在paxos复制的事务参与者上进行了两阶段提交；
- 他们使用同步时间来获得非常有效的只读事务。



### Q：工作原理

（paxos+读写事务2pc / 只读事务无锁+Follower请求+TrueTime时间戳的MVCC）

首先，Spanne使用Paxos Group来组织每个数据分区（分片）。即同一种分片的节点被一个Group管理，每个Group运行与同一个数据中心。也就是说每个数据中心都有一份完整的数据。这会提供一下好处：

![spanner-dc](/static/image/2022-01-16/spanner-dc.png)

- 数据分片会保证通过并行来提高系统吞吐量
- 客户端可以选择距离最近的数据中心，降低网络延迟
- 数据中心挂掉不影响系统可用性
- Paxos只需要半数以上节点响应就可以工作，这可以容忍慢节点

下面讨论一下外部一致性，它是一种比线性一致性（linearizability，也叫可线性化）更强的一致性特性。这里需要注意，线性一致性只是描述系统对客户端请求的表现。而外部一致性重点针对事务，它会在可串行化调度中，按照事务提交的时间顺序选择一个事务调度方式。

> **线性一致性**，是指AB两个操作，若A先于B被响应，那么A操作就先于B操作发生 / 或者说实际操作会发生在请求和响应之间的某个地方。这个操作会被全局所有节点发现。
>
> 调度是指事务中每个操作被执行的顺序。
>
> **可串行化调度**，是指系统每次只运行一个事务。比如，两个事务到来`Ta{A1, A2, A3}`和`Tb{B1, B2, B3}`，只会存在两种调度方式：`{A1, A2, A3，B1, B2, B3}`或`{B1, B2, B3，A1, A2, A3}`。 
>
> 而**外部一致性**会根据事务提交的时间顺序选择一种调度。比如`Ta`比`Tb`更早提交，那么只有`{A1, A2, A3，B1, B2, B3}`。注意，如果是在单机数据库中，可串行化是与外部一致性完全相同。

支持外部一致性会给系统带来更低的性能，如可用性、延迟和吞吐量。这就是为什么其他数据库支持更弱的一致性，或者不支持分布式事务。

那么Spanner为什么能同一两者？因为Spanner迫使客户端在请求前指定只读和只写，并对各个场景做了优化。

**外部一致性**

- 当读写事务只在同一个分片中执行。那么该事务将会在这个Paxos Group的Leader上做两段锁协议。这与单节点数据库很像，通过两段锁就可以提供外部一致性。
- 当读写事务在不同分片上执行时。系统将运行两阶段提交，通过长期持有的锁来达到外部一致性。这里Spanner解决事务协调者宕机后引起的全局卡死的方法，是通过利用Paxos提高事务协调者的容错性，让它难以宕机。 

**外部一致性 - 读写事务**

对于2PC具体来说：

- 对于读请求而言，客户端可以直接访问对应分片的Leader读取数据。注意这里Leader将持有某个数据的读锁。
- 事务客户端会将写请求缓存下来，当事务提交时发送给任意一个分片的Leader。
  - 该Leader将作为事务协调者来发送Prepare信息给其他参与的分片Leader，各个分片的Leader还会向该分片的所有Follower转发Prepare请求。各个参与者Leader会管理锁和故障恢复日志。
  - 事务协调者Leader在准备Commit时，还会向所有Leader发送Commit，每个Leader也还会发送Commit给各个Follower。当事务参与者响应ACK后，释放锁并删除故障回复日志；当所有ACK被受到后，TC结束自己的工作，删除故障回复日志。

**外部一致性 - 只读事务**

这就是Sapnner针对读写事务的基本做法。Spanner还针对只读事务进行优化，使得它比读写事务有更低的延迟（10x）：

1. 只读事务不会执行2PC来服务请求。这是无锁的过程。
2. 客户端可以在各个分片的Follower上做读请求，而非Leader。这就可以利用客户端最近的节点来做读请求。

但是，这种不加锁的读取任意Follower会导致两个问题：

1. 读取任意Follower会导致读取到非最新数据。
2. 不加锁会导致只读事务的出现，让系统不再支持外部一致性。如只读事务的每个读请求会穿插在各个RW事务中。

对于问题2，Spanner使用**快照隔离**来解决。（MySQL通过MVCC提供了支持读已提交和可重复读的快照隔离）

- 对于只读事务，Spanner选择第一个读请求开始的时间作为事务时间戳。
- 对于读写事务，Leader选择Commit的时间作为事务事件戳。

那么Spanner可以让只读事务只访问小于该时间戳的事务结果。

```
                      x@10=9         x@20=8
                      y@10=11        y@20=12
    T1 @ 10:  Wx  Wy  C
    T2 @ 20:                 Wx  Wy  C
    T3 @ 15:             Rx             Ry

  "@ 10" indicates the time-stamp.

  - Now,T3's reads will both be served from the @10 versions.
    T3 won't see T2's write even though T3's read of y occurs after T2.

  - The results are now serializable: T1 T3 T2.
    The serial order is the same as the time-stamp order.
```

对于问题1，Spanner为了防止这种冲突，让每个副本维护安全时间属性，即它最新的更新时间戳。Leader会按时间戳顺序向Follower发送写操作，同时更新Follower上的安全时间（最近的时间戳）。所以当只读事务的时间戳大于Follower的安全时间时，它将不得不等待，**直到Follower的安全时间大于事务时间戳**。

注意：

- Spanner**解决死锁问题是用[wound-wait locking rule](http://www.mathcs.emory.edu/~cheung/Courses/554/Syllabus/8-recv+serial/deadlock-woundwait.html)**。这是指旧事务T1当请求被新事务T2持有的锁时，T1会直接kill掉T2并占有锁，并且撤销重试的新事务将会保留原有的事务标号。这保证不会出现环形等待情况，同时也保证先提交的事务会优先获得锁以达到外部一致性。
- 所以Spanner是通过两段锁和2PC来确保外部一致性的。

**TrueTime 同步时间**

当各个节点的时间没有同步的话，会产生什么样的故障？

- 对于读写事务来说，他们使用了2PC（锁）协调事务，所以不同的时间不会造成任何问题。
- 对与只读事务，这会影响外部一致性：
  - 如果事务时间戳过大，只会导致事务执行变慢，不会影响正确性。
  - 如果事务时间戳过小，那么将会错过本应读取的事务数据。

时钟在分布式系统中是不可靠的。所以这里还是需要担心时间同步问题，Spanner在这里使用了Google TrueTime(TT)的技术来提供高精度的时钟。TrueTime是全球分布的时钟服务，它使用了GPS和原子时钟作为时刻参考，提供了比政府实验室的协议更精确的时钟：谷歌提到了集群中节点之间的时钟偏差的上限为7ms，而使用NTP进行时钟同步将给出100ms到250ms之间的某个值。

TT有两种API，它们的返回值如下：

![true-time-api](/static/image/2022-01-16/true-time-api.png)

TT会保证系统正确时钟会徘徊在一个区间内。Spanner利用这个特性保证了外部一致性：

- **Start Rule**. 对于所有事务，事务开始时间被选择为区间的最大值为TS。这保证了只读事务一定在TS之前触发。
- **Commit Wait**. 对于读写事务，必须当`TT.after(TS)`为真时才提交事务。这保证了事务一定是在TS之后被提交。



## FaRM

### Q：目标

（乐观并发控制的分布式事务+严格可串行化）

FaRM（Fast Remote Memory）是微软提出的主存计算平台的研究原型。它支持**乐观并发控制的分布式事务**，宣称没有妥协地实现了**严格可串行化**、**高性能和高可用**。

首先，FaRM系统是在一个数据中心中被组织的。这个数据中心将包含一个配置管理器ZooKeeper节点，一些主节点和备份节点。注意这里的备份节点不参与读操作，只会接受作为备份的写操作。其中ZooKeeper用来记录各个存活节点的角色。

下面介绍FaRM的一些特性：

- **RAM + NVRAM**：所有数据存储于RAM之中，在数据读取过程中没有任何硬盘操作；同时使用NVRAM，所以没有任何的硬盘写操作。这降低了系统硬盘IO的性能瓶颈。
- **RDMA**：FaRM的事务和复制协议都使用了单边RDMA，它能够绕过CPU来对某个服务器主存进行快速访问能力。这降低了CPU和内核系统调用的性能瓶颈。



### Q：工作原理

（NVRAM+RDMA、乐观并发控制）

**NVRAM的挑战**

FaRM使用RAM来存储数据，它最大的缺点在于存储节点宕机后数据会消失。FaRM分情况应对这个问题：

- **断电的情况**：分布式UPS被用来解决问题。当节点断电后，服务器立即切换UPS电源，FaRM此时通过SSD进行数据持久化。恢复供电后FaRM可以根据镜像来恢复内存。
  - 分布式UPS是平行于集中式UPS的概念，它指每个服务器/机架边会有一个UPS。分布式UPS比集中式好的地方在于，它排除了集中式UPS会全体失效的问题。
  - 这里也可以使用NVDIMM来解决，不过FaRM使用了更廉价的方法。
- **宕机的情况**：如硬件崩溃、软件缺陷等，FaRM选择将副本节点的主存数据读出。



**RDMA的挑战**

一个通用的计算机网络通信方式如下图。数据需要经过内核的多次系统调用和中断层层包装，然后在网卡上发送对应的二进制位到网络上，最终还要在目标机上重复这个过程才到达目标应用程序。

![networking-before-rdma](/static/image/2022-01-17/networking-before-rdma.png)

这个过程中需要大量的CPU计算，包括系统调用、中断、和函数帧跳转等。FaRM提出了两个想法来降低这里的消耗：

- **绕过内核**：应用程序直接与网卡通信，避免系统调用。
- **RDMA**：使用RDMA，对某个服务器的主存进行读写可以绕过CPU。

![networking-after-rdma](/static/image/2022-01-17/networking-after-rdma.png)

但回想Spanner对分布式事务的实现，其中2PC阶段大量需要CPU来进行申请锁、检查快照版本号等工作。这里直接读取主存势必导致一致性不能达到严格序列化，比如旧事务能读到新事务的数据。FaRM用一种乐观并发控制来处理，它只会用到读写者的CPU。



### Q：乐观并发控制

**Phase#1 - Execute**

执行阶段，对读请求来说，TC会直接用单边RDMA向对应主节点请求数据；对写请求而言，TC需要在本地缓存变动。如果数据在本地存储，那么TC直接进行数据读写，而不通过RDMA。

注意，TC会记录所有它访问的数据的地址和版本号，它们会在接下来的阶段用到。接下来可以进入Commit阶段了。

**Phase#2 - Commit**

![farm-arch](/static/image/2022-01-17/farm-arch.png)

首先介绍每个节点的主存结构：

- Region。主存的第一项就是存储数据分片的region，它被分配有2GB，并被复制到包括主节点和备份节点在内的1+f台机器上。
  - 在Region中，每个数据对象都有一个附带的64位版本号和锁标志。并且每个对象都有一个用于索引的地址。
- Ring buffer。主存中还有一个用作FIFO队列的ring buffer，作为接受事务Log的buffer和消息队列。
  - 事务Log在这里会被当作系统的写请求来对待。每个发送者都会通过RDMA向队列尾提交事务Log。

其次，每个FaRM实例都运行着一个事件循环，执行应用程序的代码并轮询RDMA队列。

**LOCK**

注意，LOCK过程只针对事务中的写请求：

- 对写数据，TC通过RDMA向对应主节点发送一个LOCK记录。
  - 这个记录包括TID、所有写数据对应的分片ID、数据地址、数据版本号和新值。
- 当参与者主节点发现LOCK记录后，会在对应版本号上尝试用CAS锁住地址对应的数据，并在对应数据头上标记LOCK字段。接下来主节点发送成功或失败信息。实际上LOCK操作会失败，具体情况如下：
  1. 其他事务已经占用该记录的锁，该数据头上已经标记LOCK。
  2. 版本号没有对齐，导致CAS失败
- 当TC受到所有的LOCK请求成功的消息后，进行下一步骤；如果LOCK操作失败，那么TC会直接撤销事务：向所有参与事务的主节点发送ABORT，来释放占用的锁。

**VALIDATE**

对于事务中的读操作：

- TC通过单边RDMA来重新获取每个对象的版本号及其锁标志的状态，来做提交验证。
  - 如果设置了锁标志，或者版本号在事务读取后发生了更改，那么TC将中止事务。

这种优化避免了在只读事务中持有锁，从而加快了它们的执行速度。

其实在非失败条件下，锁定和验证步骤就已经能够保证所有已提交FaRM事务的严格可序列化性。它们确保执行并发事务的结果与依次执行的结果相同，并且产生结果的顺序与实时一致。具体来说，LOCK+VALIDATE保证:

- 如果事务没有冲突，它读取的对象版本就不会发生变化。

- 如果事务有冲突，它将看到一个改变的版本号（某个事务已经写过它）或它访问的对象上的锁（某个事务正在写它）。

这里我们可以论证一下：

- 对于任何写请求Wx，在当前请求LOCK后并且事务提交结束前，不可能有任何其他写请求起效（因为LOCK占用），也不可能有任何读请求在Validate后读到旧数据（因为版本号改变）。可以说写写&读写之间相当于上了2PL锁。
- 但是如果在Validate过程中，出现其他事务的写请求Wx，并且本事务没有申请x的锁Lx。那么Wx将成功获取锁，Vx也可以成功通过验证。于是本事务提交成功，矛盾出现。
  - 我认为出现这种问题的原因是，Validate和Commit这两个步骤应当是原子性的（V和C之间没有任何W），但是FaRM没有实现它。你也不能在Rx或Vx的时候上锁，因为这里用RDMA绕过了CPU。不过，可以在Validate之前增加一个写性质的LockValidate步骤：让事务参与者根据读请求的地址，在数据上加锁（若加锁不成功则撤销事务）。直到TC提交事务后再释放所有写操作的锁。

```
  T1:                  Wx  C
  T2:  Rx  Ry  Rz  Vx  Vy  Vz  C

(Where L and V represent the Lock and Validate stages)
T1 happens before T2, but T2 does not see T2's result 
```

**COMMIT BACKUP**

但是在出现系统故障时，上述协议不足以保证可串行性。我们可能会遇到这样一种情况：事务在提交到一个主节点后，其他主节点有部分宕机，导致提交时失败了。这违反了事务原子性，事务原子性要求如果一个事务在一台机器上成功，那么它必须在所有其他机器上成功。

FaRM通过使用COMMIT-BACKUP记录来实现这种原子性：在发送COMMIT-PRIMARY记录之前，协调器将向事务中涉及的主节点的所有备份节点发送COMMIT-BACKUP记录，并等待确认在所有备份中被持久化。这个COMMIT-BACKUP记录包含与事务的LOCK记录相同的数据。

通过等待确认从至少一个主节点存储COMMIT PRIMARY记录，TC就能确保系统故障不会影响到事务原子性。

**COMMIT PRIMARY**

- 如果所有的主节点都对LOCK和VALIDATE请求响应成功，那么TC将决定提交事务。它通过在主节点的日志中附加一条COMMIT-PRIMARY记录，将这个决策广播给所有写请求相关的主节点。
  - 这个COMMIT-PRIMARY记录包含要提交的事务的ID。
- 接下来，主节点将事务的LOCK记录中对象的新值复制到内存中，增加对象的版本号，并清除对象的锁标志来处理该记录。

- 当TC受到一个来自主服务器的RDMA确认信息后，TC会直接向客户端报告成功。稍后我们将看到为什么这是安全的。

**TRUNCATE**

TC在接收到来自所有存储了提交记录的主节点的确认后，将发送TRUNCATE请求来截断主节点和备份节点上的日志。



## Memcache

### Q：目标

（全球集群：web+memcached+存储）

Facebook的架构包括多个web、memcache和数据库服务器。一组web和memcache服务器组成一个前端集群，多个前端集群组成一个Region。同一个Region内的前端集群共用同一个存储集群。Facebook在世界不同地区复制集群，指定一个地区为主要地区，其他地区为次要地区。

![memcache-architecture](/static/image/2022-01-21/memcache-architecture.png)



### Q：工作原理

**Cluster中的挑战：延时和负载**

对于web&cache组成的一个集群而言，主要有两大优化：

- 降低web与cache间的**时延**

  1. 通过数据依赖做**DAG的批请求**，这样能减少网络往返次数。
  2. **UDP读TCP写**。UDP减少了TCP中的安全机制，降低了时延和开销。不过这里UDP应该在应用层做了魔改：添加了序号防止丢包和乱序。
  3. 滑动窗口**解决Incast拥塞**。因为memcached使用一致性hashing来处理分布式哈希，所以web请求将会有大量来自服务器的响应到达客户端上，导致Incast congestion。这里memcache用到自己实现的类似与TCP滑动窗口的流量控制来解决这个问题。论文中讲到不同于TCP窗口只针对一个流，他们的可以针对目标不依赖的窗口。

- 降低cache与存储间的**负载**：减少因为cache missing导致的负载问题。memcache使用的是数据修改租约lease

  - 针对**过期写**。指当web服务器在memcache中为一个键设置了一个过时的值。常见于对键的并发更新被重新排序的情况下。当客户端遇到cache miss时，memcache服务器将给它一个lease。当同步的数据写回cache时，它必须提供这个token供memcache验证。而且当memcache收到该键的删除请求时，它将使该键的任何token失效。

  ```
    key 'k' not in cache
    C1 get(k), misses  (a token should be assigned here)
    C1 reads v1 from DB as the value of k
      C2 writes k = v2 in DB
      C2 delete(k)  (recall that any DB writes will invalidate key in cache)
    C1 set(k, v1)  (validate the token; if not then ignore it)
    now mc has stale data, since delete(k) has already happened
    will stay stale indefinitely until k is next written
  ```

  

  - 针对**惊群效应**。这里的惊群效应体现在很多请求在互相抢夺资源做同一件事：许多客户端尝试为一个无效的键读取数据，导致将有很多写回请求打到memcached上。为了防止它，memcache服务器每10秒只给每个键一次租期。如果在租约发出后10秒内又有另一个客户端请求密钥，则该请求将不得不等待。

  > **惊群效应**：当大量等待某个事件的进程或线程在该事件发生时被唤醒，但只有一个进程能够处理该事件时，就会出现这种问题。当进程醒来时，<u>它们将各自尝试处理事件，但只有一个会获胜。所有进程都将争夺资源，可能会冻结计算机，直到群体再次平静下来</u>

**Region中的挑战：复制**

因为cache是无状态的，所以可以像web一样做弹性分配。随着系统负载增加，他们可以通过在前端集群中添加更多的memcache和web服务器来扩展他们的系统，并进一步划分key set。然而，这有两个主要的局限性:

- 随着memcache服务器数量的增加，Incast拥塞将变得更糟，因为客户端必须与更多的服务器通信。

- 分区对热键并没有多大帮助，因为单个服务器需要处理该键的所有请求。在这种情况下，复制数据可以帮助我们在不同的服务器之间共享负载。



- **Region Pool**
  - 数据集中不常被访问且规模较大的项目很少需要复制。对于这些键，有一个适当的优化，每个区域只存储一个副本。
  - Facebook将这些密钥存储在一个区域池中，该区域池包含一组memcache服务器，由多个前端集群共享。这比具有低访问速率的过度复制项更有效。

- **Cold Cluster Warmup**
  - 当一个新的前端集群上线时，任何对它的请求都会导致缓存丢失，这可能会导致数据库过载。Facebook有一个名为Cold Cluster Warmup的机制来缓解这种情况。
  - Cold Cluster Warmup通过允许“冷集群”(即拥有空缓存的前端集群)中的客户端从“暖集群”(即拥有正常命中率缓存的集群)而不是持久化存储中检索数据来缓解这一问题。



**Region间的挑战：一致性**

Memcache指定一个Region含有主数据库，因此所有写操作都必须进入的主Region的数据库，而其他Region则只包含数据存储的只读副本。它们使用**MySQL的复制机制**使复制数据库与主数据库保持同步。这里的关键挑战是保持memcache中的数据与主数据库一致，而主数据库可能位于另一个区域。

![mysql-replication](/static/image/2022-01-21/mysql-replication.png)

这里考虑到用户不会在意他们会偶尔看到老旧数据，所以为了性能考虑Facebook会让从Region看到轻微的老旧数据。所以这里的目标是减少老旧数据的窗口，并保证所有region是最终一致性的。

- 对于主Region的写请求
  - 写操作直接发送到区域中的存储集群，然后存储集群将其复制到次要区域。如果在将更改复制到这些区域时存在延迟，次要区域中的客户端可能会为这里修改的任何键读取过时的数据。
- 对于从Region的写请求

```
  Key k starts with value v1
  C1 is in a secondary region
  C1 updates k=v2 in primary DB
  C1 delete(k)  (in local region)
  C1 get(k), miss
  C1 reads local DB  -- sees v1, not v2!
  later, v2 arrives from primary DB (replication lag)
```

这违反了写后读一致性的保证，远程标记可以用于防止这种情况：

1. 在Region Pool中设置 remote marker $r_k$。可以将$r_k$看作一条memcache记录，它表示键k的额外信息。

2. 将写操作发送到主区域，并在请求中包含$r_k$，以便主区域在复制写操作时知道将$r_k$作废。

3. 删除本端集群中的k。

于是web服务器下一次对k的请求将导致缓存丢失，之后它将检查区域池来找到rk。如果rk存在，则意味着本地区域中的数据已经过时，服务器将把读操作定向到主区域。否则，它将从本地区域读取。
